---
layout: full_article
title: "Fish Species Recognition with Faster R-CNN Inception-v2 using QUT FISH Dataset"
author: "Yonatan Adiwinata, Akane Sasaoka, I Putu Agung Bayupati, Oka Sudana"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-66597 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-66597"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-66597"  
comments: true
---

<p><span class="font1" style="font-weight:bold;">LONTAR KOMPUTER VOL. 11, NO. 3 DECEMBER 2020</span></p>
<p><span class="font1" style="font-weight:bold;">DOI : 10.24843/LKJITI.2020.v11.i03.p03</span></p>
<p><span class="font1" style="font-weight:bold;">Accredited B by RISTEKDIKTI Decree No. 51/E/KPT/2017</span></p>
<p><span class="font1" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font1" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font2" style="font-weight:bold;"><a name="bookmark1"></a>Fish Species Recognition with Faster R-CNN Inception-v2 using QUT FISH Dataset</span></h1>
<p><span class="font1">Yonatan Adiwinata<sup>a1</sup>, Akane Sasaoka<sup>b2</sup>, I Putu Agung Bayupati<sup>a3</sup>, Oka Sudana<sup>a4</sup></span></p>
<p><span class="font1"><sup>a</sup>Department of Information Technology, Faculty of Engineering, Udayana University, Jl. Raya Kampus Unud Bukit Jimbaran, Bali, Indonesia </span><a href="mailto:1yonatanadiwinata@student.unud.ac.id"><span class="font1" style="text-decoration:underline;"><sup>1</sup>yonatanadiwinata@student.unud.ac.id</span><span class="font1"> </span></a><span class="font0" style="font-variant:small-caps;">(c</span><span class="font0">orresponding author) </span><a href="mailto:3bayuhelix@yahoo.com"><span class="font1" style="text-decoration:underline;"><sup>3</sup>bayuhelix@yahoo.com</span></a><span class="font1" style="text-decoration:underline;"> </span><a href="mailto:4agungokas@unud.ac.id"><span class="font1" style="text-decoration:underline;"><sup>4</sup>agungokas@unud.ac.id</span></a></p>
<p><span class="font1"><sup>b</sup>Electrical Engineering and Computer Science, Kanazawa University Kanazawa, Ishikawa, Japan</span></p>
<p><a href="mailto:2acannie0530@gmail.com"><span class="font1" style="text-decoration:underline;"><sup>2</sup>acannie0530@gmail.com</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">Fish species conservation had a big impact on the natural ecosystems balanced. The existence of efficient technology in identifying fish species could help fish conservation. The most recent research related to was a classification of fish species using the Deep Learning method. Most of the deep learning methods used were Convolutional Layer or Convolutional Neural Network (CNN). This research experimented with using object detection method based on deep learning like Faster R-CNN, which possible to recognize the species of fish inside of the image without more image preprocessing. This research aimed to know the performance of the Faster R-CNN method against other object detection methods like SSD in fish species detection. The fish dataset used in the research reference was QUT FISH Dataset. The accuracy of the Faster R-CNN reached 80.4%, far above the accuracy of the Single Shot Detector (SSD) Model with an accuracy of 49.2%.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">Fish Species Recognition, Object Detection, Faster R-CNN, QUT FISH Dataset, Deep Learning</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font1">Ocean makes up two-thirds of the earth's surface. Ocean ecosystems have an important role in the balance of nature, with a variety of living things that live in it, like fishes. More than 22,000 species of fishes make up nearly half of the total 55,000 species of vertebrates living on earth [1]. The development of technology related to the cultivation of fish species was very important for the preservation and protection of marine ecosystems because fish was an important factor in the marine ecosystem. The existence of efficient technology in fish species recognition could help the fish cultivation process because the cultivation method for each fish was not always the same. Fish species were identified through manual observation by humans in the past, which required humans to study various fish characteristics in order to recognize the fish species, and recently the fish species recognition could be done by utilizing artificial intelligence technology.</span></p>
<p><span class="font1">The latest research that has been done related to the fish species classification was using the Deep Learning method. The deep learning method that was used currently was the Convolutional Layer or Convolutional Neural Network (CNN) [1]–[3]. The classification method in the research references needs background removal preprocessing to recognize the fish species inside the image. This research experimented with using object detection method based on deep learning like Faster R-CNN, which possible to recognize the species of fish inside of the image without more image preprocessing (fish species detection).</span></p>
<p><span class="font1">The main method used in this research was the Faster R-CNN method with Inception-v2 architecture. Single Shot Detector (SSD) is also used in this research as a compliment. This research aimed to know the performance of the Faster R-CNN method against other object</span></p>
<p><span class="font1">detection methods like SSD in fish species detection. Faster R-CNN was chosen because Faster R-CNN was a method that popular recently, and it had a great performance in object detection, which better than other basic object detection methods like SSD [4] and Yolo-V3 [5]. The result of this research was comparison performance in fish species recognition between Faster R-CNN and SSD object detection method.</span></p>
<p><span class="font1">The first research reference used was research from Praba Hridayami et al., which discussed the classification of fish species using the Convolutional Neural Network (CNN) with VGG-16 Architecture. The dataset used was the QUT FISH Dataset. The data used were 50 classes with ten training data and 5 test data for each class. The total data used was 750 cropped image data. Evaluation of test results was carried out using the Genuine Acceptance Rate (GAR), False Acceptance Rate (FAR), and False Rejection Rate (FRR). The best test results obtained were with GAR 96.4%, FAR 3.6%, and FRR 3.6% [1].</span></p>
<p><span class="font1">The next research reference was about improving the performance of transfer learning in the Squeeze-and-Excitation networks + Bilinear CNN (SE+BCNN) method. This research was done by Chenchen Qiu et al. The highest accuracy achieved on the QUT FISH dataset was 71.80% [2].</span></p>
<p><span class="font1">The next research reference was researched by M. Sarigül and M. Avci about the comparison of test results from three different custom Convolutional Layers architectures. The dataset used was the QUT FISH Dataset. This research used 93 species from this dataset. The highest accuracy that was obtained in this study was 46.02% [3].</span></p>
<p><span class="font1">The next research reference was about traffic light detection research from Janahiraman and Subhan. This research was comparing the results of traffic light detection between SSD-MobileNet-V2 and Faster R-CNN Inception-v2 Architecture. The results of the test accuracy that have been obtained by the Faster R-CNN Inception-v2 method was 97.02%, and SSD-Mobilenet-v2 was 58.21% [4].</span></p>
<p><span class="font1">The next research reference was about livestock detection, which was also carried out by comparing several object detection methods by Han et al. The dataset used was an image containing livestock with a resolution of 4000 pixels x 3000 pixels taken from the air. The methods compared in the journal were Faster R-CNN, YOLOv3, and the Unet + Inception Method. On the Faster R-CNN, the accuracy obtained was 89.1%, Yolo-V3 gets 83% accuracy, and Unet + Inception gets 89.3% accuracy [5].</span></p>
<p><span class="font1">The next research reference was about investigating fruit species detection with Faster R-CNN from Basri et al. This research used object class images of mango and dragon fruit as image data. The object detection model moves with the help of the Tensorflow library. The results in this research were reached accuracy, up to 70.6% [6].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h2></li></ul>
<p><span class="font1">There was 4 phase in this research. These phases were Data Collecting Phase, Data Processing Phase, Data Training Phase, and Testing Phase.</span></p><img src="https://jurnal.harianregional.com/media/66597-1.jpg" alt="" style="width:265pt;height:134pt;">
<p><span class="font1" style="font-weight:bold;">Figure 1. </span><span class="font1">Research Flowchart</span></p>
<p><span class="font1">The data collection phase was the phase of collecting data needed in this research. The data that must be collected was fish dataset. The fish dataset used in this research was the QUT FISH dataset [7].</span></p>
<p><span class="font1">The data processing phase was the phase of adjusting the data from the dataset obtained for use in the Data Training Phase. This research used 50 fish classes with ten training images and 5 test images for each class. The total image data used was 750 data. The reason for using this amount of data was so that the results obtained could be compared with current research references [1] because of similar data usage conditions. The 50 names of fish data classes from QUT FISH used in this research could be seen in Table 1.</span></p>
<p><span class="font1">The data training phase was the phase of training the object detection model with the Faster R-CNN Method Inception V-2 Architecture using training data that has been prepared in the previous phase. The data training process was done with Google Colab Cloud service.</span></p>
<p><span class="font1">The testing phase was the phase to test the performance of the object detection model that has been trained and evaluating the test results. Evaluation of test results would be compared with the results obtained from previous related research [1]–[3].</span></p>
<p><span class="font1">The 50 classes of fish used in this research were selected based on the consideration that each class must have a minimum of 15 data from this research reference [1]. These 15 data would be used in the training and testing phase.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font1" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Faster R-CNN</span></h2></li></ul>
<p><span class="font1">The popularity of machine learning was increasing following the popularity of Artificial Neural Networks (ANN). ANN was a non-linear complex learning system that occurs in a network of neurons [8]. Convolutional Neural Network (CNN) was one of the most developed ANN derivatives currently [1]. CNN was a deep learning algorithm that uses a convolutional layer for feature extraction and a fully connected layer for classification [9]. CNN could be applied in image and text classification [10], [11]. The method used in this research was Faster R-CNN.</span></p>
<p><span class="font1">Faster R-CNN was a deep learning algorithm developed from CNN that could be used in object detection systems [12]. The object detection system was a system that has a function to localize objects in the image, so the classification process would get better results [13].</span></p><img src="https://jurnal.harianregional.com/media/66597-2.jpg" alt="" style="width:302pt;height:151pt;">
<p><span class="font1" style="font-weight:bold;">Figure 2. </span><span class="font1">Illustration of the Faster R-CNN Method Workflow [16]</span></p>
<p><span class="font1">Faster R-CNN was the development of Fast R-CNN. Fast R-CNN was an object detection method that used the selective search method in the region proposal search process [14]. The region Proposal module task was to find regions or areas that may contain objects in it [15]. Shaoqing Ren, in his research on the implementation of Faster R-CNN as Real-Time Object Detection, revealed that this method generally consists of two modules, namely Region Proposal Network (RPN) and Fast R-CNN. [16]. Figure 2 was an illustration of the Faster R-CNN method workflow. The input that was entered into the system will be processed in the Convolutional Network first to get the feature of the object in the image, named Feature Maps. Then Feature Maps from the Convolutional Network will be forwarded to the Region Proposal Network (RPN) module and the Fast R-CNN module. The region Proposal function was to find regions or areas that may contain</span></p>
<p><span class="font1">objects in it (Region Proposal) [17]. The Fast R-CNN module function was refining the region proposals of the RPN and classifying the objects in it [16].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font1" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Single Shot Detector (SSD)</span></h2></li></ul>
<p><span class="font1">SSD was a single-shot detector for multiple class objects that was faster than YOLO. The SSD method was based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections. SSD only needs an input image and ground truth boxes for each object during training. SSD object detection method was designed to create a deep learning object detection method with a lighter process than other object detection methods based on deep learning processes like YOLO and Faster R-CNN [18].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font1" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Genuine Acceptance Rate (GAR), False Acceptance Rate (FAR), False Rejection</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark12"></a>Rate (FRR) and Accuracy (ACC)</span></h2></li></ul>
<p><span class="font1">Genuine Acceptance Rate (GAR) was the percentage of the number of objects that were correctly recognized [19]. The results of the object classification must get the correct class with a probability above the threshold value used. The formula of GAR showed in (1) [20].</span></p>
<p><span class="font3">GAR = 1 - FRR &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font1">(1)</span></p>
<p><span class="font1">False Acceptance Rate (FAR) was the percentage of the number of objects received, but the class classification results were wrong [21]. False Acceptance Rate could also be said as </span><span class="font1" style="font-style:italic;">False Positive</span><span class="font1">. The formula of FAR showed in (2) [20].</span></p>
<div>
<p><span class="font3">FAR =</span></p>
</div><br clear="all">
<p><span class="font4" style="text-decoration:underline;">Total number of fish species identified with another fish species </span><span class="font4">Total number of test data</span></p>
<div>
<p><span class="font1">(2)</span></p>
</div><br clear="all">
<p><span class="font1">False Rejection Rate (FRR) was the percentage of the number of objects that do not get a single classification result. The False Rejection Rate was also commonly referred to as the False Negative. The formula of FRR showed in (3) [20].</span></p>
<div>
<p><span class="font3">FRR =</span></p>
</div><br clear="all">
<p><span class="font4" style="text-decoration:underline;">Total number genuine of fish species rejected </span><span class="font4">Total number of test data</span></p>
<div>
<p><span class="font1">(3)</span></p>
</div><br clear="all">
<p><span class="font1">Accuracy (ACC) was calculated as the number of all correct predictions divided by the total number of the test data. The formula of ACC showed in (4).</span></p>
<div>
<p><span class="font3">ACC =</span></p>
</div><br clear="all">
<p><span class="font4" style="text-decoration:underline;">Total number of fish species identified correctly </span><span class="font4">Total number of test data</span></p>
<div>
<p><span class="font1">(4)</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark13"></a><span class="font1" style="font-weight:bold;"><a name="bookmark14"></a>2.4 &nbsp;&nbsp;&nbsp;Evaluation Protocol</span></h2></li></ul>
<p><span class="font1">The process of evaluating test results was calculating the values of GAR, FAR, FRR, and Accuracy from both of Faster R-CNN and SSD models. GAR, FAR, and FRR was used based on this research references [1]. Accuracy is used to complement the evaluation of test results. The formula of GAR, FAR, FRR, and Accuracy could be seen in section 2.3. The detection result used was the recognition result with the highest confidence percentage.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark15"></a><span class="font1" style="font-weight:bold;"><a name="bookmark16"></a>3. &nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font1">This section describes the results and discussion of this research about fish species recognition using the R-CNN Faster and SSD Method with Inception-v2 Architecture with the QUT FISH Dataset.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark17"></a><span class="font1" style="font-weight:bold;"><a name="bookmark18"></a>3.1. &nbsp;&nbsp;&nbsp;Preparing Training Data and Testing Data</span></h2></li></ul>
<p><span class="font1">Training data was the data used in the model training process. The training data used were 10 data for each class. Total fish classes used in this research were 50 fish classes. Total training</span></p>
<p><span class="font1">data used were 500 data from the QUT FISH Dataset. Examples of training data used in this</span></p><img src="https://jurnal.harianregional.com/media/66597-3.jpg" alt="" style="width:364pt;height:167pt;">
<p><span class="font1" style="font-weight:bold;">Figure 3. </span><span class="font1">Training Data Samples</span></p>
<p><span class="font1">research could be seen in Figure 3.</span></p>
<p><span class="font1">Test data was the data used in the testing data phase. The test data used were 5 data for each class. The total fish class used in this research were 50 fish classes. Total test data used were 250 data from the QUT FISH Dataset. Examples of test data used in this research could be seen in Figure 4.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark19"></a><span class="font1" style="font-weight:bold;"><a name="bookmark20"></a>3.2. &nbsp;&nbsp;&nbsp;Implementation</span></h2></li></ul>
<p><span class="font1">This subsection contained the implementation of the testing phase. The testing phase was done by running the detection process upon the test image using the object detection model of the training result. The optimum threshold used in Faster R-CNN model testing was 72%, which was the optimum threshold of the FAR and FRR values. The optimum threshold used in Single Shot Detector (SSD) model testing was 54%, which was the optimum threshold of the FAR and FRR values. Figure 5 was an example of a detection result with one correct detection result.</span></p>
<p><span class="font1">Figure 5 was an example of test results with one correct detection result, which belonged to the Genuine Acceptance Rate (GAR). The class object contained in the image was Aluterus Scriptus, and the detection results obtained were the Aluterus Scriptus class with 99% confidence. The confidence value was the percentage of object similarity in the image to the object recognized according to the object detection model or object classification model. In the detection results of the object detection method, there might be images that had more than one detection result which had confidence above the threshold value. An example of this case could be seen in Figure 5.</span></p><img src="https://jurnal.harianregional.com/media/66597-4.jpg" alt="" style="width:366pt;height:150pt;">
<p><span class="font1" style="font-weight:bold;">Figure 4. </span><span class="font1">Testing Data Samples</span></p><img src="https://jurnal.harianregional.com/media/66597-5.jpg" alt="" style="width:301pt;height:168pt;">
<p><span class="font1" style="font-weight:bold;">Figure 5. </span><span class="font1">The Test Results with One Correct Detection Result</span></p>
<p><span class="font1">Figure 6 was an example of test results that get more than one detection result. The image used was Bodianus Diana class test image. The detection results obtained were the Cirrhilabrus Cyanopleura class with 86% confidence and Diana Bodianus class with 77% confidence. In this test image, the detection results used were Cirrhilabrus Cyanopleura class because it had the highest confidence percentage. This test data result belonged to the False Acceptance Rate</span></p><img src="https://jurnal.harianregional.com/media/66597-6.jpg" alt="" style="width:311pt;height:139pt;">
<p><span class="font1" style="font-weight:bold;">Figure 6. </span><span class="font1">The Test Results with more than One Detection Result</span></p>
<p><span class="font1">(FAR) because it had wrong recognition. Figure 7 was an example of test results that did not get</span></p><img src="https://jurnal.harianregional.com/media/66597-7.jpg" alt="" style="width:308pt;height:148pt;">
<p><span class="font1" style="font-weight:bold;">Figure 7. </span><span class="font1">The Test Results without any Detection Result</span></p>
<p><span class="font1">any detection results. The test results from Figure 7 belonged to the False Rejection Rate (FRR). The test image used was the Stethojulis Bandanensis class test image.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font1" style="font-weight:bold;"><a name="bookmark22"></a>3.3. &nbsp;&nbsp;&nbsp;Testing Result</span></h2></li></ul>
<p><span class="font1">This section contained the testing result of the Faster R-CNN and SSD model in fish species detection. Fish species detection was recognized as the fish species inside a raw fish image. The raw fish image was an image of fish that not has been preprocessed. Table 3 contained a comparison of the testing result between Faster R-CNN and SSD. Evaluation of the testing result used was GAR, FAR, FRR, and Accuracy.</span></p>
<p><span class="font1" style="font-weight:bold;text-decoration:underline;">Table 3. </span><span class="font1" style="text-decoration:underline;">Comparison of GAR, FAR, FRR, and ACC between Faster R-CNN and SSD</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font1">Method</span></p></td><td style="vertical-align:top;">
<p><span class="font1">GAR</span></p></td><td style="vertical-align:top;">
<p><span class="font1">FAR</span></p></td><td style="vertical-align:top;">
<p><span class="font1">FRR</span></p></td><td style="vertical-align:top;">
<p><span class="font1">ACC</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">SSD Inception-v2</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">74%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">24.8%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">26%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">49.2%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Faster R-CNN Inception-v2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">90.4%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">10%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">9,6%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">80.4%</span></p></td></tr>
</table>
<p><span class="font1">The performance of each Faster R-CNN and SSD model could be seen in Table 3. The Faster R-CNN model had much better performance than the SSD model. Faster R-CNN accuracy was 80.4%, much better than SSD accuracy that was 49.2%. SSD model made a more wrong prediction of up to 24.8% (FAR) and more no detection result up to 26% (FRR). More wrong predictions and no detection result cause the SSD model to have low accuracy, although already using the optimum threshold in the testing phase. Faster R-CNN had higher performance than the SSD model proved that Faster R-CNN was more reliable for fish species detection.</span></p>
<p><span class="font1">Test data that had the most failed prediction in Faster R-CNN were from four class fish, such as Anyperodon Leucogrammicus, Bodianus Diana, Cephalopholis Sexmaculata, and Pseudocheilinus Hexataenia.</span></p>
<p><span class="font1">All test data from the Anyperodon Leucogrammicus class got a failed prediction. Three test data got the wrong prediction, and two test data got no detection result.</span></p><img src="https://jurnal.harianregional.com/media/66597-8.jpg" alt="" style="width:263pt;height:73pt;"><img src="https://jurnal.harianregional.com/media/66597-9.jpg" alt="" style="width:329pt;height:73pt;"><img src="https://jurnal.harianregional.com/media/66597-10.jpg" alt="" style="width:131pt;height:74pt;">
<p><span class="font1" style="font-weight:bold;">Figure 8. </span><span class="font1">Some Samples of Anyperodon Leucogrammicus Training Data</span></p>
<p><span class="font1">Figure 8 showed some samples of Anyperodon Leucogrammicus training data. Those Anyperodon Leucogrammicus training data had inconsistent features that made the model difficult to recognize the fish species in Anyperodon Leucogrammicus tests data. There were two training</span></p>
<p><span class="font1">data with binary color mode. In Faster R-CNN, object color was an important feature of an object. Those binary colored images interfered with the training of the model. Then there were two training data with greeny color that the fish did not have a dorsal fin. That inconsistency shape in training data caused the Faster R-CNN model could not detect the fish in test data correctly.</span></p>
<p><span class="font1">Four test data from Bodianus Diana's class got failed prediction. Two test data got the wrong prediction, and two test data got no detection result.</span></p><img src="https://jurnal.harianregional.com/media/66597-11.jpg" alt="" style="width:407pt;height:153pt;">
<p><span class="font1" style="font-weight:bold;">Figure 9. </span><span class="font1">Some Samples of Bodianus Diana Testing Data</span></p>
<p><span class="font1">Figure 9 showed some samples of not good Bodianus Diana training data. These two binary colored images caused one wrong prediction and one test data with no detecting result. Another two test data with failed prediction was in good quality images so that two another failed prediction caused by the failure of Faster R-CNN model.</span></p>
<p><span class="font1">Four test data from the Cephalopholis Sexmaculata class got failed prediction. Those four test data got got no detection result.</span></p><img src="https://jurnal.harianregional.com/media/66597-12.jpg" alt="" style="width:410pt;height:225pt;">
<p><span class="font1" style="font-weight:bold;">Figure 10. </span><span class="font1">Cephalopholis Sexmaculata Testing Data that Got Failed Prediction</span></p>
<p><span class="font1">Figure 10 showed Cephalopholis Sexmaculata testing data that got failed prediction. There were two data that got any prediction result, but the confidence level below the optimum threshold used (72%). One of those two data got the correct prediction result, so one failed prediction result was caused by the threshold used to high. Another three test data with failed prediction was in good</span></p>
<p><span class="font1">quality images, so that three another failed prediction caused by the failure of Faster R-CNN model. Three test data from Pseudocheilinus Hexataenia got failed prediction. Those three test data got got wrong prediction result.</span></p><img src="https://jurnal.harianregional.com/media/66597-13.jpg" alt="" style="width:393pt;height:326pt;">
<p><span class="font1" style="font-weight:bold;">Figure 11. </span><span class="font1">Three Pseudocheilinus Hexataenia Testing Data (Left) and Three Samples of Halichoeres Melanurus Training Data (Right)</span></p>
<p><span class="font1">Figure 11 showed three Pseudocheilinus Hexataenia testing data on the left side and three samples of Halichoeres Melanurus training data on the right side. Pseudocheilinus Hexataenia had a similar pattern with Halichoeres Melanurus that was horizontal lines. Faster R-CNN failed to extract more features from Pseudocheilinus Hexataenia like head shape and the fish fin, so the model probably made the wrong prediction in Pseudocheilinus Hexataenia test data.</span></p>
<p><span class="font1">Overall Faster R-CNN model had a good performance on fish species detection with 80.4% accuracy than SSD with 49.2% accuracy. Faster R-CNN probably could get better accuracy in fish species detection if using other architecture that more suitable for extracting fish features. Need more research to got that more suitable architecture for extracting fish features in Faster R-CNN.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark23"></a><span class="font1" style="font-weight:bold;"><a name="bookmark24"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font1">Overall Faster R-CNN model had a good performance on fish species detection with 80.4% accuracy than SSD with 49.2% accuracy. Faster R-CNN got worse prediction result upon test data on Anyperodon Leucogrammicus, Bodianus Diana, Cephalopholis Sexmaculata, and Pseudocheilinus Hexataenia class object. Faster R-CNN probably could get better accuracy in fish species detection if using other architecture that more suitable for extracting fish features. Need more research to get more suitable architecture for extracting fish features in Faster R-CNN.</span></p>
<h2><a name="bookmark25"></a><span class="font1" style="font-weight:bold;"><a name="bookmark26"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;P. Hridayami, I. K. G. D. Putra, and K. S. Wibawa, &quot;Fish species recognition using VGG16 deep convolutional neural network,&quot; </span><span class="font1" style="font-style:italic;">The Journal of Computer Science and Engineering</span><span class="font1">, vol. 13, no. 3, pp. 124–130, 2019, doi: 10.5626/JCSE.2019.13.3.124.</span></p></li>
<li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;C. Qiu, S. Zhang, C. Wang, Z. Yu, H. Zheng, and B. Zheng, &quot;Improving transfer learning and squeeze- and-excitation networks for small-scale fine-grained fish image classification,&quot; </span><span class="font1" style="font-style:italic;">IEEE Access</span><span class="font1">, vol. 6, pp. 78503–78512, 2018, doi: 10.1109/ACCESS.2018.2885055.</span></p></li>
<li>
<p><span class="font1">[3] &nbsp;&nbsp;&nbsp;M. Sarigül and M. Avci, &quot;Comparison of Different Deep Structures for Fish Classification,&quot; </span><span class="font1" style="font-style:italic;">International Journal of Computer Theory and Engineering</span><span class="font1">, vol. 9, no. 5, pp. 362–366, 2017, doi: 10.7763/ijcte.2017.v9.1167.</span></p></li>
<li>
<p><span class="font1">[4] &nbsp;&nbsp;&nbsp;T. V. Janahiraman and M. S. M. Subuhan, &quot;Traffic light detection using tensorflow object detection framework,&quot; </span><span class="font1" style="font-style:italic;">2019 IEEE 9th International Conference on System Engineering and Technology (ICSET) 2019 - Proceeding</span><span class="font1">, no. October, pp. 108–113, &nbsp;2019, doi:</span></p></li></ul>
<p><span class="font1">10.1109/ICSEngT.2019.8906486.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;L. Han, P. Tao, and R. R. Martin, &quot;Livestock detection in aerial images using a fully convolutional network,&quot; </span><span class="font1" style="font-style:italic;">Computational Visual Media</span><span class="font1">, vol. 5, no. 2, pp. 221–228, 2019, doi: 10.1007/s41095-019-0132-5.</span></p></li>
<li>
<p><span class="font1">[6] &nbsp;&nbsp;&nbsp;H. Basri, I. Syarif, and S. Sukaridhoto, &quot;Faster R-CNN implementation method for multi-fruit detection using tensorflow platform,&quot; </span><span class="font1" style="font-style:italic;">International</span><span class="font1"> Electronics </span><span class="font1" style="font-style:italic;">Symposium</span><span class="font1"> on </span><span class="font1" style="font-style:italic;">Knowledge </span><span class="font1">Creation and Intelligent </span><span class="font1" style="font-style:italic;">Computing</span><span class="font1"> (</span><span class="font1" style="font-style:italic;">IES</span><span class="font1">-</span><span class="font1" style="font-style:italic;">KCIC</span><span class="font1">) </span><span class="font1" style="font-style:italic;">2018 Proceedings</span><span class="font1">, pp. 337–340, 2019, doi: 10.1109/KCIC.2018.8628566.</span></p></li>
<li>
<p><span class="font1">[7] &nbsp;&nbsp;&nbsp;A. Karad, G. Padhar, R. Agarwal, and S. Kumar, &quot;Fish species detection using computer vision,&quot; vol. 4, no. 6, pp. 2–6, 2020.</span></p></li>
<li>
<p><span class="font1">[8] &nbsp;&nbsp;&nbsp;D. Kristianto, C. Fatichah, B. Amaliah, and K. Sambodho, &quot;Prediction of Wave-induced Liquefaction using Artificial Neural Network and Wide Genetic Algorithm,&quot; </span><span class="font1" style="font-style:italic;">Lontar Komputer Jurnal Ilmiah Teknologi Informasi</span><span class="font1">, vol. 8, no. 1, p. 1, &nbsp;&nbsp;2017, doi:</span></p></li></ul>
<p><span class="font1">10.24843/lkjiti.2017.v08.i01.p01.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[9] &nbsp;&nbsp;&nbsp;O. Sudana, I. W. Gunaya, and I. K. G. D. Putra, &quot;Handwriting identification using deep convolutional neural network method,&quot; </span><span class="font1" style="font-style:italic;">Telkomnika (Telecommunication, &nbsp;Computing,</span></p></li></ul>
<p><span class="font1" style="font-style:italic;">Electronics and Control</span><span class="font1">, vol. 18, no. 4, pp. 1934–1941, &nbsp;&nbsp;2020, doi:</span></p>
<p><span class="font1">10.12928/TELKOMNIKA.V18I4.14864.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[10] &nbsp;&nbsp;&nbsp;I. M. Mika Parwita and D. Siahaan, &quot;Classification of Mobile Application Reviews using Word Embedding and Convolutional Neural Network,&quot; </span><span class="font1" style="font-style:italic;">Lontar Komputer Jurnal Ilmiah Teknologi Informasi</span><span class="font1">, vol. 10, no. 1, p. 1, 2019, doi: 10.24843/lkjiti.2019.v10.i01.p01.</span></p></li>
<li>
<p><span class="font1">[11] &nbsp;&nbsp;&nbsp;S. Wang, M. Huang, and Z. Deng, &quot;Densely connected CNN with multi-scale feature attention for text classification,&quot; </span><span class="font1" style="font-style:italic;">International Joint Conference on Artificial Intelligence.</span><span class="font1">, vol. 2018-July, pp. 4468–4474, 2018, doi: 10.24963/ijcai.2018/621.</span></p></li>
<li>
<p><span class="font1">[12] &nbsp;&nbsp;&nbsp;H. Jiang and E. Learned-Miller, &quot;Face Detection with the Faster R-CNN,&quot; </span><span class="font1" style="font-style:italic;">Proc. - 12th IEEE International Conference on Automatic Face Gesture Recognition, FG 2017 - 1st Int. Work. Adapt. Shot Learn. Gesture Underst. Prod. ASL4GUP 2017, Biometrics Wild, Bwild 2017, Heteroge</span><span class="font1">, pp. 650–657, 2017, doi: 10.1109/FG.2017.82.</span></p></li>
<li>
<p><span class="font1">[13] &nbsp;&nbsp;&nbsp;P. Garg, D. R. Chowdhury, and V. N. More, &quot;Traffic Sign Recognition and Classification Using YOLOv2, Faster RCNN and SSD,&quot; </span><span class="font1" style="font-style:italic;">2019 10th Int. Conf. Comput. Commun. Netw. Technol.</span><span class="font1">, pp. 1–5, 2019.</span></p></li>
<li>
<p><span class="font1">[14] &nbsp;&nbsp;&nbsp;K. Wang, Y. Dong, H. Bai, Y. Zhao, and K. Hu, &quot;Use fast R-CNN and cascade structure for face detection,&quot; </span><span class="font1" style="font-style:italic;">VCIP 2016 - 30th ANNIVERSARY OF VISUAL COMMUNICATION AND IMAGE PROCESSING</span><span class="font1">, pp. 4–7, 2017, doi: 10.1109/VCIP.2016.7805472.</span></p></li>
<li>
<p><span class="font1">[15] &nbsp;&nbsp;&nbsp;L. Zhang, L. Lin, X. Liang, and K. He, &quot;Is faster R-CNN doing well for pedestrian detection?,&quot; </span><span class="font1" style="font-style:italic;">Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)</span><span class="font1">, vol. 9906 LNCS, pp. 443–457, 2016, doi: 10.1007/978-3-319-46475-6_28.</span></p></li>
<li>
<p><span class="font1">[16] &nbsp;&nbsp;&nbsp;S. Ren, K. He, R. Girshick, and J. Sun, &quot;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,&quot; </span><span class="font1" style="font-style:italic;">The IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font1">, vol. 39, no. 6, pp. 1137–1149, 2016, doi: 10.1109/TPAMI.2016.2577031.</span></p></li>
<li>
<p><span class="font1">[17] &nbsp;&nbsp;&nbsp;Y. Nagaoka, T. Miyazaki, Y. Sugaya, and S. Omachi, &quot;Text Detection by Faster R-CNN with Multiple Region Proposal Networks,&quot; </span><span class="font1" style="font-style:italic;">Proc. International Conference on Document Analysis and Recognition</span><span class="font1">, pp. 15–20, 2017, doi: 10.1109/ICDAR.2017.343.</span></p></li>
<li>
<p><span class="font1">[18] &nbsp;&nbsp;&nbsp;W. Liu </span><span class="font1" style="font-style:italic;">et al.</span><span class="font1">, &quot;SSD: Single Shot MultiBox Detector Wei,&quot; </span><span class="font1" style="font-style:italic;">European Conference on Computer</span></p></li></ul>
<p><span class="font1" style="font-style:italic;">Vision</span><span class="font1">, vol. 1, pp. 21–37, 2016, doi: 10.1007/978-3-319-46448-0 2.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[19] &nbsp;&nbsp;&nbsp;T. S. Indi and Y. A. Gunge, &quot;Early Stage Disease Diagnosis System Using Human Nail Image Processing,&quot; </span><span class="font1" style="font-style:italic;">International Journal of Information Technology and Computer Science.</span><span class="font1">, vol. 8, no. 7, pp. 30–35, 2016, doi: 10.5815/ijitcs.2016.07.05.</span></p></li>
<li>
<p><span class="font1">[20] &nbsp;&nbsp;&nbsp;Z. Waheed, A. Waheed, and M. U. Akram, &quot;A robust non-vascular retina recognition system using structural features of retinal image,&quot; </span><span class="font1" style="font-style:italic;">Proc. 2016 13th International Bhurban Conference on Applied Sciences &amp;&nbsp;Technology Technol. IBCAST 2016</span><span class="font1">, pp. 101–105, 2016, doi: 10.1109/IBCAST.2016.7429862.</span></p></li>
<li>
<p><span class="font1">[21] &nbsp;&nbsp;&nbsp;S. Bharathi and R. Sudhakar, &quot;Biometric recognition using finger and palm vein images,&quot; </span><span class="font1" style="font-style:italic;">Soft Computing,</span><span class="font1"> vol. 23, no. 6, pp. 1843–1855, 2019, doi: 10.1007/s00500-018-3295-6.</span></p></li></ul>
<p><span class="font1">154</span></p>