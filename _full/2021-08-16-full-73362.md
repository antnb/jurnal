---
layout: full_article
title: "Offline Signature Identification Using Deep Learning and Euclidean Distance"
author: "Made Prastha Nugraha, Adi Nurhadiyatna, Dewa Made Sri Arsa"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-73362 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-73362"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-73362"  
comments: true
---

<p><span class="font2" style="font-weight:bold;">LONTAR KOMPUTER VOL. 12, NO. 2 AUGUST 2021</span></p>
<p><span class="font2" style="font-weight:bold;">DOI : 10.24843/LKJITI.2021.v12.i02.p04</span></p>
<p><span class="font2" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 30/E/KPT/2018</span></p>
<p><span class="font2" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font2" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>Offline Signature Identification Using Deep Learning and Euclidean Distance</span></h1>
<p><span class="font2">Made Prastha Nugraha<sup>a1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Adi Nurhadiyatna<sup>b1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Dewa Made Sri Arsa<sup>a2</sup></span></p>
<p><span class="font2"><sup>a</sup>Department of Information Technology, Udayana University</span></p>
<p><span class="font2">Badung, Indonesia</span></p>
<p><span class="font2"><sup>b</sup>Faculty of Electrical Engineering and Computing, University of Zagreb</span></p>
<p><span class="font2">Zagreb, Croatia</span></p>
<p><a href="mailto:1nmadeprastha@gmail.com"><span class="font2"><sup>1</sup>nmadeprastha@gmail.com</span></a><span class="font2"> </span><span class="font1" style="font-variant:small-caps;">(c</span><span class="font1">orresponding author) </span><a href="mailto:2adi.nurhadiyatna@fer.hr"><span class="font2"><sup>2</sup>adi.nurhadiyatna@fer.hr</span></a></p>
<p><a href="mailto:3dewamsa@unud.ac.id"><span class="font2"><sup>3</sup>dewamsa@unud.ac.id</span></a></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font2" style="font-style:italic;">Hand signature is one of the human characteristics that humans have since birth, which can be used for identity recognition. A high accuracy signature recognition is needed to identify the correct owner of the signature. This study presents signature identification using a combination method between Deep Learning and Euclidean Distance. This study uses three different signature datasets are used in this study which consists of SigComp2009, SigComp2011, and private dataset. First, signature images are preprocessed using binary image conversion, Region of Interest, and thinning. The preprocessed image then has its feature extracted using DenseNet201 and further identified using Euclidean Distance. Several testing scenarios are also applied to measure proposed method robustness, such as using various Pretrained Deep Learning, dataset augmentation, and dataset split ratio modifiers. The best accuracy achieved is 99.44%, with a high precision rate.</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font2" style="font-style:italic;">Hand Signature, SigComp2009, SigComp2011, Thinning, Region of Interest, Identification, Deep Learning, Euclidean Distance</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h3></li></ul>
<p><span class="font2">Signature is human identifier biometrics that is well known and recognized as a tool for identifying a person [1]. A signature is a handwriting or hand stroke with a unique writing style, such as a line of stroke that resembles the name of the signature owner or symbol used as proof of an individual's identity. The signature was recognized as a biometric feature after UNCITRAL established the first digital signature law in the early 90s.</span></p>
<p><span class="font2">Signature Recognition can be classified into two main groups, which consist of online signature and offline signature. Online signature recorded by using touch screens panels like smartphones or tablets. The recorded signature then has its feature extracted, such as pressure points and the path or steps taken while creating the signature. Offline signature only needs scanning process on the signature image and remove the needed feature based on the scanned image [2].</span></p>
<p><span class="font2">Offline Signature identification is considered more difficult than online signature since offline signature does not have a dynamic feature that is present on online signature [1]. Offline signatures depend only on the capture signature shape available from the signature image, while online signatures can use various features such as pressure points and velocity of the drawn signature [3].</span></p>
<p><span class="font2">Signature is used as identity when making a transaction on an online market or e-commerce. Signature is also used as an attendance mark on the high amount of workspace, which is why research on signature identification has recently gotten a lot of attention. Various methods are used to identify a signature, such as research [3] conducted using binary image conversion and image morphology which consist of erosion and dilation as image preprocessing. Convolutional</span></p>
<p><span class="font2">Neural Network is used as both training and identification methods. This study offers a 92% accuracy average as the final result using the dataset from SigComp2011.</span></p>
<p><span class="font2">This study used Convolutional Neural networks as feature extraction and identification methods. Convolutional Neural Network is also used in this study [4], where the study used median filter, extracted signature line, and centering as image preprocessing. The highest result achieved in this study is 73% accuracy in predicting grayscale signature by using 7:3 training data and testing split data ratio.</span></p>
<p><span class="font2">Study [5] conducted a study using a random forest classifier to identify handwritten signature and binary image conversion as image preprocessing. This study also implemented various classification methods by using the SigComp2009 Dataset. The highest accuracy obtained is 60%. The problem in this study is that the proposed method is too flexible and has a high chance of false results.</span></p>
<p><span class="font2">Study [6] used combination methods for signature recognition, such as Principal Components Analysis (PCA) as feature extraction and Euclidean Distance as classification methods. Image preprocessed by using Gray Level Thresholding. Study [6] achieved a 95% accurate result. This is achieved by using a private dataset that consists of two writer classes. The dataset used is too small and need more writer classes</span></p>
<p><span class="font2">Six years later, Study [7] continued the previous study [6] and conducted a similar study using different methods and datasets. This study used ten writer classes as its dataset and used gray scaling as image preprocessing. The preprocessed image then has its dimension changed into 100x100 px and 50x50px, which further has its feature extracted using Gray Level Co-Occurrence (GLCM). The extracted feature is used as an identification process with Euclidean Distance. Study [7] obtained 67.5% accuracy as the study's highest result by splitting the dataset by 3:2 ratio of training and testing data. This study still needs further improvement for a better result, both on the feature extraction process and the amount of dataset used.</span></p>
<p><span class="font2">The proposed study used the combined method from previous studies, starting from image preprocessing consisting of image conversion, Region of Interest area, and image thinning. One of the signature dataset used is SigComp2011 that is also used in the study [3], while feature extraction is done by using Pretrained Deep Learning, and image classifier using Euclidean Distance similar to study [6] and [7]. The result of this study is a better performance signature identification system using the combined method from a previous study and its performance in several testing scenarios.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font2" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h3></li></ul>
<p><span class="font2">This study focused on improving system performance by combining several methods mentioned or tested in the previous study. These methods include using DenseNet201 as a feature extraction method while using Euclidean Distance as an identification method, with various image preprocessing steps to increase the system's final performance further. The general process of the proposed methods is shown in Figure 1.</span></p>
<div><img src="https://jurnal.harianregional.com/media/73362-1.png" alt="" style="width:154pt;height:272pt;">
<p><span class="font2" style="font-weight:bold;">Figure 1. </span><span class="font2">General Process</span></p>
</div><br clear="all">
<p><span class="font2">Signature Identification was done by doing two separate processes, such as making a training signature feature database and the actual identification process. Both training signatures and testing signatures went through the same image preprocessing and feature extraction. Feature extraction is done by using DenseNet201. Input picture is set into 100x120 pixels, while extracted feature is adjusted into 17280 rows.</span></p>
<div><img src="https://jurnal.harianregional.com/media/73362-2.jpg" alt="" style="width:33pt;height:28pt;">
<p><span class="font4" style="font-weight:bold;">Preprocessed Image</span></p>
</div><br clear="all">
<div>
<p><span class="font0" style="font-weight:bold;">Reshape 100x120</span></p><img src="https://jurnal.harianregional.com/media/73362-3.jpg" alt="" style="width:243pt;height:88pt;">
<p><span class="font4" style="font-weight:bold;">Feature Adjustment</span></p>
<p><span class="font4" style="font-weight:bold;">DenseNet201</span></p>
<p><span class="font2" style="font-weight:bold;">Figure 2. </span><span class="font2">Feature Extraction using DenseNet201</span></p>
</div><br clear="all">
<div>
<p><span class="font0" style="font-weight:bold;">Flatten 17280 Feature</span></p><img src="https://jurnal.harianregional.com/media/73362-4.png" alt="" style="width:12pt;height:23pt;">
<p><span class="font0" style="font-weight:bold;">Extracted Feature</span></p>
</div><br clear="all">
<p><span class="font2">Extracted training signatures feature saved as feature database and as comparison with test signature features for signature identification. The final result will show the predicted signature class or owner.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark6"></a><span class="font2" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Datasets</span></h3></li></ul>
<p><span class="font2">This study used three different datasets that were also used by the previous research, which consist of ICDAR 2009 Signature Verification Competition (SigComp2009) [8], ICDAR 2011 Signature Verification Competition (SigComp2011) [9], and private dataset. Details of used datasets are shown in Table 1.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 1. </span><span class="font2">Dataset Details</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Dataset</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Total Image</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Writer Classes</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">SigComp2009</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">936</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">78</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">SigComp2011</span></p></td><td style="vertical-align:top;">
<p><span class="font2">480</span></p></td><td style="vertical-align:top;">
<p><span class="font2">20</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Private</span></p></td><td style="vertical-align:top;">
<p><span class="font2">750</span></p></td><td style="vertical-align:top;">
<p><span class="font2">50</span></p></td></tr>
</table>
<p><span class="font2">Datasets will be divided into training signatures dataset and testing signatures dataset. The private dataset is divided into ten training signature images and five testing signature images per class. SigComp2009 consists of 4 training signature images and eight testing signature images on each class, while SigComp2011 consists of 15 training signature images and nine testing signature images. Different proportion on the dataset is applied to find out the impact of modified dataset total to system performance.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark8"></a><span class="font2" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Image Preprocessing</span></h3></li></ul>
<p><span class="font2">Both training and testing signature images will go through various image preprocessing methods. Image preprocessing needed to be done since original signature images are affected by the different conditions when captured, such as different lighting and noises from the scanning device [10]. Image preprocessing steps are shown in Figure 3.</span></p><img src="https://jurnal.harianregional.com/media/73362-5.jpg" alt="" style="width:310pt;height:60pt;">
<p><span class="font2" style="font-weight:bold;">Figure 3. </span><span class="font2">Preprocessing Steps</span></p>
<p><span class="font2">The result of image preprocessing is shown in Figure 4. Original signature image (a) will be converted into grayscale image (b), then further converted into a binary image (c). The region of interest (ROI) method is applied to the binary signature image to reduce the background image (d). ROI can remove unused background from the image for the system to do better and faster processing [11].</span></p><img src="https://jurnal.harianregional.com/media/73362-6.jpg" alt="" style="width:409pt;height:53pt;">
<p><span class="font10">Original Image &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Grayscale Image Binary Image &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ROI Image Thinned Image</span></p>
<p><span class="font10">(a) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(d) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(e)</span></p>
<p><span class="font2" style="font-weight:bold;">Figure 4. </span><span class="font2">Image Preprocessing Process</span></p>
<p><span class="font2">Image preprocessing then continued into thinning (e). Thinning is one of the morphological image operations used to remove foreground pixels from the binary image. Thinning can also be defined as reducing the image to some extend and preserved the points needed for image processing [12].</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark10"></a><span class="font2" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Feature Extraction</span></h3></li></ul>
<p><span class="font2">The preprocessed image gets its feature extracted using Pretrained Deep Learning. Pretrained Deep Learning is a series of neural networks used to classify the object. Pretrained Deep Learning is also called Transfer Learning and can save time since researchers do not need to train the models from scratch like traditional Convolutional Neural networks (CNN) [13]. CNN consists of neural networks with untrained weights and bias, which makes CNN take longer time to do the identification process [14]. There are various Pretrained Deep Learning architecture model, such as Inception [15], Xception [16], VGG [17], ResNet [18], MobileNet [19], and DenseNet [20]. The numbers behind Pretrained Deep Learning architecture said behind the model used to show the</span></p>
<p><span class="font2">value of layers used, as, in DenseNet201, the model architecture used is DenseNet [20]. It has 201 layers of deep Convolutional Neural Network.</span></p>
<p><span class="font2">Preferred Pretrained Deep Learning model must be loaded first so the model can extract the feature within images. Feature Extraction steps are shown in Figure 5.</span></p>
<div><img src="https://jurnal.harianregional.com/media/73362-7.jpg" alt="" style="width:290pt;height:52pt;">
<p><span class="font2" style="font-weight:bold;">Figure 5. </span><span class="font2">Feature Extraction Steps</span></p>
</div><br clear="all">
<p><span class="font2">The output of feature extraction is a difference based on the initial input. Training signature images will get their feature extracted and saved on a single folder as a csv file, which will be used in the identification process. Testing signature images will get their feature extracted and directly compared with saved training signature features.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark12"></a><span class="font2" style="font-weight:bold;"><a name="bookmark13"></a>2.4. &nbsp;&nbsp;&nbsp;Signature Identification</span></h3></li></ul>
<p><span class="font2">Testing signature image feature will be compared to training signature feature that has been saved. Euclidean Distance will be used to calculate the similarity between both features. The identification process will be shown in Figure 6.</span></p><img src="https://jurnal.harianregional.com/media/73362-8.jpg" alt="" style="width:299pt;height:133pt;">
<p><span class="font2" style="font-weight:bold;">Figure 6. </span><span class="font2">Identification Process</span></p>
<p><span class="font2">Euclidean Distance is a method to calculate distance between 2 points. Euclidean Distance draws a straight line between these 2 points [7] Euclidean Distance equation used in this study is shown below.</span></p>
<div>
<p><span class="font2">(1)</span></p>
</div><br clear="all">
<p><span class="font9" style="font-style:italic;font-variant:small-caps;">d</span><span class="font7" style="font-style:italic;"> -</span><span class="font10"> √ (^</span><span class="font6">ι</span><span class="font10"><sup>-</sup> ^</span><span class="font6">1</span><span class="font10">)<sup>2</sup> + (^</span><span class="font6">∑</span><span class="font10"><sup>-</sup> ^</span><span class="font6">2</span><span class="font10">)<sup>2</sup>+∙ ■ ■ ■ +(^</span><span class="font6">n</span><span class="font10"><sup>-</sup> ^</span><span class="font6">n</span><span class="font10">)<sup>2</sup></span></p>
<p><span class="font2">Equation (1) is a multidimensional Euclidean Distance calculation. The equation is used if two compared points have an n-dimension vector. D is the value of Euclidean Distance, while X and Y represent the vector value of two points being compared respectively. Lower Euclidean Distance value means the compared points or data have high similarity. The predicted signature class will be shown as the lowest distance of the respective training signature feature class.</span></p>
<p><span class="font2">Proposed methods performance is also measured by using Receiver Operation Characteristic (ROC). ROC shows the value of False Acceptance Rate (FAR) and False Rejection Rate (FRR) on a graph.</span></p>
<div>
<p><span class="font8" style="font-style:italic;">FAR</span><span class="font8"> -</span></p>
</div><br clear="all">
<p><span class="font10" style="font-style:italic;">Total value of signature image identified as wrong writer class</span></p>
<div>
<p><span class="font2">(2)</span></p>
</div><br clear="all">
<div>
<h2><a name="bookmark14"></a><span class="font8" style="font-style:italic;"><a name="bookmark15"></a>FRR</span><span class="font8"> =</span></h2>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">Total value of rejected signature image total test images</span></p>
</div><br clear="all">
<div>
<p><span class="font2">(3)</span></p>
</div><br clear="all">
<div>
<p><span class="font2">False Rejection Rate (FRR) is calculated by dividing the rejected signature image (false negative) value by total test images. Image is rejected if the result value is not within the threshold.</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">GAR = 1- FRR</span></p>
</div><br clear="all">
<div>
<p><span class="font2">(4)</span></p>
</div><br clear="all">
<div>
<p><span class="font2">FRR is used to calculate the Genuine Acceptance Rate value (GAR). Genuine Acceptance Rate (GAR) is the percentage value of a signature that is identified correctly [21].</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/73362-9.jpg" alt="" style="width:266pt;height:114pt;">
<p><span class="font2" style="font-weight:bold;">Figure 7. </span><span class="font2">ROC Graph</span></p>
</div><br clear="all">
<p><span class="font2">Figure 6 shows the Receiver Operation Characteristic (ROC) Graph. Intersections of FAR and FRR is called Equal Error Rate (EER).</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark16"></a><span class="font2" style="font-weight:bold;"><a name="bookmark17"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h3></li></ul>
<p><span class="font2">This study conducts several tests on different scenarios to measure the robustness and real case problems. The proposed method is tested using augmented training images, different ratio applications on the used dataset, and comparing three datasets mentioned in section 2.1.</span></p>
<p><span class="font2">The first test is conducted using the SigComp2011 [9] dataset to evaluate each Pretrained Deep Learning model. There are 11 Pretrained Deep Learning used on this test, which is shown in Table 2.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 2. </span><span class="font2">Pretrained Deep Learning Trial Result</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Pretrained Deep Learning</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Feature Extraction Time Used (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Identification</span></p>
<p><span class="font2">Time Used (s)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Accuracy</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Xception</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">21.84</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">79.64</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">73.68%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">VGG19</span></p></td><td style="vertical-align:top;">
<p><span class="font2">15.16</span></p></td><td style="vertical-align:top;">
<p><span class="font2">22.59</span></p></td><td style="vertical-align:top;">
<p><span class="font2">96.59%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">VGG16</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">15.00</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">22.19</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">66.48%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">ResNet50</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">24.71</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">106.15</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">96.59%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">MobileNetV2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">19.19</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">46.57</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">93.75%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">MobileNet</span></p></td><td style="vertical-align:top;">
<p><span class="font2">15.72</span></p></td><td style="vertical-align:top;">
<p><span class="font2">36.75</span></p></td><td style="vertical-align:top;">
<p><span class="font2">98.86%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">InceptionV3</span></p></td><td style="vertical-align:top;">
<p><span class="font2">28.89</span></p></td><td style="vertical-align:top;">
<p><span class="font2">34.02</span></p></td><td style="vertical-align:top;">
<p><span class="font2">76.14%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">InceptionResNetV2</span></p></td><td style="vertical-align:top;">
<p><span class="font2">46.69</span></p></td><td style="vertical-align:top;">
<p><span class="font2">48.36</span></p></td><td style="vertical-align:top;">
<p><span class="font2">78.41%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">DenseNet201</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">57.59</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">90.26</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">99.43%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">DenseNet169</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">39.52</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">92.05%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">DenseNet121</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">31.92</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">51.76</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96.02%</span></p></td></tr>
</table>
<p><span class="font2">Based on Table 2, VGG16, VGG19, and MobileNet offer a much shorter time used on feature extraction and identification steps. Both VGG only needs 38 seconds to finish the identification process, while MobileNet is not far behind, with the time required is 51 seconds. This result varies from each Pretrained Deep Learning architecture because the value of networks on those models differs.</span></p>
<p><span class="font2">For accuracy, the best Pretrained Deep Learning to use for this study is DenseNet201, which has 99.43% accuracy. MobileNet and VGG are not far behind, with 98.86% and 96.59% accuracy values, respectively. DenseNet201 provides the best result as DenseNet architecture has additional inputs from all preceding layers, making the network compact and thinner. This is beneficial since the signature dataset used is not a high-resolution image.</span></p>
<p><span class="font2">The next test is to add several distance-based measurements to compare with DenseNet201 as a feature extraction method. The distance method used are Manhattan Distance, Minkowski Distance, and Cosine Distance.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 3. </span><span class="font2">Augmented Image Result</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Distance Method</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Accuracy</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Euclidean</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99.43%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Manhattan</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">97.73%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Minkowski</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99.43%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Cosine</span></p></td><td style="vertical-align:top;">
<p><span class="font2">99.43%</span></p></td></tr>
</table>
<p><span class="font2">The test result shows that only Manhattan Distance has a different result. This result is because Manhattan Distance is only optimized for integer calculation, while the extracted feature has a float number.</span></p>
<p><span class="font2">The third test is using the Augmentation Dataset, which consists of brightness and rotation modification. These augmentations are used because these are the most relevant on signature real case problems. Brightness modifications have five values between the range of 0.5 to 0.9 of the original image brightness, while rotation modifications have ten values between the range of -10 to 10. The original dataset used on this test is SigComp2011 [9].</span></p>
<p><span class="font2" style="font-weight:bold;">Table 4. </span><span class="font2">Augmented Image Result</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Augmentation</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Total Training Image</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Accuracy</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Brightness</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">298</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99.43%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Rotation</span></p></td><td style="vertical-align:top;">
<p><span class="font2">1800</span></p></td><td style="vertical-align:top;">
<p><span class="font2">86.93%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Brightness + Rotation</span></p></td><td style="vertical-align:top;">
<p><span class="font2">37800</span></p></td><td style="vertical-align:top;">
<p><span class="font2">85.23%</span></p></td></tr>
</table>
<p><span class="font2">The test result on augmented training signature image is underwhelming since its lower than the normal test result, not to mention the amount of time consumed to augment the images and extract its feature. The highest accuracy was achieved by brightness augmentation, which gives a 99.43% accuracy value, the same as the highest accuracy achieved on the normal dataset.</span></p>
<p><span class="font2">The fourth test is to modify the split data ratio. Ratio split is used on all signature images of the used datasets to divide signature images into training data and testing data. The range of ratio split starts from 0.1 to 0.9, with a 0.1 increase value on each iteration. The dataset used on this test is a private dataset consisting of 400 images in 50 writer classes, while the Pretrained Deep Learning model used DenseNet201. This test is carried to find out the effect of different value data training and data testing used on system performance.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 5. </span><span class="font2">Data Split Result</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Split Ratio</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Total Training Image</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Total Testing Image</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Accuracy</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">0.7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">280</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">120</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99.17%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">0.8</span></p></td><td style="vertical-align:top;">
<p><span class="font2">320</span></p></td><td style="vertical-align:top;">
<p><span class="font2">80</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100.00%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">0.9</span></p></td><td style="vertical-align:top;">
<p><span class="font2">360</span></p></td><td style="vertical-align:top;">
<p><span class="font2">40</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100.00%</span></p></td></tr>
</table>
<p><span class="font2">As Table 5 shown, the higher the training signature image ratio is used, the higher accuracy grows. But the accuracy results do not prove that higher training images offer higher accuracy. In this test, the incorrectly identified signature testing images are moved into training images as the ratio increases, affecting the accurate result.</span></p>
<p><span class="font2">The final test is a comparison to the various dataset, which mentioned in section 2.1. This test evaluates the proposed method's performance on different datasets with different intraclass and interclass signatures values.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 6. </span><span class="font2">Datasets Detail</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Dataset</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Writer Classes</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Total Training Image</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Total Testing Image</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">SigComp2009</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">78</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">8</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">SigComp2011</span></p></td><td style="vertical-align:top;">
<p><span class="font2">20</span></p></td><td style="vertical-align:top;">
<p><span class="font2">15</span></p></td><td style="vertical-align:top;">
<p><span class="font2">4</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Private</span></p></td><td style="vertical-align:top;">
<p><span class="font2">50</span></p></td><td style="vertical-align:top;">
<p><span class="font2">10</span></p></td><td style="vertical-align:top;">
<p><span class="font2">5</span></p></td></tr>
</table>
<p><span class="font2">Table 6 show the detail of multiple datasets that used in this study. SigComp2009 has 78 writer classes which consist of 4 training signature images and eight testing signature images, while SigComp2011 has 20 writer classes which consist of 10 training signature images and four testing images. The private dataset has 50 classes and consists of 10 training signature images and five testing signature images.</span></p><img src="https://jurnal.harianregional.com/media/73362-10.jpg" alt="" style="width:211pt;height:64pt;">
<p><span class="font5">threshold</span></p>
<p><span class="font10">^^^^^M FAR ^^^^^» FRR</span></p>
<p><span class="font2" style="font-weight:bold;">Figure 8 </span><span class="font2">SigComp2009 ROC</span></p>
<p><span class="font2">Figure 8 represent SigComp2009 Receiver Operation Characteristic (ROC) graph. ROC shows False Acceptance Rate (FAR) and False Rejection Rate (FRR), and the intersections point of FAR and FRR, which is called Equal Error Rate (EER). EER from SigComp2009 dataset is obtained on threshold 76 with 0,089 value and Genuine Acceptance Rate acquired is 91%.</span></p><img src="https://jurnal.harianregional.com/media/73362-11.jpg" alt="" style="width:239pt;height:93pt;">
<p><span class="font5">threshold</span></p>
<p><span class="font10">^^^^^^ FAR ^^^^^^ FRR</span></p>
<p><span class="font2" style="font-weight:bold;">Figure 9 </span><span class="font2">SigComp2011 ROC</span></p>
<p><span class="font2">Figure 9 represents the SigComp2011 ROC graph. Equal Error Rate is obtained on threshold 90 with 0,0057 value, and Genuine Acceptance Rate acquired is 99%. SigComp2011 has better results compared to SigComp2009 since SigComp2011 has fewer writer classes.</span></p><img src="https://jurnal.harianregional.com/media/73362-12.jpg" alt="" style="width:239pt;height:93pt;">
<p><span class="font5">threshold</span></p>
<p><span class="font10">«■■■■■M FAR ^^^^^^^ FRR</span></p>
<p><span class="font2" style="font-weight:bold;">Figure 10 </span><span class="font2">Private Dataset ROC</span></p>
<p><span class="font2">Figure 10 represents the Private Dataset ROC graph. Equal Error Rate is obtained on threshold 61 with 0,09 value, and Genuine Acceptance Rate acquired is 91%.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 7. </span><span class="font2">Multiple Dataset Result</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font2">Dataset</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Threshold</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">EER</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">GAR</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">SigComp2009</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">76</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">0.089</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">91%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">SigComp2011</span></p></td><td style="vertical-align:top;">
<p><span class="font2">90</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0.0057</span></p></td><td style="vertical-align:top;">
<p><span class="font2">99%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Private</span></p></td><td style="vertical-align:top;">
<p><span class="font2">61</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0.09</span></p></td><td style="vertical-align:top;">
<p><span class="font2">91%</span></p></td></tr>
</table>
<p><span class="font2">Table 7 represents the result of the signature identification test using the Receiver Operation Characteristic (ROC) approach. SigComp2011 dataset has 99% Genuine Acceptance Rate (GAR) value, while both SigComp20009 and Private dataset has GAR with 91% value. This result shows that the number of classes, used training, and testing signature images significantly impact identification accuracy.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark18"></a><span class="font2" style="font-weight:bold;"><a name="bookmark19"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h3></li></ul>
<p><span class="font2">This study proposed an offline signature identification using combination methods between Pretrained Deep Learning and Euclidean Distance. Pretrained Deep Learning is used as feature extraction, while Euclidean Distance is used as an identification method. Various Pretrained Deep Learning such as DenseNet, Inception, ResNet, VGG, Xception, and MobileNet are evaluated as a comparison for finding the best result. Several scenarios of testing are also conducted to measure the robustness of the proposed method in various conditions.</span></p>
<p><span class="font2">The highest accuracy was measured using DenseNet201 as a feature extraction method, which gives a 99.43% accuracy value. This Pretrained Deep Learning is also used on other databases, such as Sigcomp2009 and private databases. The result of the test using those databases are both 91.00%</span></p>
<h3><a name="bookmark20"></a><span class="font2" style="font-weight:bold;"><a name="bookmark21"></a>References</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;H. Saikia and K. Chandra Sarma, “Approaches and Issues in Offline Signature Verification System,” </span><span class="font2" style="font-style:italic;">International Journal of Computer Applications</span><span class="font2">, vol. 42, no. 16, pp. 45–52, Mar. 2012, doi: 10.5120/5780-8035.</span></p></li>
<li>
<p><span class="font2">[2] &nbsp;&nbsp;&nbsp;M. Taskiran and Z. G. Cam, “Offline signature identification via HOG features and artificial neural networks,” in </span><span class="font2" style="font-style:italic;">2017 IEEE 15th International Symposium on Applied Machine Intelligence and Informatics (SAMI)</span><span class="font2">, Jan. 2017, pp. 000083–000086, doi: 10.1109/SAMI.2017.7880280.</span></p></li>
<li>
<p><span class="font2">[3] &nbsp;&nbsp;&nbsp;M. A. Djoudjai, Y. Chibani, and N. Abbas, “Offline signature identification using the histogram of symbolic representation,” </span><span class="font2" style="font-style:italic;">2017 5th International Conference on Electrical Engineering -Boumerdes (ICEE-B)</span><span class="font2">, vol. 2017-Janua, pp. 1–6, 2017, doi: 10.1109/ICEE-B.2017.8192092.</span></p></li>
<li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;T. Sultan Rana, H. Muhammad Usman, and S. Naseer, “Static Handwritten Signature</span></p></li></ul>
<p><span class="font2">Verification Using Convolution Neural Network,” </span><span class="font2" style="font-style:italic;">3rd International Conference on Innovative Computing (ICIC)</span><span class="font2">, no. Icic, 2019, doi: 10.1109/ICIC48496.2019.8966696.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;M. Thenuwara and H. R. K. Nagahamulla, “Offline handwritten signature verification system using random forest classifier,” </span><span class="font2" style="font-style:italic;">17th International Conference on Advances in ICT for Emerging Regions (ICTer) &nbsp;2017</span><span class="font2">, &nbsp;vol. 2018-Janua, pp. 191–196, &nbsp;2017, doi:</span></p></li></ul>
<p><span class="font2">10.1109/ICTER.2017.8257828.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;E. Utami and R. Wulanningrum, “Use of Principal Component Analysis and Euclidean Distance to Identify Signature Image,” </span><span class="font2" style="font-style:italic;">Iptek-Kom</span><span class="font2">, vol. 16, no. 1, pp. 1–16, 2014, [Online]. Available: </span><a href="https://jurnal.kominfo.go.id/index.php/iptekkom/article/viewFile/505/327"><span class="font2">https://jurnal.kominfo.go.id/index.php/iptekkom/article/viewFile/505/327</span></a><span class="font2">.</span></p></li>
<li>
<p><span class="font2">[7] &nbsp;&nbsp;&nbsp;G. D. Angel and R. Wulanningrum, “Machine Learning untuk Identifikasi Tanda Tangan Menggunakan GLCM dan Euclidean Distance,” </span><span class="font2" style="font-style:italic;">Prosiding SEMNAS INOTEK (Seminar Nasional Inovasi Teknologi)</span><span class="font2">, pp. 297–301, 2020.</span></p></li>
<li>
<p><span class="font2">[8] &nbsp;&nbsp;&nbsp;V. L. Blankers, C. E. Van Den Heuvel, K. Y. Franke, and L. G. Vuurpijl, “The ICDAR 2009 signature verification competition,” </span><span class="font2" style="font-style:italic;">Proceeding 10th International Conference on Document Analysis and Recognition, ICDAR</span><span class="font2">, pp. 1403–1407, 2009, doi: 10.1109/ICDAR.2009.216.</span></p></li>
<li>
<p><span class="font2">[9] &nbsp;&nbsp;&nbsp;M. Liwicki </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, “Signature verification competition for online and offline skilled forgeries (SigComp2011),” </span><span class="font2" style="font-style:italic;">Proceeding International Conference on Document Analysis and Recognition, ICDAR</span><span class="font2">, pp. 1480–1484, 2011, doi: 10.1109/ICDAR.2011.294.</span></p></li>
<li>
<p><span class="font2">[10] &nbsp;&nbsp;&nbsp;X. Yan, L. Wen, L. Gao, and M. Perez-Cisneros, “A Fast and Effective Image Preprocessing Method for Hot Round Steel Surface,” </span><span class="font2" style="font-style:italic;">Mathematical Problems in Engineering</span><span class="font2">, vol. 2019, 2019, doi: 10.1155/2019/9457826.</span></p></li>
<li>
<p><span class="font2">[11] &nbsp;&nbsp;&nbsp;A. H. Pratomo, W. Kaswidjanti, and S. Mu’arifah, “Implementasi Algoritma Region of Interest ( ROI ) Untuk Meningkatkan Performa Algoritma Deteksi Dan Klasifikasi Kendaraan,” </span><span class="font2" style="font-style:italic;">Jurnal Teknologi Informasi dan Ilmu Komputer</span><span class="font2">, vol. 7, &nbsp;no. 1, pp. 155–162, 2020, doi:</span></p></li></ul>
<p><span class="font2">10.25126/jtiik.202071718.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[12] &nbsp;&nbsp;&nbsp;Abhisek and K. Lakshmesha, “Thinning approach in digital image processing,” </span><span class="font2" style="font-style:italic;">Last</span></p></li></ul>
<p><span class="font2" style="font-style:italic;">Accessed April</span><span class="font2">, pp. 326–330, 2018.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[13] &nbsp;&nbsp;&nbsp;A. Foroozandeh, A. Askari Hemmat, and H. Rabbani, “Offline Handwritten Signature</span></p></li></ul>
<p><span class="font2">Verification and Recognition Based on Deep Transfer Learning,” </span><span class="font2" style="font-style:italic;">International Conference on Machine Vision and Image Processing. MVIP</span><span class="font2">, &nbsp;vol. 2020-Janua, &nbsp;2020, doi:</span></p>
<p><span class="font2">10.1109/MVIP49855.2020.9187481.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[14] &nbsp;&nbsp;&nbsp;I. M. Mika Parwita and D. Siahaan, “Classification of Mobile Application Reviews using Word Embedding and Convolutional Neural Network,” </span><span class="font2" style="font-style:italic;">Lontar Komputer Jurnal Ilmiah Teknologi Informasi</span><span class="font2">, vol. 10, no. 1, p. 1, 2019, doi: 10.24843/lkjiti.2019.v10.i01.p01.</span></p></li>
<li>
<p><span class="font2">[15] &nbsp;&nbsp;&nbsp;J. A. Gliner, G. A. Morgan, N. L. Leech, J. A. Gliner, and G. A. Morgan, “Measurement Reliability and Validity,” </span><span class="font2" style="font-style:italic;">Research Methods in Applied Settings</span><span class="font2">, pp. 319–338, 2021, doi: 10.4324/9781410605337-29.</span></p></li>
<li>
<p><span class="font2">[16] &nbsp;&nbsp;&nbsp;S.-H. Tsang, “No Title,” </span><span class="font2" style="font-style:italic;">Review: Xception - With Depthwise Separabale Convolution, Better THan Inception-V3</span><span class="font2">, 2018. review: Xception - With Depthwise Separabale Convolution, Better THan Inception-V3 (accessed May 18, 2021).</span></p></li>
<li>
<p><span class="font2">[17] &nbsp;&nbsp;&nbsp;O. Sudana, I. W. Gunaya, and I. K. G. D. Putra, “Handwriting identification using deep convolutional &nbsp;neural network method,” &nbsp;</span><span class="font2" style="font-style:italic;">Telkomnika (Telecommunication Computing</span></p></li></ul>
<p><span class="font2" style="font-style:italic;">Electronics and Control)</span><span class="font2">, vol. 18, no. 4, pp. 1934–1941, &nbsp;&nbsp;2020, doi:</span></p>
<p><span class="font2">10.12928/TELKOMNIKA.V18I4.14864.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[18] &nbsp;&nbsp;&nbsp;K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” </span><span class="font2" style="font-style:italic;">Proceeding IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="font2">, vol. 2016-Decem, pp. 770–778, 2016, doi: 10.1109/CVPR.2016.90.</span></p></li>
<li>
<p><span class="font2">[19] &nbsp;&nbsp;&nbsp;Y. Harjoseputro, I. P. Yuda, and K. P. Danukusumo, “MobileNets: Efficient Convolutional Neural Network for Identification of Protected Birds,” </span><span class="font2" style="font-style:italic;">International Journal on Advanced Science, Engineering and Information Technology</span><span class="font2">, vol. 10, no. 6, pp. 2290–2296, 2020, doi: 10.18517/ijaseit.10.6.10948.</span></p></li>
<li>
<p><span class="font2">[20] &nbsp;&nbsp;&nbsp;G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” </span><span class="font2" style="font-style:italic;">Proceeding - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</span><span class="font2">, vol. 2017-Janua, pp. 2261–2269, &nbsp;2017, doi:</span></p></li></ul>
<p><span class="font2">10.1109/CVPR.2017.243.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[21] &nbsp;&nbsp;&nbsp;Y. Adiwinata, A. Sasaoka, I. P. Agung Bayupati, and O. Sudana, “Fish Species Recognition with Faster R-CNN Inception-v2 using QUT FISH Dataset,” </span><span class="font2" style="font-style:italic;">Lontar Komputer Jurnal Ilmiah Teknologi Informasi</span><span class="font2">, vol. 11, no. 3, p. 144, 2020, doi: 10.24843/lkjiti.2020.v11.i03.p03.</span></p></li></ul>
<p><span class="font2">111</span></p>