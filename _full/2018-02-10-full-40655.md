---
layout: full_article
title: "ASALTAG : Automatic Image Annotation Through Salient Object Detection and Improved k-Nearest Neighbor Feature Matching"
author: "Theresia Hendrawati, Duman Care Khrisne"
categories: jeei
canonical_url: https://jurnal.harianregional.com/jeei/full-40655 
citation_abstract_html_url: "https://jurnal.harianregional.com/jeei/id-40655"
citation_pdf_url: "https://jurnal.harianregional.com/jeei/full-40655"  
comments: true
---

<p><span class="font8">Journals of Electrical, Electronics and Informatics, p-ISSN: 2549-8304</span></p>
<p><span class="font8">6</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font10"><a name="bookmark1"></a>ASALTAG : Automatic Image Annotation Through Salient Object Detection and Improved k-Nearest Neighbor Feature Matching</span></h1>
<p><span class="font9">Theresia Hendrawati<sup>2</sup>, Duman Care Khrisne<sup>1</sup></span></p>
<ul style="list-style:none;"><li>
<p><span class="font8"><sup>1</sup> &nbsp;&nbsp;&nbsp;Computer Science Graduate Program Ganesha University of Education (UNDIKSHA) Singaraja, Bali, Indonesia </span><a href="mailto:theresiahendrawati@gmail.com"><span class="font8">theresiahendrawati@gmail.com</span></a></p></li>
<li>
<p><span class="font8"><sup>2</sup> &nbsp;&nbsp;&nbsp;Electrical Engineering Department, Faculty of Engineering Udayana University (UNUD) Badung, Bali, Indonesia </span><a href="mailto:duman@unud.ac.id"><span class="font8">duman@unud.ac.id</span></a></p></li></ul>
<p><span class="font8" style="font-weight:bold;">Abstract</span><span class="font8">—Image databases are becoming very large nowadays, and there is an increasing need for automatic image annotation, for assiting on finding the desired specific image. In this paper, we present a new approach of automatic image annotation using salient object detection and improved k-Nearest Neigbor classifier named ASALTAG. ASALTAG is consist of three major part, the segmentation using Minimum Barirer Salienct Region Segmentation, feature extraction using Block Truncation Algorithm, Gray Level Co-occurrence Matrix and Hu’ Moments, the last part is classification using improved k-Nearest Neigbor. As the result we get maximum accuracy of 79.56% with k=5, better than earlier research. It is because the saliency object detection we do before the feature extraction proccess give us more focused object in image to annotate. Normalization of the feature vector and the distance measure that we use in ASALTAG also improve the kNN classifier accuracy for labeling image.</span></p>
<p><span class="font7" style="font-weight:bold;font-style:italic;">Index Terms</span><span class="font7" style="font-weight:bold;">—ASALTAG, Automatic Image Annotation, k-Nearest Neighbor, Salient Region.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font8"><a name="bookmark3"></a>I.</span><span class="font8" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Introduction<sup>1</sup></span></h2></li></ul>
<p><span class="font8">Nowadays, image databases are becoming very large and there is an increasing need for automatic tools to automatically annotate and organize image databases. For example average people upload 350 million photos to Facebook per day [1]. The number of images, have impact on the difficulty of people in finding the desired specific image. Information Retrieval researcher using two approach to retrieve or manage such large quantities of images. One using text-based approach and the other use content-based approach. This two approach have its own advantages and disadvantages. To take advantage of each approach the Automatic Image Annotation (AIA) emerge, to automatically annotate each test image with keywords by training a statistical model on a labeled training set [2], [3].</span></p>
<p><span class="font8">In this paper we try to automaticaly annotate image using baseline approach porposed in [3], and attempted to improve labeling accuracy from what is done in [4]. We</span></p>
<p><span class="font8">first do image segmentation to obtain the object of the image, to do this we use Minimum Barier Detection (MBD) [5]. The salient region extracted from image will be treated as the object to be labeled. Next step is to extract the feature of the image, we use color feature extraction using Block Truncation Algorithm (BTA) [6], the texture feature we use Gray Level Co-occurrence Matrix (GLCM) [7] and for shape feature we use Hu’s Moments [8]. Arguably, one of the simplest annotation schemes is to treat the problem of annotation as that of image-retrieval [3]. One can find image nearest neighbor defined by distance feature measure, from the training set, and assign all the keywords of the nearest image to the input test image, so we decide to made feature vector of these image feature and then we use k-NN to clasify image to find it’s label, we named the system as ASALTAG. The main contributions of ASALTAG is to introduce new approach to automatically annotate image, which label come from the most salient region of image or the object.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font8"><a name="bookmark5"></a>II.</span><span class="font8" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Related Work</span></h2></li></ul>
<p><span class="font8">A number of approach have been proposed for automatic image annotation and retrieval. Among all the proposed approach or models, nearest neighbor are shown to be most successful. Some models successfully used the nearest neighbor, some of them are [3], [4], [9], [10]. Makadia et al. [3] provided the baseline for image annotation based on the nearest neighbor. In [4] Khrisne and Putra use expanded image color-feature by using Block Truncation Algorithm proposed in [6], and with other image feature, they use K-NN to label image. TagProp [9] allows the integration of metric learning by directly maximizing the log-likelihood of the tag predictions in the training set. In this manner, they can optimally combine a collection of image similarity metrics that cover different aspects of image content, such as local shape descriptors, or global color histograms.</span></p>
<p><span class="font8">In nearest neighbor approach the image feature vector are play important role, they usually used as the characteristic of the image to be classified. Although many methods use the nearest neighbor approach, but the image features are usually obtained from the whole picture as in [3], [4], [6], [9], [10]. Study on image salient region [5], [11] says</span></p>
<p><span class="font8">people pay more attention to the image area that is very contrast with the surrounding environment (salient). If features extracted directly from whole image, then the image feature will contain a lot of information with less meaning, because humans tend to pay attention only to the dominant object in the image. So image salient region segmentation is needed for more accurate label of what people see at the image.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font8"><a name="bookmark7"></a>III.</span><span class="font8" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Proposed Approach</span></h2></li></ul>
<p><span class="font8">The proposed work for ASALTAG in this paper is consist of three major part. The segmentation (A), Feature Extraction (B, C, D) and Classification (E).</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">A. &nbsp;&nbsp;&nbsp;Salient Region Segmentattion</span></p></li></ul>
<p><span class="font8">One of the ASALTAG novelty is the image salient region segmentation, before doing feature extraction, using Minimum Barrier Distance (MBD) Transform. In this paper the image features are not obtained from the whole picture, but from part of the image. The part of image is the most salient region or the one that most viewer see as the image’s object. For that we use a highly efficient, yet powerful, salient object detection method proposed in [5]. To make saliency map [5] are using Image Boundary Connectivity without using super-pixel representation, for better speed performance. They use MBD to measure image boundary connectivity, and Fast-MBD for better result. We adopt the algoritmh of their system.</span></p>
<p><span class="font8">Given an input image:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">1. &nbsp;&nbsp;&nbsp;For each channel in the Lab color space:</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font8">a. Apply Minimum Barrier Distance transform to compute MBD maps, which measure image boundary connectivity of each pixel</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font8">2. &nbsp;&nbsp;&nbsp;Average the MBD maps of the color channels</span></p></li>
<li>
<p><span class="font8">3. &nbsp;&nbsp;&nbsp;Apply postprocessing to improve the saliency map quality for object segmentation</span></p></li>
<li>
<p><span class="font8">4. &nbsp;&nbsp;&nbsp;Optionally, we can further enhance the saliency map by leveraging the Backgroundness cue at a moderately increased cost. Backgroundness assumes image boundary regions are mostly background.</span></p></li></ul>
<p><span class="font8">After getting saliency map, we make a saliency binary map by thresholding the saliency map. By getting binary map, we can get the results of the image segmentation by multiplying the binary map with the original image.</span></p>
<p><span class="font8">Apart from the segmentation, the feature extraction method play important role in ASALTAG. ASALTAG are not using a local feature or descriptor because as sugested above, we try to simplify the problem of annotation as of image-retreival in [12]. Global feature are known to be perform better for this task [13]. Whe choose color, texture and shape feature for getting numerical representation of segmented image using method as followed B to D feature extraction part.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">B. &nbsp;&nbsp;&nbsp;Block Truncation Algorithm (BTA)</span></p></li></ul>
<p><span class="font8">The BTA is an algorithm to extract the color features from an image. Image is divided by the color components R, G and B, the mean of each color component becomes the value for separating the color components into two H and L. Where H for pixels in an image that has a value higher than the average pixel in a color component and L for pixels in an image that has a value lower than the average pixel value in a color component. Thus the color of an image forms 6 groups of RH, RL, GH, GL, BH and BL. The moments of this group are the color features of the BTA. Stricker and Orengo [14], use three central moments of an image's color distribution Mean, Standard deviation and Skewness. In (1), (2) and (3), pk</span><span class="font5">ij </span><span class="font8">is the value of the </span><span class="font8" style="font-style:italic;">k</span><span class="font8">-th color component of the ij-image pixel and </span><span class="font8" style="font-style:italic;">P</span><span class="font8"> is the height of the image, and </span><span class="font8" style="font-style:italic;">Q</span><span class="font8"> is the width of the image. In ASALTAG whe only use two moment, they are Mean (E</span><span class="font5">k</span><span class="font8">) and Standart Deviation (SD</span><span class="font5">k</span><span class="font8">), because we don’t want the shape of color distribution, the other feature such texture and shape will replace it.</span></p>
<p><span class="font8">Moment 1 - Mean :</span></p>
<p><span class="font8" style="font-style:italic;">p &lt;1</span></p>
<p><span class="font8">^=⅛∑∑<sup>p</sup>^ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)</span></p>
<p><span class="font6" style="font-weight:bold;">i=Lj=l</span></p>
<div>
<p><span class="font8">Moment 2 - Standart Deviation</span></p><img src="https://jurnal.harianregional.com/media/40655-1.jpg" alt="" style="width:120pt;height:43pt;">
</div><br clear="all">
<div>
<p><span class="font8">(2)</span></p>
</div><br clear="all">
<p><span class="font8">After the extraction process we will get 2 × 6 = 12 color feature.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">C. &nbsp;&nbsp;&nbsp;Gray Level Co-occurrence Matrix (GLCM)</span></p></li></ul>
<p><span class="font8">GLCM is an image-texture feature extraction technique initiated by [7], it has been utilized as the main tool in image texture analysis in many research paper according to [15], so does in ASALTAG. Haralick suggested statistics</span></p>
<p><span class="font8">equations that can be calculated from the co-occurrence matrix and be used in describing the image texture. To create a co-occurrence matrix, we need a matrix consisting of reference pixels and neighboring pixels, with distance </span><span class="font8" style="font-style:italic;">d </span><span class="font8">and angle </span><span class="font1" style="font-weight:bold;">θ</span><span class="font8">, for angel x-axis is the reference for 0</span><span class="font1" style="font-weight:bold;">° </span><span class="font8">and increasing every 45</span><span class="font1" style="font-weight:bold;">° </span><span class="font8">counter-clockwise. Fig. 1 show the configuration of working matrix.</span></p>
<table border="1">
<tr><td colspan="2" rowspan="2" style="vertical-align:top;"></td><td colspan="4" style="vertical-align:top;">
<p><span class="font8">neighbor</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font8">2</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font8">N</span></p></td></tr>
<tr><td rowspan="4" style="vertical-align:middle;">
<p><span class="font8">Q</span></p>
<p><span class="font8">O</span></p>
<p><span class="font8">S</span></p>
<p><span class="font8">P</span></p>
<p><span class="font8">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1,1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">1,2</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8">1,N</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font8">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">2,1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8">2,2</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8">2,N</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font8">N</span></p></td><td style="vertical-align:top;">
<p><span class="font8">N<sup>…</sup>,1</span></p></td><td style="vertical-align:top;">
<p><span class="font8">N<sup>…</sup>,2</span></p></td><td style="vertical-align:middle;">
<p><span class="font8">...</span></p></td><td style="vertical-align:middle;">
<p><span class="font8">N,N</span></p></td></tr>
</table>
<p><span class="font6">Fig. 1. The configuration of working matrix</span></p>
<p><span class="font8">In order to estimate the similarity between different gray level co-occurrence matrices, Haralick [7] proposed 14 statistical features extracted from them. To reduce the computational complexity, only some of these features were selected. The description of 4 most relevant features that are widely used in literature [16], they are Energy (</span><span class="font8" style="font-style:italic;">Er</span><span class="font8">), Entropy (</span><span class="font8" style="font-style:italic;">En</span><span class="font8">), Contrast (</span><span class="font8" style="font-style:italic;">Ct</span><span class="font8">) and Homogeneity (Hg). Equations equations (4) through (7), show us how the texture feature is extracted from the co-occurrence matrix P, with i and j representing the column and row of the matrix P element’s and M and N represent number of matrix’s coloumn and row.</span></p>
<p><span class="font8">Texture Feature 1 - Energy :</span></p>
<p><span class="font5" style="font-weight:bold;font-style:italic;">M N</span></p>
<p><span class="font8">(4)</span></p>
<p><span class="font6">L=Dj=D</span></p>
<p><span class="font8">Texture Feature 2 - Entropy :</span></p>
<p><span class="font5" style="font-weight:bold;font-style:italic;">M N</span></p>
<p><span class="font8">(5)</span></p>
<p><span class="font6">L=Dj=D</span></p>
<p><span class="font8">Texture Feature 3 - Contrast : </span><span class="font5" style="font-weight:bold;font-style:italic;">M N</span></p>
<p><span class="font8">(6)</span></p>
<p><span class="font6">L = D j = D</span></p>
<p><span class="font8">Texture Feature 4 - Homogeneity :</span></p>
<p><span class="font8">(7)</span></p>
<p><span class="font1" style="font-weight:bold;">I = Dj=O</span></p>
<p><span class="font8">In ASALTAG the feature of co-occurrence matrix P is obtained using distance </span><span class="font8" style="font-style:italic;">d</span><span class="font8"> = 1 to 3 and angle </span><span class="font5" style="font-weight:bold;font-style:italic;">θ</span><span class="font8"> of 0</span><span class="font1" style="font-weight:bold;">°</span><span class="font8">, 45</span><span class="font1" style="font-weight:bold;">°</span><span class="font8">, 90</span><span class="font1" style="font-weight:bold;">° </span><span class="font8">and 135</span><span class="font1" style="font-weight:bold;">°</span><span class="font8">, after feature extraction we get 3 (distance) × 4 (angle) × 4 (feature) = 48 texture feature.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">D. &nbsp;&nbsp;&nbsp;Hu’s Momment</span></p></li></ul>
<p><span class="font8">The moment invariants were first introduced by Hu [8].</span></p>
<p><span class="font8">Hu moments algorithm is chosen to extract image's shape features since the generated features are rotation, scale and translation invarian. Hu defined seven values, calculated by normalizing central moments completed order three that are invariant to object scale, position, and orientation. In terms of the central moments, the Hu’s seven moments are given as shown in (8).</span></p>
<p><span class="font1" style="font-weight:bold;">0ι = M<sub>20</sub> <sup>+</sup> M<sub>02</sub></span></p>
<p><span class="font1" style="font-weight:bold;">0<sub>2</sub> = (M<sub>20</sub>-M<sub>02</sub>)<sup>2 + 4</sup>Mn</span></p>
<p><span class="font1" style="font-weight:bold;">0<sub>3</sub><sup>=</sup> (mo<sup>- 3</sup>Mi<sub>2</sub>)<sup>2 +</sup> </span><span class="font1" style="font-weight:bold;font-variant:small-caps;">(<sup>3</sup>m<sub>2</sub>i<sup>-</sup> m<sub>03</sub>)<sup>2</sup></span></p>
<p><span class="font2" style="font-style:italic;">Φ<sub>i</sub></span><span class="font1" style="font-weight:bold;"> = (m<sub>0</sub>+M<sub>22</sub>)<sup>2</sup> + G⅛ι + </span><span class="font1" style="font-weight:bold;font-variant:small-caps;">m<sub>03</sub>)<sup>2</sup></span></p>
<p><span class="font1" style="font-weight:bold;">05 = </span><span class="font1" style="font-weight:bold;font-variant:small-caps;">(Mso <sup>- 3</sup>Mi<sub>2</sub>)(m<sub>2</sub>i</span><span class="font1" style="font-weight:bold;"> — ⅛a)<sup>2</sup>[⅛jo<sup>+</sup>Mιι)<sup>j</sup>^3 ⅛21 <sup>+ </sup>⅛a)<sup>2</sup>) <sup>+</sup> ^O<sup>i</sup>21 <sup>-</sup> </span><span class="font1" style="font-weight:bold;font-variant:small-caps;">M<sub>03</sub>)(m<sub>2</sub>i</span><span class="font1" style="font-weight:bold;"> <sup>+</sup> M<sub>0</sub>a)I<sup>3</sup>(M<sub>30</sub><sup>+</sup> Mi<sub>2</sub>)<sup>2 -</sup></span></p>
<p><span class="font1" style="font-weight:bold;">(<sup>3</sup>M<sub>2</sub>I <sup>+</sup> M<sub>03</sub>)<sup>2</sup>]</span></p>
<p><span class="font8">(8)</span></p>
<p><span class="font1" style="font-weight:bold;">06 <sup>=</sup> ⅛20<sup>-</sup>∕<sup>i</sup>02)l </span><span class="font1" style="font-weight:bold;font-variant:small-caps;">(M<sub>3o</sub> +</span><span class="font1" style="font-weight:bold;"> <sup>3</sup>Ml<sub>2</sub>)<sup>2-</sup> 3 ⅛<sub>2</sub>ι + M<sub>0</sub>s)<sup>2</sup>] —</span></p>
<p><span class="font1" style="font-weight:bold;"><sup>4</sup>MuM<sub>30</sub> + Mi<sub>2</sub>Gz<sub>2</sub>I + M<sub>03</sub>'<sup>1</sup></span></p>
<p><span class="font1" style="font-weight:bold;">07= <sup>3</sup>Gz<sub>2</sub>I- Moa)+ (Mao<sup>+</sup>Mi<sub>2</sub>)I(Mso<sup>+</sup>Mi<sub>2</sub>)<sup>2</sup>- <sup>3</sup> Gz<sub>2</sub>I <sup>+ </sup>M<sub>02</sub>)<sup>2</sup>] — </span><span class="font1" style="font-weight:bold;font-variant:small-caps;">(Mso“ <sup>3</sup>Mi<sub>2</sub>) + (m<sub>2</sub>i — Mo<sub>2</sub>)I<sup>3</sup>(m<sub>2</sub>o<sup>+</sup> Mi<sub>2</sub>)<sup>2</sup> —</span></p>
<p><span class="font1" style="font-weight:bold;">Gz<sub>2</sub>ι + M<sub>03</sub>)<sup>2</sup>]</span></p>
<p><span class="font8">With Hu’s seven moment, ASALTAG get 7 addition feature to be the image characteristics. Now the image’s salient segmentation have a 12 + 48 + 7 = 67 feature as deskriptor.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">E. &nbsp;&nbsp;&nbsp;Improved K-Nearest Neighbor</span></p></li></ul>
<p><span class="font8">K-nearest-neighbor (kNN) classification is one of the most fundamental and simple classification methods and should be one of the first choices for a classification study when there is little or no prior knowledge about the distribution of the data. K-nearest-neighbor classification was developed from the need to perform discriminant analysis when reliable parametric estimates of probability densities are unknown or difficult to determine [17]. The kNN algorithm as in [4] are:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">1. &nbsp;&nbsp;&nbsp;Start.</span></p></li>
<li>
<p><span class="font8">2. &nbsp;&nbsp;&nbsp;Input : Training data, labels for training data, k value, test data.</span></p></li>
<li>
<p><span class="font8">3. &nbsp;&nbsp;&nbsp;Calculate distance of test data to each training data.</span></p></li>
<li>
<p><span class="font8">4. &nbsp;&nbsp;&nbsp;Select the training data k which is closest to the test data.</span></p></li>
<li>
<p><span class="font8">5. &nbsp;&nbsp;&nbsp;Check the label of the k-nearest training data.</span></p></li>
<li>
<p><span class="font8">6. &nbsp;&nbsp;&nbsp;Determine the label of test data with the most frequent label.</span></p></li>
<li>
<p><span class="font8">7. &nbsp;&nbsp;&nbsp;Stop.</span></p></li></ul>
<p><span class="font8">Differ from [4] that using euclidean-distance for calculating distance between two feature vector, in ASALTAG we decided to improve the distance algorithm. It is because the number of features that we use, the distance is considered as a multi-dimensional distance. The feature vector also have to be normalized first before we calculate the similarity or distance between two feature vector. The distance we use is calculated by (9) proposed by [18].</span></p>
<div><img src="https://jurnal.harianregional.com/media/40655-2.jpg" alt="" style="width:111pt;height:29pt;">
</div><br clear="all">
<div>
<p><span class="font8">(9)</span></p>
</div><br clear="all">
<p><span class="font8">purpose we use random sampling for every category. with proportion of 67% as training data and 33% as test data for each category. The category and number of image data in dataset can be seen on Table 1.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font8"><a name="bookmark9"></a>IV.</span><span class="font8" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Result</span></h2></li></ul>
<p><span class="font8">To implement ASALTAG we use python 2.7 programming language. For image dataset we use the same image dataset with [4], [19], [20] it is Corel Image Dataset. From the dataset we filter the image category and create 11 category with label in Bahasa Indonesia. It is because of the ASALTAG approach, we only take the image from objek-type image in dataset. Fig. 2. Show us when the feature is extracted from single image. As a lot of image consist in one category, we use batch processing for every image on one category, before that we have to set the label for every image in one category, as seen in Fig. 3.</span></p><img src="https://jurnal.harianregional.com/media/40655-3.jpg" alt="" style="width:244pt;height:281pt;">
<p><span class="font6">Fig. 2. Feature extraction proccess for single image</span></p>
<p><span class="font0">L⅛’Python 2.7,12 Shell’ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;□ X</span></p>
<p><span class="font0">File Edit Shell Debug Options Window Help</span></p>
<p><span class="font4">Python 2.7.12 (v2.7.12:d33eθcf91556, Jun 27 2016, 15:19:22} [NSC v.1500 32 bit ( </span><span class="font4" style="text-decoration:underline;">’ </span><span class="font4">Intel}] on Win32</span></p>
<p><span class="font4">Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license(}&quot; for more information.</span></p>
<p><span class="font4">RESTART: D:\FILE PASCA ICHA</span><span class="font3">∖</span><span class="font4">SEMESTER III</span><span class="font3">∖</span><span class="font4">ΞOFT COMPUTING</span><span class="font3">∖</span><span class="font4">KODE</span><span class="font3">∖</span><span class="font4">BatchProcess.py</span></p>
<p><span class="font4">Label Gambar ? Kuda</span></p>
<p><span class="font4">Warning (from warnings module):</span></p>
<p><span class="font4">File &quot;C:\Python27\lib\site-pacJcages\slcimage\util\dtype.py&quot;, line 122</span></p>
<p><span class="font4">.format(dtypeobj_in, dtypeobj_out))</span></p>
<p><span class="font4">UserWarning: Possible precision loss when converting from float64 to uintS</span></p>
<p><span class="font4">D:</span><span class="font3">∖</span><span class="font4">Test</span><span class="font3">∖</span><span class="font4">700.jpg</span></p>
<p><span class="font4">D:</span><span class="font3">∖</span><span class="font4">Test</span><span class="font3">∖</span><span class="font4">701.jpg</span></p>
<p><span class="font4">D:</span><span class="font3">∖</span><span class="font4">Test</span><span class="font3">∖</span><span class="font4">702.jpg</span></p>
<p><span class="font4">D:</span><span class="font3">∖</span><span class="font4">Test</span><span class="font3">∖</span><span class="font4">703.jpg</span></p>
<p><span class="font4">D:</span><span class="font3">∖</span><span class="font4">Test</span><span class="font3">∖</span><span class="font4">704.jpg</span></p>
<p><span class="font6">Fig. 3. Batch proccessing feature extraction image category</span></p>
<p><span class="font8">In total we extract 1925 image for its feature. For testing</span></p>
<p><span class="font6">TABLE I</span></p>
<p><span class="font8" style="font-variant:small-caps;">I</span><span class="font6" style="font-variant:small-caps;">mage </span><span class="font8" style="font-variant:small-caps;">C</span><span class="font6" style="font-variant:small-caps;">ategory and </span><span class="font8" style="font-variant:small-caps;">D</span><span class="font6" style="font-variant:small-caps;">ataset</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font6">No.</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Label for Category</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Number of Image in Dataset</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-style:italic;">Akordion</span><span class="font6"> (Accordion)</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">49</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">2</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Pesawat Terbang</span><span class="font6"> (Airplane)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">57</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">3</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Wajah Manusia</span><span class="font6"> (Human Faces)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">435</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">4</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Bunga Mawar</span><span class="font6"> (Roses)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">5</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Dinosaurus</span><span class="font6"> (Dinosaur)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">97</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">6</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Bus</span><span class="font6"> (Bus)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">7</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Kucing</span><span class="font6"> (Cat)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">8</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Fitness</span><span class="font6"> (Fitnes)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">200</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">9</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Mobil</span><span class="font6"> (Car)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">410</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">10</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Pintu</span><span class="font6"> (Door)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">277</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">11</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-style:italic;">Kuda</span><span class="font6"> (Horses)</span></p></td><td style="vertical-align:top;">
<p><span class="font6">100</span></p></td></tr>
</table>
<p><span class="font8">We do about 12 times, kNN classification with random test set, to get the mean accuracy of our system. Fig. 4 show us the process of kNN classification process. We change the value of </span><span class="font8" style="font-style:italic;">k</span><span class="font8"> with 3, 5, 7 and do classification test on data test four times for each </span><span class="font8" style="font-style:italic;">k</span><span class="font8"> value configuration of kNN. The result of this test can be seen on Table 2.</span></p>
<p><span class="font6">TABLE II</span></p>
<p><span class="font6">ASALTAG </span><span class="font5" style="font-style:italic;">K</span><span class="font6">NN </span><span class="font8" style="font-variant:small-caps;">L</span><span class="font6" style="font-variant:small-caps;">abeling </span><span class="font8" style="font-variant:small-caps;">A</span><span class="font6" style="font-variant:small-caps;">ccuracy</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font6">No.</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Training Set</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Test Set</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-style:italic;">k</span><span class="font6">-value</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Mean</span></p>
<p><span class="font6">Acc for </span><span class="font6" style="font-style:italic;">k </span><span class="font6">value</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font6">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">1286</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">638</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">78.84%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1280</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">644</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">78.10%</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font6">77.70%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">3</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1265</span></p></td><td style="vertical-align:top;">
<p><span class="font6">659</span></p></td><td style="vertical-align:top;">
<p><span class="font6">3</span></p></td><td style="vertical-align:top;">
<p><span class="font6">76.32%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1265</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">659</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">77.54%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1294</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">630</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">78.73%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">6</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1283</span></p></td><td style="vertical-align:top;">
<p><span class="font6">641</span></p></td><td style="vertical-align:top;">
<p><span class="font6">5</span></p></td><td style="vertical-align:top;">
<p><span class="font6" style="font-weight:bold;">79.56%</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">78.17%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font6">7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">1282</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">642</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">75.70%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1299</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">625</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">78.72%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font6">9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">1260</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">664</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">78.46%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font6">10</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">1283</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">641</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">78.62%</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font6">78.15%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">11</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1263</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">661</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">77.15%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">12</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1318</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">606</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">78.38%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:top;"></td><td colspan="2" style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">Mean Accuracy</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">78.01%</span></p></td><td style="vertical-align:top;"></td></tr>
</table>
<p><span class="font8">Our proposed method in ASALTAG is showing a better accuracy, than what they do in [4], they have maximium-accuracy of 73.26% compared to our maximum-accuracy with 79.56%. however with different k-value our system get average Accuracy of 78.01%. For the k-value, we found that the accuracy is better with </span><span class="font8" style="font-style:italic;">k</span><span class="font8"> = 5.</span></p>
<p><span class="font8">To get better understanding of our saliency segmentation approach with our ASALTAG expanded kNN, compared to kNN without normalization and using euclidean distance (proposed in [4] and we call it normal kNN) as their similarity check, we do kNN test classification for random test set from same dataset and same scenario, to get the mean accuracy of normal kNN. The result of this test can be seen on Table 3.</span></p>
<p><span class="font8">From Tabel 2 and Table 3, we found out that ASALTAG kNN accuracy is better that the accuracy of normal kNN. Our way to change kNN distance measure and the</span></p>
<p><span class="font8">normalisation of the dataset, give us an improvement of kNN way to classify image on dataset. We think the saliency segmentation approach give us more accuracy, as we can see in Table 3. we get better accuracy result than overall accuracy than other earlier research result.</span></p>
<p><span class="font6">TABLE III</span></p>
<table border="1">
<tr><td colspan="6" style="vertical-align:top;">
<p><span class="font8" style="font-style:italic;font-variant:small-caps;">N</span><span class="font6" style="font-style:italic;font-variant:small-caps;">ormal k</span><span class="font8" style="font-variant:small-caps;">NN</span><span class="font6"> L</span><span class="font5">ABELING </span><span class="font6">A</span><span class="font5">CCURACY</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">No.</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Training Set</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Test Set</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-style:italic;">k</span><span class="font6">-value</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Mean</span></p>
<p><span class="font6">Acc for </span><span class="font6" style="font-style:italic;">k </span><span class="font6">value</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1304</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">620</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">69.02%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1302</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">622</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">62.54%</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font6">64.65%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">3</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1266</span></p></td><td style="vertical-align:top;">
<p><span class="font6">658</span></p></td><td style="vertical-align:top;">
<p><span class="font6">3</span></p></td><td style="vertical-align:top;">
<p><span class="font6">67.47%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">4</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1300</span></p></td><td style="vertical-align:top;">
<p><span class="font6">624</span></p></td><td style="vertical-align:top;">
<p><span class="font6">3</span></p></td><td style="vertical-align:top;">
<p><span class="font6">63.94%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1283</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">641</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">65.05%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">6</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1246</span></p></td><td style="vertical-align:top;">
<p><span class="font6">678</span></p></td><td style="vertical-align:top;">
<p><span class="font6">5</span></p></td><td style="vertical-align:top;">
<p><span class="font6">67.84%</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font6">65.75%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">7</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1246</span></p></td><td style="vertical-align:top;">
<p><span class="font6">678</span></p></td><td style="vertical-align:top;">
<p><span class="font6">5</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62.83%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1285</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">639</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">67.29%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">9</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1313</span></p></td><td style="vertical-align:top;">
<p><span class="font6">611</span></p></td><td style="vertical-align:top;">
<p><span class="font6">7</span></p></td><td style="vertical-align:top;">
<p><span class="font6">67.26%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font6">10</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">1291</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">633</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">66.35%</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">67.54%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">11</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">1289</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">635</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">68.66%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">12</span></p></td><td style="vertical-align:top;">
<p><span class="font6">1276</span></p></td><td style="vertical-align:top;">
<p><span class="font6">648</span></p></td><td style="vertical-align:top;">
<p><span class="font6">7</span></p></td><td style="vertical-align:top;">
<p><span class="font6">67.90%</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:top;"></td><td colspan="2" style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">Mean Accuracy</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">66.10%</span></p></td><td style="vertical-align:top;"></td></tr>
</table><img src="https://jurnal.harianregional.com/media/40655-4.jpg" alt="" style="width:241pt;height:161pt;"><img src="https://jurnal.harianregional.com/media/40655-5.jpg" alt="" style="width:241pt;height:161pt;">
<p><span class="font6">Fig. 3. kNN classification process</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font8"><a name="bookmark11"></a>V.</span><span class="font8" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font8">After some test we found that our proposed approach in ASALTAG is getting better accuracy than what the [4] do, using the same classification technique, kNN. It is due to saliency object detection before the feature extraction proccess. Another value we found is, normalization of the feature vector and the distance measure that we use in</span></p>
<p><span class="font8">ASALTAG kNN improve the kNN classifier to get better accuracy for labeling image.</span></p>
<p><span class="font8">For future research we suggest the use of generatif model to increase accuracy, also we got information of the global and local descriptor combined together can be realy useful for image autolabel.</span></p>
<h2><a name="bookmark12"></a><span class="font8" style="font-variant:small-caps;"><a name="bookmark13"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font6">[1] &nbsp;&nbsp;&nbsp;V. N. Murthy, E. F. Can, and R. Manmatha, “A Hybrid Model for Automatic Image Annotation,” in Proceedings of International Conference on Multimedia Retrieval. ACM, 2014.</span></p></li>
<li>
<p><span class="font6">[2] &nbsp;&nbsp;&nbsp;M. Ames and M. Naaman, “Why We Tag : Motivations for Annotation in Mobile and Online Media,” Proc. SIGCHI Conf. Hum. factors Comput. Syst. ACM, 2007.</span></p></li>
<li>
<p><span class="font6">[3] &nbsp;A. Makadia, V. Pavlovic, and S. Kumar, “Baselines for Image</span></p></li></ul>
<p><span class="font6">Annotation,” Int. J. Comput. Vis., vol. 1, no. 90, pp. 88–105, 2010.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font6">[4] &nbsp;D. C. Khrisne and D. Putra, “Automatic Image Annotation</span></p></li></ul>
<p><span class="font6">Menggunakan Block Truncation dan K-Nearest Neighbor” Lontar Komputer, vol. 4, no. 1, pp. 224–230, 2013.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font6">[5] &nbsp;&nbsp;&nbsp;J. Zhang, S. Sclaroff, Z. Lin, X. Shen, and B. Price, “Minimum Barrier Salient Object Detection at 80 FPS,” in Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1404– 1412.</span></p></li>
<li>
<p><span class="font6">[6] &nbsp;&nbsp;&nbsp;S. Silakari, M. Motwani, and M. Maheshwari, “Color Image Clustering using Block Truncation Algorithm,” Int. J. Comput. Sci. Issues, vol. 4, no. 2, pp. 2–6, 2009.</span></p></li>
<li>
<p><span class="font6">[7] &nbsp;&nbsp;&nbsp;R. M. Haralick, K. Shanmugam, and I. Dinstein, “Haralick-TexturalFeatures.pdf,” IEEE Trans. Syst. Man Cybern., vol. SMC-3, no. 6, pp. 610–621, 1973.</span></p></li>
<li>
<p><span class="font6">[8] &nbsp;&nbsp;&nbsp;M. K. Hu, “Visual Pattern Recognition by Moment Invariants,” IRE Trans. Inf. Theory, vol. 8, no. 2, pp. 179–187, 1962.</span></p></li>
<li>
<p><span class="font6">[9] &nbsp;&nbsp;&nbsp;M. Guillaumin, T. Mensink, J. Verbeek, C. Schmid, and L. J. Kuntzmann, “TagProp : Discriminative Metric Learning in Nearest Neighbor Models for Image Auto-Annotation,” in Computer Vision, 2009 IEEE 12th International Conference, 2009, pp. 309–316.</span></p></li>
<li>
<p><span class="font6">[10] &nbsp;&nbsp;&nbsp;L. Wu, R. Jin, and A. K. Jain, “Tag Completion for Image Retrieval,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 3, pp. 716–727, 2013.</span></p></li>
<li>
<p><span class="font6">[11] &nbsp;&nbsp;&nbsp;W. Zhu, S. Liang, Y. Wei, and J. Sun, “Saliency Optimization from Robust Background Detection,” in IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 2814–2821.</span></p></li>
<li>
<p><span class="font6">[12] &nbsp;&nbsp;&nbsp;D. C. Khrisne and M. D. Yusanto, “Content-Based Image Retrieval Menggunakan Metode Block Truncation Algorithm dan Grid Partitioning”, S@ CIES vol. 5, no. 2, pp. 79-85, 2015.</span></p></li>
<li>
<p><span class="font6">[13] &nbsp;&nbsp;&nbsp;C. Singh and P. Sharma, “Performance analysis of various local and global shape descriptors for image retrieval,” Multimed. Syst., vol. 19, no. 4, pp. 339–357, 2013.</span></p></li>
<li>
<p><span class="font6">[14] &nbsp;&nbsp;&nbsp;M. Stricker and M. Orengo, “Similarity of Color Images,” in Storage and Retrieval for Image and Video Databases (SPIE), 1995, pp. 381– 392.</span></p></li>
<li>
<p><span class="font6">[15] &nbsp;&nbsp;&nbsp;H. Y. Chai, L. K. Wee, T. T. Swee, S. H. Salleh, and A. K. Ariff, “Gray-Level Co-occurrence Matrix Bone Fracture Detection,” Am. J. Appl. Sci., vol. 8, no. 1, p. 26, 2011.</span></p></li>
<li>
<p><span class="font6">[16] &nbsp;&nbsp;&nbsp;M. Partio, B. Cramariuc, M. Gabbouj, and A. Visa, “Rock Texture Retrieval using Gray Level Co-occurrence Matrix,” in Proc. of 5th Nordic Signal Processing Symposium, 2002.</span></p></li>
<li>
<p><span class="font6">[17] &nbsp;&nbsp;&nbsp;Peterson, Leif E. &quot;K-nearest neighbor.&quot; Scholarpedia 4.2 (2009): 1883.</span></p></li>
<li>
<p><span class="font6">[18] &nbsp;&nbsp;&nbsp;Golub, G. H. and Van Loan, C. F. “Matrix Computations”, Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15.</span></p></li>
<li>
<p><span class="font6">[19] &nbsp;&nbsp;&nbsp;D. Tao, X. Li, and S. J. Maybank, “Negative Samples Analysis in Relevance Feedback,” IEEE Transactions on Knowledge and Data Engineering (TKDE), vol. 19, no. 4, pp. 568-580, April 2007</span></p></li>
<li>
<p><span class="font6">[20] &nbsp;&nbsp;&nbsp;Wang, J. Z., Li, J., and Wiederhold, G., “SIMPLIcity: SemanticsSensitive Integrated Matching for Picture Libraries,” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 23, no. 9, pp. 947-963, September 2001.</span></p></li></ul>