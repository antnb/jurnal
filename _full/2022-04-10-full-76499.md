---
layout: full_article
title: "Kekarangan Balinese Carving Classification Using Gabor Convolutional Neural Network"
author: "I Putu Bagus Gede Prasetyo Raharja, I Made Suwija Putra, Tony Le"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-76499 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-76499"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-76499"  
comments: true
---

<p><span class="font4" style="font-weight:bold;">LONTAR KOMPUTER VOL. 13, NO. 1 APRIL 2022</span></p>
<p><span class="font4" style="font-weight:bold;">DOI : 10.24843/LKJITI.2022.v13.i01.p01</span></p>
<p><span class="font4" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 158/E/KPT/2021</span></p>
<p><span class="font4" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font4" style="font-weight:bold;">e-ISSN 2541</span><span class="font4">-</span><span class="font4" style="font-weight:bold;">5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font5" style="font-weight:bold;"><a name="bookmark1"></a>Kekarangan Balinese Carving Classification Using Gabor Convolutional Neural Network</span></h1>
<p><span class="font4">I Putu Bagus Gede Prasetyo Raharja<sup>a1</sup>, I Made Suwija Putra<sup>a2</sup> , Tony Le<sup>b3</sup></span></p>
<p><span class="font7">a</span></p>
<p><span class="font4">Department of InformationTechnology, Udayana University</span></p>
<p><span class="font4">Bukit Jimbaran, Bali, Indonesia </span><a href="mailto:1prasetyo.raharja@student.unud.ac.id"><span class="font1"><sup>1</sup></span><span class="font4">prasetyo.raharja@student.unud.ac.id</span></a><span class="font4"> </span><span class="font2">(Corresponding author)</span></p>
<p><span class="font4"><sup>2</sup> </span><a href="mailto:putrasuwija@unud.ac.id"><span class="font4">putrasuwija@unud.ac.id</span></a></p>
<p><span class="font4"><sup>b</sup> Department of Electrical and Computer Engineering, University of New Orleans 2000 Lakeshore Dr, New Orleans, LA70148, United States</span></p>
<p><a href="mailto:3tle2496@gmail.com"><span class="font1"><sup>3</sup></span><span class="font4">tle2496@gmail.com</span></a></p>
<p><span class="font4" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font4" style="font-style:italic;">Balinese traditional carvings are Balinese culture that can easily be found on the island of Bali, starting from the decoration of Hindu temples and traditional Balinese houses. One of the types of Balinese traditional carving ornaments is Kekarangan ornament carving. Apart from the many traditional Balinese carvings, Balinese people only know the shape of the carving without knowing the name and characteristics of the carving itself. Lack of understanding in traditional Balinese carving is caused by the difficulty of finding sources of materials to study traditional Balinese carvings. A traditional Kekarangan Balinese carving classification system can help Balinese people to identify classes of traditional Balinese carving. This study used the Gabor CNN method. The Multi Orientation Gabor Filter is used in feature extraction and image augmentation, coupled with the Convolutional Neural Network method for image classification. The usage of the Gabor CNN method can produce the highest image classification accuracy of 89%.</span></p>
<p><span class="font4" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font4" style="font-style:italic;">Balinese Carving, Convolutional Neural Network, Computer Vision, Gabor Filter, Machine Learning</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font4" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font4">Bali is one of the tourist destinations that still preserve its traditional culture. One of Bali’s traditional cultures is Balinese traditional carving. Balinese traditional carving comes from the imagination abilities and creativity of the carver toward the natural shape. Balinese traditional carvings can be found in Hindus Temple and Balinese traditional houses. One type of Balinese’s traditional carving ornament is </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4">. Animals and plants inspired the shape of the </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> ornaments. </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> carving ornament can consist of several classes of other carving ornaments. The different ability of each carver to interpret natural shape into carving makes </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional carving unique. The classification of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional carving can help Balinese people identify the Balinese traditional carving class despite the uniqueness of each carving. It also creates digital documentation to preserve Balinese’s traditional culture. It can be one of the media for future generations to learn Balinese traditional carving, especially the </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> ornament.</span></p>
<p><span class="font4">The classification of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional carving has been done before by manually identifying the characteristics of lines, dots, shape, and accent of the carving. The pattern classification of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional carving can be performed automatically using computer vision technology and deep learning. The Gabor filter is a feature extractor that has been used in the computer vision field because of its excellent properties in the spatial domain . A Convolutional Neural Network (CNN) is a popular deep learning classification method. Recently, CNN has been shown to carry the process of pattern recognition automatically on a large amount of data with extraordinary power. Our research proposed the CNN method and Gabor Filter to classify three classes of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional carving using 300 RGB images with 256 x 256 shapes each. These three classes are </span><span class="font4" style="font-style:italic;">Karang Gajah</span><span class="font4">, </span><span class="font4" style="font-style:italic;">Karang Daun</span><span class="font4">, and </span><span class="font4" style="font-style:italic;">Karang Goak</span><span class="font4">. We use image augmentation to overcome the limited data problem. The model evaluation of the neural network showed the highest accuracy of 89%.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font4" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Related Works</span></h2></li></ul>
<p><span class="font4">Artificial Intelligence (AI) has been grown rapidly in recent years. It shows that collaboration in the AI field is becoming more common, and the scope of research projects is greater [1]. Convolutional Neural Network method has been successfully applied in some research on computer vision problems, especially in image classification and object detection problems. In this section, we discuss the supporting studies on this research.</span></p>
<p><span class="font4">Rahul Chauhan et al. [2] used simple CNN models with data augmentation to perform image recognition on the MNIST and CIFAR-10 datasets. The research proved that data augmentation could increase the accuracies of the model in a large dataset. The data augmentation techniques used were mirroring, random cropping, rotation, and color shifting. The accuracy of the CNN model on MNIST is 99.6% and on CIFAR-10 is 80.17%.</span></p>
<p><span class="font4">Research by Maha et.al [3] has shown that Computer Vision and Machine Learning methods can use for classifying traditional Balinese carvings. The computer vision method used in the feature extraction step is the HOG feature extraction method and the PCA feature extraction method, both of which will be used as training data with the LVQ method. The results of the recognition of the test dataset obtained an average accuracy of 72.2% and the PCA feature extraction method of 23.67%. The hyperparameters used in the LVQ model are learning rate with a value of 0.001 and epoch with 1000 value.</span></p>
<p><span class="font4">Another research on the classification of traditional Balinese carvings was carried out by Suman-tara et.al [4]. Their research shows the identification method of traditional Balinese carvings using the ORB (Oriented FAST and Rotated BRIEF) feature extraction method and classification using the Hamming Brute Force method. The Oriented FAST and Rotated BRIEF methods on its application to the Bali Carving Recognition application obtains a recognition accuracy of 48%. Classification success is influenced by several factors, such as the quality of the reference image, lighting, and distance.</span></p>
<p><span class="font4">The use of the Gabor Filter feature extraction method and classification method using Deep Learning on a dataset of traditional Balinese carvings has been carried out by Kesiman et.al [5]. The dataset used in the form of traditional Balinese carving images with a number of classes of 18 classes and a total number of data is 258 images. The feature extraction method used is the Gabor Filter plus the Multilayer Perceptron method with the Backpropagation algorithm. The classification model has an accuracy of 43.3%.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font4" style="font-weight:bold;"><a name="bookmark7"></a>3. &nbsp;&nbsp;&nbsp;Research Methods</span></h2></li></ul>
<p><span class="font4">The flow overview of the system is shows on Figure 2. The image datasets used in this research were taken by the researcher with Samsung Galaxy A50 25 MP Main Camera in Gianyar, Badung, and Buleleng district in Bali. The datasets consist of 249 images across three classes. Figure 1 shows image example of 3 class of Kekarangan Balinese traditional carving, consist of </span><span class="font4" style="font-style:italic;">Karang Daun</span><span class="font4"> that resembles flower and leaf, </span><span class="font4" style="font-style:italic;">Karang Gajah</span><span class="font4"> that resembles elephant, and </span><span class="font4" style="font-style:italic;">Karang Goak </span><span class="font4">that resembles bird. From the Table 2 we can see detailed info about the number of images of each class.</span></p><img src="https://jurnal.harianregional.com/media/76499-1.jpg" alt="" style="width:213pt;height:213pt;">
<p><span class="font4" style="font-weight:bold;">Figure 1. </span><span class="font4">Sample images from each carving classes</span></p>
<p><span class="font4" style="font-weight:bold;">Table 1. </span><span class="font4">Image acquisition details</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font4">Class Name</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Number of Images</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4" style="font-style:italic;">Karang Daun</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">86</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4" style="font-style:italic;">Karang Goak</span></p></td><td style="vertical-align:top;">
<p><span class="font4">88</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4" style="font-style:italic;">Karang Gajah</span></p></td><td style="vertical-align:top;">
<p><span class="font4">75</span></p></td></tr>
</table>
<p><span class="font4">In the processing step, the raw image from the camera turned into the Gabor filter feature image dataset. The dataset will be split into the testing dataset, validation dataset, and training dataset. From the dataset, we trained the CNN model to classify the image using the training dataset. Also, validated the step with the validation dataset. The evaluation step evaluated the performance by predicting images from the testing dataset and build a confusion matrix based on the prediction result.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font4" style="font-weight:bold;"><a name="bookmark9"></a>3.1. &nbsp;&nbsp;&nbsp;Image Augmentation</span></h2></li></ul>
<p><span class="font4">The number of datasets given during the training process of CNN model is one of the factor that determine performance of neural network. A simple neural network can learn from a set of images dataset to do task such as character recognition, image segmentation, and image classification [6]. However, it required an enormous amount of labeled data for the model to perform computer vision task with high performance [7]. Data augmentation overcomes the lack of data training by artificially inflating the training set with label-preserving transformations.</span></p>
<p><span class="font4">Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. Transformation carries out in geometric methods such as zoom, flipping, rotation, and cropping schemes, then in photometric methods such as color jittering, edge enhancement, and fancy PCA [8]. New training samples can be created using prior knowledge on transformation-invariance properties to improve the learning of a machine. An attractive characteristic of these augmentation methods is their ability to be combined with multiple image augmentation method [9]. The data augmentation method used in this research was horizontal flip and image rotation with 20</span><span class="font1"><sup>o</sup> </span><span class="font4">and -20</span><span class="font1"><sup>o</sup></span><span class="font4">, whics is the optimal rotation degree that still preserve the label of the data [10]. We will apply data augmentation on the raw dataset until the number of all classes is balanced. From the image augmentation process, we generated total of 300 images with 100 images in each class.</span></p>
<div><img src="https://jurnal.harianregional.com/media/76499-2.jpg" alt="" style="width:213pt;height:311pt;">
<p><span class="font4" style="font-weight:bold;">Figure 2. </span><span class="font4">Proposed system workflow</span></p>
</div><br clear="all">
<p><span class="font4" style="font-weight:bold;">Table 2. </span><span class="font4">Result of Image Augmentation</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font4">Class Name</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Number of Images</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4" style="font-style:italic;">Karang Daun</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4" style="font-style:italic;">Karang Goak</span></p></td><td style="vertical-align:top;">
<p><span class="font4">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4" style="font-style:italic;">Karang Gajah</span></p></td><td style="vertical-align:top;">
<p><span class="font4">100</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font4" style="font-weight:bold;"><a name="bookmark11"></a>3.2. &nbsp;&nbsp;&nbsp;Gabor Filter Feature Extraction</span></h2></li></ul>
<p><span class="font4">Feature extraction is a significant component of object detection and image classification [11]. Gabor filter was used to extract features from the image. Although CNN can perform feature extraction using multiple layers of feature detectors [9], The Gabor filter can help to extract features of some spatial information that cannot be learned. General approaches on feature extraction exist but have problems in practice since they require precise segmentation and uniform lighting. Gabor filter can overcome this limitation since Gabor features can preserve pose information in the feature space [12]. We used the Gabor filter with four directions, namely 0</span><span class="font1"><sup>o</sup></span><span class="font4">, 45</span><span class="font1"><sup>o</sup></span><span class="font4">, 90</span><span class="font1"><sup>o</sup></span><span class="font4">, 134</span><span class="font1"><sup>o</sup></span><span class="font4">, and with 0.1 frequency in this paper. Gabor feature image of each class and each directions is shown at Figure 5. Equation 1 shows the complex Gabor function used to build the Gabor filter feature extractor in this research. From the process of Gabor Filter Feature Extraction, each image generated four different feature images making a total of 1200 images as training and testing dataset.</span></p>
<p><span class="font9" style="font-style:italic;">x<sup>l</sup>2 +</span><span class="font8"> γ</span><span class="font1"><sup>2</sup> </span><span class="font9" style="font-style:italic;">y</span><span class="font1" style="font-style:italic;">'</span><span class="font7" style="font-style:italic;">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font9" style="font-style:italic;">χ'</span></p>
<div>
<p><span class="font4">(1)</span></p>
</div><br clear="all">
<div>
<p><span class="font4">(2)</span></p>
</div><br clear="all">
<p><span class="font9" style="font-style:italic;">g</span><span class="font2" style="font-style:italic;"><sup>(</sup></span><span class="font9" style="font-style:italic;"><sup>x</sup>, y;<sup>λ,</sup> θ<sup>, ψ, σ,</sup></span><span class="font8"> γ</span><span class="font9">) = eχp(--</span><span class="font8">^—) </span><span class="font9"><sup>eχ</sup>p((</span><span class="font8">i</span><span class="font9">(<sup>2</sup></span><span class="font3"><sup>φ</sup> </span><span class="font9">j <sup>+ </sup></span><span class="font3"><sup>ψ</sup></span><span class="font9">))</span></p>
<p><span class="font9" style="font-style:italic;">x' = x</span><span class="font9"> cos </span><span class="font8">θ </span><span class="font9">+ </span><span class="font9" style="font-style:italic;">y</span><span class="font9"> sin </span><span class="font8">θ</span></p>
<div>
<p><span class="font4">(3)</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">y</span><span class="font1" style="font-style:italic;"><sup>0</sup> </span><span class="font9" style="font-style:italic;">= </span><span class="font2" style="font-style:italic;">—</span><span class="font9" style="font-style:italic;">x</span><span class="font9"> sin </span><span class="font9" style="font-style:italic;">θ</span><span class="font9"> + </span><span class="font9" style="font-style:italic;">y</span><span class="font9"> cos </span><span class="font9" style="font-style:italic;">θ</span></p><img src="https://jurnal.harianregional.com/media/76499-3.jpg" alt="" style="width:255pt;height:255pt;">
<p><span class="font4" style="font-weight:bold;">Figure 3. </span><span class="font4">Gabor image result from different filter orientation</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font4" style="font-weight:bold;"><a name="bookmark13"></a>3.3. &nbsp;&nbsp;&nbsp;Transfer Learning</span></h2></li></ul>
<p><span class="font4">Convolutional Neural Network (CNN) is one kind of neural network and development of Multilayer Perceptron (MLP). CNN is an efficient recognition algorithm that widely uses in pattern recognition and image processing. Convolutional Neural Network use Convolution Layer and Pooling Layer to perform feature extraction of an image. Transfer learning is one tool that can solve the problem of insufficient training data while training CNN model [13]. Research in [14] proved that training a small sample of data on a pre-trained model with fine-tuning has a better result, or in the worst case, performed the same as a CNN trained from scratch. Researchers have implemented deep learning models, as VGG16 [15]. MobileNet [16], GoogLeNet [17], and ResNet [18]. We use the MobileNet model to speed up our training process combined by 1024x1024 as a fine-tuning layer and softmax layer with three outputs as a classifier in this research. Compared to other models, MobileNet has excellent performance and is suitable to use on the mobile platform [19].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font4" style="font-weight:bold;"><a name="bookmark15"></a>3.4. &nbsp;&nbsp;&nbsp;Confusion Matrix</span></h2></li></ul>
<p><span class="font4">Confusion matrices can be used to evaluate performance of deep learning model on image classification task. From the confusion matrix result we can analyze and identifying the effectiveness of the classification task on each training and testing class. In this research, we calculated accuracy, precision, recall, and f score metrics based on the confusion matrix. The following equation 4-7 is used to calculate confusion matrix metrics. </span><span class="font8">l </span><span class="font4">is total class, </span><span class="font8">tp </span><span class="font4">is true positive of class, </span><span class="font8">tn </span><span class="font4">is true negative of class, </span><span class="font8">fn </span><span class="font4">is false negative of class, </span><span class="font8">fp </span><span class="font4">is true positive of class , and </span><span class="font8">β </span><span class="font4">is weighted harmonic mean of precision and recall [20].</span></p>
<div>
<p><span class="font1">l</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">AverageAccuracy =</span></p>
</div><br clear="all">
<div>
<p><span class="font7">Σ </span><span class="font1">i</span><span class="font7">=1</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;text-decoration:underline;">tp</span><span class="font6" style="font-style:italic;text-decoration:underline;">i</span><span class="font7" style="font-style:italic;text-decoration:underline;">+tn</span><span class="font6" style="font-style:italic;text-decoration:underline;">i </span><span class="font7" style="font-style:italic;">tp</span><span class="font6" style="font-style:italic;">i</span><span class="font7" style="font-style:italic;">+tn</span><span class="font6" style="font-style:italic;">i</span><span class="font7" style="font-style:italic;">+fp</span><span class="font6" style="font-style:italic;">i</span><span class="font7" style="font-style:italic;">+fn</span><span class="font6" style="font-style:italic;">i</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">l</span></p>
</div><br clear="all">
<div>
<p><span class="font4">(4)</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">Precision</span><span class="font7" style="font-style:italic;">M </span><span class="font9" style="font-style:italic;">=</span></p>
</div><br clear="all">
<div>
<p><span class="font1">l </span><span class="font4"><sup>P</sup></span></p>
<p><span class="font7" style="font-style:italic;">i=1</span></p>
</div><br clear="all">
<div>
<p><span class="font1">tp</span><span class="font0">i </span><span class="font7" style="font-style:italic;">tp</span><span class="font6" style="font-style:italic;">i</span><span class="font7" style="font-style:italic;">+fp</span><span class="font6" style="font-style:italic;">i</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">l</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">Recall</span><span class="font7" style="font-style:italic;">M </span><span class="font9" style="font-style:italic;">=</span></p>
</div><br clear="all">
<div>
<p><span class="font7">Σ </span><span class="font1">i</span><span class="font7">=1</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">tp</span><span class="font0">i </span><span class="font7" style="font-style:italic;">tp</span><span class="font6" style="font-style:italic;">i</span><span class="font7" style="font-style:italic;">+fn</span><span class="font6" style="font-style:italic;">i</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">l</span></p>
</div><br clear="all">
<div>
<p><span class="font4">(5)</span></p>
<p><span class="font4">(6)</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">FScore</span><span class="font7" style="font-style:italic;">M </span><span class="font9" style="font-style:italic;">=</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="font-style:italic;">(β<sup>2</sup></span><span class="font9"> + </span><span class="font9" style="font-style:italic;">1)Precision</span><span class="font7" style="font-style:italic;">M </span><span class="font9" style="font-style:italic;">Recall</span><span class="font7" style="font-style:italic;">M </span><span class="font9" style="font-style:italic;">β </span><span class="font1" style="font-style:italic;"><sup>2</sup></span><span class="font9" style="font-style:italic;">Precision</span><span class="font7" style="font-style:italic;">M</span><span class="font9"> + </span><span class="font9" style="font-style:italic;">Recall</span><span class="font7" style="font-style:italic;">M</span></p>
</div><br clear="all">
<div>
<p><span class="font4">(7)</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font4" style="font-weight:bold;"><a name="bookmark17"></a>3.5. &nbsp;&nbsp;&nbsp;Training and Model Evaluation Scenario</span></h2></li></ul>
<p><span class="font4">In this research, we will conduct six types of training and model evaluation scenarios. The detail of each scenario can be seen in Table 3. We will use two types of the dataset to evaluating the performance of our Gabor CNN model. The first dataset is the dataset that consists of </span><span class="font4" style="font-style:italic;">Kekarangan </span><span class="font4">carving images that resulted from the image augmentation process without adding the Gabor filter feature extraction step. This dataset will be our benchmark model. The second dataset is the dataset that consists of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> carving image that has passed the image augmentation process and Gabor filter feature extraction step. We also fine-tuned our model optimizer function using three optimizers which are Adam, RMSProp, and SGD. We used 50 epoch and 1x10</span><span class="font1">-</span><span class="font9">5 </span><span class="font4">learning rate as hyperparamater for all scenarios. The detail of our training and model evaluation scenario can be seen on Table 7.</span></p>
<p><span class="font4" style="font-weight:bold;">Table 3. </span><span class="font4">Image acquisition details</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font4">Class Name</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Images Acquired</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Benchmark Dataset</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Gabor Dataset</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4" style="font-style:italic;">Karang Daun</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">86 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">100 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">400 images</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4" style="font-style:italic;">Karang Goak</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">88 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">100 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">400 images</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4" style="font-style:italic;">Karang Gajah</span></p></td><td style="vertical-align:top;">
<p><span class="font4">75 images</span></p></td><td style="vertical-align:top;">
<p><span class="font4">100 images</span></p></td><td style="vertical-align:top;">
<p><span class="font4">400 images</span></p></td></tr>
</table>
<p><span class="font4" style="font-weight:bold;">Table 4. </span><span class="font4">Scenario Details</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font4">Scenario</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Dataset</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Optimizer</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Benchmark Datast</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Adam</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4">2</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Bencmark Dataset</span></p></td><td style="vertical-align:top;">
<p><span class="font4">RMSProp</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4">3</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Benchmark Dataset</span></p></td><td style="vertical-align:top;">
<p><span class="font4">SGD</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4">4</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Gabor Datast</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Adam</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Gabor Dataset</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">RMSProp</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4">6</span></p></td><td style="vertical-align:top;">
<p><span class="font4">Gabor Dataset</span></p></td><td style="vertical-align:top;">
<p><span class="font4">SGD</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font4" style="font-weight:bold;"><a name="bookmark19"></a>4. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional classification was conducted on machine using using Intel Core </span><span class="font7">TM </span><span class="font4">i7 2.60 GHz processor, 8 GB RAM, NVIDIA® GeForce® GTX 950M 4GB GDDR5 VRAM , Python 3.7 with Keras and Tensorflow 1.4 library. In this section, we discuss the result of our training and model evaluation scenarios..</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark20"></a><span class="font4" style="font-weight:bold;"><a name="bookmark21"></a>4.1. &nbsp;&nbsp;&nbsp;Benchmark Dataset</span></h2></li></ul>
<p><span class="font4">In this scenario, we conducted training with the dataset consists of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> carving images that resulted from the image augmentation process without adding the Gabor filter feature extraction step. We split the dataset into training, testing, and validation dataset. The details of each</span></p>
<p><span class="font4">dataset can be seen in 7. The highest training obtained by the model is 100%, and the highest validation accuracy of 76% from the model trained with Adam optimizer. Figure 4shows the accuracy and loss graph of every model trained with a benchmark dataset on each epoch. The graph shows that validation accuracy and loss value are not stable. The model with a high or unstable loss value tends to have an unsteady performance on classifying images that not include in the training or validation dataset. To further evaluate our model, we conduct testing step by predicting all images on the testing dataset and analyze the result using a confusion matrix. From the testing step, the model acquired the highest accuracy of 72%, highest recall of 72%, highest precision of 76%, and highest F1 score of 71%.</span></p>
<div><img src="https://jurnal.harianregional.com/media/76499-4.jpg" alt="" style="width:425pt;height:185pt;">
<p><span class="font4" style="font-weight:bold;">Figure 4. </span><span class="font4">Training result graph</span></p>
<p><span class="font4">on benchmark dataset: (a) Adam Optimizer, (b) RMSProp Opti-</span></p>
</div><br clear="all">
<p><span class="font4" style="font-weight:bold;">Table 5. </span><span class="font4">Split Dataset Detail for Benchmark Dataset</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font4">Class Name</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Training</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Validation</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Testing</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4" style="font-style:italic;">Karang Daun</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">70 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">24 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">6 images</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4" style="font-style:italic;">Karang Goak</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">70 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">24 images</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">6 images</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4" style="font-style:italic;">Karang Gajah</span></p></td><td style="vertical-align:top;">
<p><span class="font4">70 images</span></p></td><td style="vertical-align:top;">
<p><span class="font4">24 images</span></p></td><td style="vertical-align:top;">
<p><span class="font4">6 images</span></p></td></tr>
</table>
<p><span class="font4">mizer, (c) SGD Optimizer</span></p>
<p><span class="font4" style="font-weight:bold;">Table 6. </span><span class="font4">Gabor Filter Performance Comparison</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font4">Metrices</span></p></td><td colspan="6" style="vertical-align:top;">
<p><span class="font4">Without Gabor Filter</span></p></td><td colspan="6" style="vertical-align:top;">
<p><span class="font4">With Gabor Filter</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:top;">
<p><span class="font4">Adam</span></p></td><td colspan="2" style="vertical-align:top;">
<p><span class="font4">RMSProp</span></p></td><td colspan="2" style="vertical-align:top;">
<p><span class="font4">SGD</span></p></td><td colspan="2" style="vertical-align:top;">
<p><span class="font4">Adam</span></p></td><td colspan="2" style="vertical-align:top;">
<p><span class="font4">RMSProp</span></p></td><td colspan="2" style="vertical-align:top;">
<p><span class="font4">SGD</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4">Accuracy</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">72</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">61</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">58</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">89</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">86</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">87</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font3">72</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">61</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">58</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">89</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">86</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">87</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4">Precision</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">76</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">66</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">57</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">89</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">87</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">87</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font4">F1 Score</span></p></td><td style="vertical-align:top;">
<p><span class="font3">71</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">60</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">57</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">89</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">86</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td><td style="vertical-align:top;">
<p><span class="font3">87</span></p></td><td style="vertical-align:top;">
<p><span class="font4">%</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark22"></a><span class="font4" style="font-weight:bold;"><a name="bookmark23"></a>4.2. &nbsp;&nbsp;&nbsp;Gabor Dataset</span></h2></li></ul>
<p><span class="font4">In this scenario, we conducted training with a dataset consist of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> carving image that has passed the image augmentation process and Gabor filter feature extraction step. We split the dataset into training, testing, and validation dataset. The details of each dataset can be seen in Table 5. The highest training obtained by the model is 100% from the model trained by RMSProp Optimizer and the highest validation accuracy of 86.25% from the model trained with</span></p>
<div><img src="https://jurnal.harianregional.com/media/76499-5.jpg" alt="" style="width:425pt;height:178pt;">
<p><span class="font4" style="font-weight:bold;">Figure 5. </span><span class="font4">Training result graph on Gabor dataset: (a) Adam Optimizer, (b) RMSProp Optimizer, (c) SGD Optimizer</span></p>
</div><br clear="all">
<p><span class="font4" style="font-weight:bold;">Table 7. </span><span class="font4">Split Dataset Detail for Gabor Dataset</span></p>
<p><span class="font4">Class Name Training Validation Testing</span></p>
<div>
<p><span class="font4" style="font-style:italic;">Karang Daun</span></p>
<p><span class="font4" style="font-style:italic;">Karang Goak</span></p>
</div><br clear="all">
<p><span class="font4">280 images 96 images 24 images</span></p>
<p><span class="font4">280 images 96 images 24 images</span></p>
<p><span class="font4" style="font-style:italic;">Karang Gajah</span><span class="font4"> 280 images 96 images 24 images</span></p>
<p><span class="font4">SGD and Adam optimizer . Figure 5 shows accuracy and loss graph of each model that trained with benchmark dataset on each epoch. From the graph, we see the model trained with the Gabor with Gabor dataset tends to be more stable and obtain more accuracy than the model trained with the dataset without Gabor filter feature extraction. To further evaluate our model, we conduct the testing step by predicting all images on the testing dataset and analyze the result using a confusion matrix. The model acquired the highest accuracy of 89%, the highest recall of 89%, the highest precision of 89%, and the highest F1 score of 89% from the testing step. The detailed result of Gabor Filter performance can be seen at Table 6. These results indicated that the Gabor filter improves model accuracy and tends to be more stable than the model that only trained on image data with geometric transformation. Figure 6 shows comparison of our proposed method with another classification method for Balinese traditional carving clasification.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark24"></a><span class="font4" style="font-weight:bold;"><a name="bookmark25"></a>5. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font4">In this paper, we proposed a novel method for the classification of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional carving. Our research shows the process of </span><span class="font4" style="font-style:italic;">Kekarangan</span><span class="font4"> Balinese traditional carving classification using Convolutional Neural Network (CNN) from Gabor filter image. The researcher established its dataset for this research. The image obtained carries out through the process of data augmentation. Then, it is used to increase the amount of dataset used on training and testing steps, and the Gabor filter is used to extracting features from the image. Confusion matrix matrices is used to carry out the evaluation steps of the model.</span></p>
<p><span class="font4">We conduct six training and model evaluation scenarios by using the benchmark dataset and Gabor dataset. Our model trained with the Gabor dataset shows increased accuracy by up to 17% from our benchmark dataset and 16.7% from the previous method used in classifying </span><span class="font4" style="font-style:italic;">Kekarangan </span><span class="font4">engraving images. Based on these results, our novel methods can use to further enhance the model performance on the Balinese traditional carving classification.</span></p><img src="https://jurnal.harianregional.com/media/76499-6.jpg" alt="" style="width:293pt;height:171pt;">
<p><span class="font7">Accuracy (%)</span></p>
<p><span class="font4" style="font-weight:bold;">Figure 6. </span><span class="font4">Comparison with different Balinese traditional carving classification method</span></p>
<h2><a name="bookmark26"></a><span class="font4" style="font-weight:bold;"><a name="bookmark27"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font4">[1] &nbsp;&nbsp;&nbsp;I. M. Widiatmika, I. Piarsa, and A. Syafiandini, “Recognition of the baby footprint characteristics using wavelet method and k-nearest neighbor (k-nn),” </span><span class="font4" style="font-style:italic;">Lontar Komputer : Jurnal Ilmiah Teknologi Informasi</span><span class="font4">, vol. 12, no. 1, pp. 41–52, 2021. [Online]. Available: </span><a href="https://ojs.unud.ac.id/index.php/lontar/article/view/70638"><span class="font4">https://ojs.unud.ac.id/index.php/lontar/article/view/70638</span></a></p></li>
<li>
<p><span class="font4">[2] &nbsp;&nbsp;&nbsp;R. Chauhan, K. K. Ghanshala, and R. C. Joshi, “Convolutional neural network (cnn) for image detection and recognition,” in </span><span class="font4" style="font-style:italic;">2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC)</span><span class="font4">, Dec 2018, pp. 278–282.</span></p></li>
<li>
<p><span class="font4">[3] &nbsp;&nbsp;&nbsp;I. M. A. Maha, “Pengenalan pola motif ukiran bali menggunakan histogram of oriented gradient (hog) dan learning vector quantization (lvq),” Master’s thesis, Institut Teknologi Surabaya, 2017.</span></p></li>
<li>
<p><span class="font4">[4] &nbsp;&nbsp;&nbsp;Sumantara, Agung Bayupati, and Wirdiani, “Rancang Bangun Aplikasi Pengenalan Ukiran Bali dengan Metode ORB,” </span><span class="font4" style="font-style:italic;">Jurnal Ilmiah Merpati</span><span class="font4">, 2017.</span></p></li>
<li>
<p><span class="font4">[5] &nbsp;&nbsp;&nbsp;Antara Kesiman, Aditra Pradnyana, and Mahayana Putra, “IDENTIFIKASI CITRA UKIRAN ORNAMEN TRADISIONAL BALI DENGAN METODE MULTILAYER PERCEPTRON,” </span><span class="font4" style="font-style:italic;">SIN-TECH</span><span class="font4">, 2021.</span></p></li>
<li>
<p><span class="font4">[6] &nbsp;&nbsp;&nbsp;Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” </span><span class="font4" style="font-style:italic;">Proceedings of the IEEE</span><span class="font4">, vol. 86, no. 11, pp. 2278–2324, Nov 1998.</span></p></li>
<li>
<p><span class="font4">[7] &nbsp;&nbsp;&nbsp;Yann LeCun, Corinna Cortes, and Christopher J.C. Burges, “The mnist database of handwritten digits,” </span><a href="http://yann.lecun.com/exdb/mnist/"><span class="font4">http://yann.lecun.com/exdb/mnist/</span></a><span class="font4">, 2013, accessed: 2019-12-3.</span></p></li>
<li>
<p><span class="font4">[8] &nbsp;&nbsp;&nbsp;L. Taylor and G. Nitschke, “Improving deep learning with generic data augmentation,” in </span><span class="font4" style="font-style:italic;">2018 IEEE Symposium Series on Computational Intelligence (SSCI)</span><span class="font4">, Nov 2018, pp. 1542–1547.</span></p></li>
<li>
<p><span class="font4">[9] &nbsp;&nbsp;&nbsp;A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classification with deep convolutional neural networks,” </span><span class="font4" style="font-style:italic;">Neural Information Processing Systems</span><span class="font4">, vol. 25, 01 2012.</span></p></li>
<li>
<p><span class="font4">[10] &nbsp;&nbsp;&nbsp;Connor Shorten and Taghi M. Khoshgoftaar , “A survey on Image Data Augmentation for Deep Learning,” </span><span class="font4" style="font-style:italic;">Journal of Big Data</span><span class="font4">, Jul 2019.</span></p></li>
<li>
<p><span class="font4">[11] &nbsp;&nbsp;&nbsp;H. Yao, L. Chuyi, H. Dan, and Y. Weiyu, “Gabor feature based convolutional neural network for object recognition in natural scene,” in </span><span class="font4" style="font-style:italic;">2016 3rd International Conference on Information Science and Control Engineering (ICISCE)</span><span class="font4">, July 2016, pp. 386–390.</span></p></li>
<li>
<p><span class="font4">[12] &nbsp;&nbsp;&nbsp;J. . Kamarainen, V. Kyrki, and H. Kalviainen, “Invariance properties of gabor filter-based features-overview and applications,” </span><span class="font4" style="font-style:italic;">IEEE Transactions on Image Processing</span><span class="font4">, vol. 15, no. 5, pp. 1088–1099, May 2006.</span></p></li>
<li>
<p><span class="font4">[13] &nbsp;&nbsp;&nbsp;C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A Survey on Deep Transfer Learning,” </span><span class="font4" style="font-style:italic;">arXiv e-prints</span><span class="font4">, p. arXiv:1808.01974, Aug 2018.</span></p></li>
<li>
<p><span class="font4">[14] &nbsp;&nbsp;&nbsp;N. Tajbakhsh, J. Y. Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B. Gotway, and J. Liang, “Convolutional neural networks for medical image analysis: Full training or fine tuning?” </span><span class="font4" style="font-style:italic;">IEEE Transactions on Medical Imaging</span><span class="font4">, vol. 35, no. 5, pp. 1299–1312, May 2016.</span></p></li>
<li>
<p><span class="font4">[15] &nbsp;&nbsp;&nbsp;K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” </span><span class="font4" style="font-style:italic;">arXiv e-prints</span><span class="font4">, p. arXiv:1409.1556, Sep 2014.</span></p></li>
<li>
<p><span class="font4">[16] &nbsp;&nbsp;&nbsp;C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going Deeper with Convolutions,” </span><span class="font4" style="font-style:italic;">arXiv e-prints</span><span class="font4">, p. arXiv:1409.4842, Sep 2014.</span></p></li>
<li>
<p><span class="font4">[17] &nbsp;&nbsp;&nbsp;A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,” </span><span class="font4" style="font-style:italic;">arXiv e-prints</span><span class="font4">, p. arXiv:1704.04861, Apr 2017.</span></p></li>
<li>
<p><span class="font4">[18] &nbsp;&nbsp;&nbsp;K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image Recognition,” </span><span class="font4" style="font-style:italic;">arXiv e-prints</span><span class="font4">, p. arXiv:1512.03385, Dec 2015.</span></p></li>
<li>
<p><span class="font4">[19] &nbsp;&nbsp;&nbsp;A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision applications,” </span><span class="font4" style="font-style:italic;">CoRR</span><span class="font4">, vol. abs/1704.04861, 2017. [Online]. Available: </span><a href="http://arxiv.org/abs/1704.04861"><span class="font4">http://arxiv.org/abs/1704.04861</span></a></p></li>
<li>
<p><span class="font4">[20] &nbsp;&nbsp;&nbsp;M. Sokolova and G. Lapalme, “A systematic analysis of performance measures for classification tasks,” </span><span class="font4" style="font-style:italic;">Information Processing &amp;&nbsp;Management</span><span class="font4">, vol. 45, no. 4, pp. 427 – 437, 2009.</span></p></li></ul>
<p><span class="font4">10</span></p>