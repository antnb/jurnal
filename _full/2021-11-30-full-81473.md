---
layout: full_article
title: "SMOTE: POTENSI DAN KEKURANGANNYA PADA SURVEI"
author: "NI PUTU YULIKA TRISNA WIJAYANTI, EKA N. KENCANA, I WAYAN SUMARJAYA"
categories: mtk
canonical_url: https://jurnal.harianregional.com/mtk/full-81473 
citation_abstract_html_url: "https://jurnal.harianregional.com/mtk/id-81473"
citation_pdf_url: "https://jurnal.harianregional.com/mtk/full-81473"  
comments: true
---

<p><span class="font1">E-Jurnal Matematika Vol. 10(4), November 2021, pp. 235-240</span></p>
<p><span class="font1">DOI: </span><a href="https://doi.org/10.24843/MTK.2021.v10.i04.p348"><span class="font1">https://doi.org/10.24843/MTK.2021.v10.i04.p348</span></a></p>
<p><span class="font1">ISSN: 2303-1751</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font4" style="font-weight:bold;"><a name="bookmark1"></a>SMOTE: POTENSI DAN KEKURANGANNYA PADA SURVEI</span></h1>
<p><span class="font2">Ni Putu Yulika Trisna Wijayanti<sup>1§</sup>, Eka N Kencana<sup>2</sup>, I Wayan Sumarjaya<sup>3</sup></span></p>
<p><span class="font1"><sup>1</sup>Program Studi Matematika, Fakultas MIPA – Universitas Udayana [Email: </span><a href="mailto:yulika.wijayanti@gmail.com"><span class="font1">yulika.wijayanti@gmail.com</span></a><span class="font1">] <sup>2</sup>Program Studi Matematika, Fakultas MIPA – Universitas Udayana [Email: </span><a href="mailto:i.putu.enk@unud.ac.id"><span class="font1">i.putu.enk@unud.ac.id</span></a><span class="font1">] <sup>3</sup>Program Studi Matematika, Fakultas MIPA – Universitas Udayana [Email: </span><a href="mailto:sumarjaya@unud.ac.id"><span class="font1">sumarjaya@unud.ac.id</span></a><span class="font1">] <sup>§</sup></span><span class="font1" style="font-style:italic;">Corresponding Author</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">ABSTRACT</span></p>
<p><span class="font2" style="font-style:italic;">Imbalanced data is a problem that is often found in real-world cases of classification. Imbalanced data causes misclassification will tend to occur in the minority class. This can lead to errors in decision-making if the minority class has important information and it’s the focus of attention in research. Generally, there are two approaches that can be taken to deal with the problem of imbalanced data, the data level approach and the algorithm level approach. The data level approach has proven to be very effective in dealing with imbalanced data and more flexible. The oversampling method is one of the data level approaches that generally gives better results than the undersampling method. SMOTE is the most popular oversampling method used in more applications. In this study, we will discuss in more detail the SMOTE method, potential, and disadvantages of this method. In general, this method is intended to avoid overfitting and improve classification performance in the minority class. However, this method also causes overgeneralization which tends to be overlapping.</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords</span><span class="font2" style="font-style:italic;">: Imbalanced Data, Oversampling, SMOTE</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;PENDAHULUAN</span></h2></li></ul>
<p><span class="font2">Data tidak seimbang merupakan permasalahan yang sering ditemukan pada kasus nyata dalam klasifikasi. Data tidak seimbang terjadi ketika jumlah pengamatan pada data latih untuk setiap label kelas tidak seimbang, di mana kondisi jumlah data pada suatu kelas jauh lebih banyak dibandingkan kelas lainnya. Kelas dengan jumlah data yang lebih banyak disebut kelas mayoritas sedangkan kelas dengan jumlah data yang lebih sedikit disebut dengan kelas minoritas (Brownlee, 2020).</span></p>
<p><span class="font2">Data tidak seimbang menyebabkan kesalahan klasifikasi akan cenderung terjadi pada kelas minoritas. Kelas minoritas akan lebih sulit untuk diprediksi karena hanya ada sedikit data pada kelas tersebut jika dibandingkan dengan kelas mayoritas. Banyak makalah penelitian tentang data tidak seimbang sepakat bahwa distribusi kelas yang tidak merata menyebabkan pengklasifikasi bias terhadap kelas mayoritas. Hal ini dikarenakan secara umum algoritma klasifikasi standar pada pembelajaran mesin cenderung mengasumsikan distribusi kelas yang sama. Sehingga, pada kasus data tidak seimbang model klasifikasi</span></p>
<p><span class="font2">akan cenderung berfokus untuk mempelajari karakteristik data pada kelas mayoritas dan mengabaikan kelas minoritas (Singh dan Sharma, 2019).</span></p>
<p><span class="font2">Hal tersebut dapat menyebabkan kesalahan dalam pengambilan keputusan, apabila kelas minoritas memiliki informasi penting dan menjadi fokus perhatian dalam penelitian. Sebagai contoh pada kasus deteksi penyakit kanker, deteksi spam, deteksi penipuan, </span><span class="font2" style="font-style:italic;">churn prediction</span><span class="font2"> dan lain-lain.</span></p>
<p><span class="font2">Umumnya terdapat dua pendekatan yang dapat dilakukan untuk menangani permasalahan data tidak seimbang, yakni pendekatan level data dan pendekatan level algoritma. Pendekatan pada level data dilakukan dengan menyeimbangkan distribusi kelas mayoritas dan minoritas dengan teknik pengambilan sampel seperti </span><span class="font2" style="font-style:italic;">undersampling</span><span class="font2">, </span><span class="font2" style="font-style:italic;">oversampling</span><span class="font2">, maupun kombinasi dari kedua metode tersebut. Sedangkan pendekatan pada level algoritma dilakukan dengan memodifikasi dan mengoptimalkan kinerja algoritma pembelajaran mesin (Santoso </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, 2017).</span></p>
<p><span class="font2">Menurut He dan Garcia dalam Maldonado, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2019) pengambilan sampel pada data telah terbukti sangat efektif untuk menangani data tidak seimbang. Pengambilan sampel merupakan pendekatan pada level data. Keuntungan dari pendekatan level data ini yakni tidak bergantung pada pengklasifikasi yang digunakan dan pendekatan ini dinilai lebih fleksibel (Ramyachitra dan Manikandan, 2014).</span></p>
<p><span class="font2" style="font-style:italic;">Undersampling</span><span class="font2"> dilakukan dengan mengurangi atau mengeliminasi beberapa data pada kelas mayoritas untuk menyeimbangkan distribusi kelas, sedangkan </span><span class="font2" style="font-style:italic;">oversampling </span><span class="font2">dilakukan dengan menambahkan data pada kelas minoritas. Pada umumnya metode </span><span class="font2" style="font-style:italic;">oversampling </span><span class="font2">lebih sering digunakan dibandingkan </span><span class="font2" style="font-style:italic;">undersampling</span><span class="font2">. Hal ini dikarenakan metode </span><span class="font2" style="font-style:italic;">undersampling</span><span class="font2"> mengurangi data pada kelas mayoritas sehingga dapat menghilangkan informasi penting pada data tersebut. Menurut Batista, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> dalam Santoso, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2017) metode </span><span class="font2" style="font-style:italic;">oversampling</span><span class="font2"> umumnya memberikan hasil yang lebih baik dibandingkan metode </span><span class="font2" style="font-style:italic;">undersampling</span><span class="font2">.</span></p>
<p><span class="font2">Metode </span><span class="font2" style="font-style:italic;">oversampling</span><span class="font2"> paling dasar dilakukan dengan menduplikasi data secara acak pada kelas minoritas, yang disebut dengan </span><span class="font2" style="font-style:italic;">random oversampling</span><span class="font2">. Namun, metode ini cenderung mengakibatkan </span><span class="font2" style="font-style:italic;">overfitting</span><span class="font2">, karena dilakukan penduplikasian data yang telah ada sebelumnya sehingga pengklasifikasi terkena informasi yang sama. Untuk mengatasi permasalahan tersebut Chawla, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2002) mengusulkan metode </span><span class="font2" style="font-style:italic;">synthetic minority oversampling technique </span><span class="font2">(SMOTE).</span></p>
<p><span class="font2">SMOTE merupakan metode </span><span class="font2" style="font-style:italic;">oversampling </span><span class="font2">yang paling populer digunakan. SMOTE dilakukan dengan menambah data sintetis pada kelas minoritas. Data sintetis merupakan data baru yang dibangkitkan. Walaupun populer, metode SMOTE tetap memiliki kekurangan yang mendorong pengembangan penelitian untuk mengatasi permasalahan tersebut. Sehingga, pada kajian ini akan dibahas lebih mendalam mengenai potensi dan kekurangan dari metode SMOTE.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font2" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;KAJIAN PUSTAKA</span></h2></li></ul>
<p><span class="font2">Ha dan Bunke dalam Chawla, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2002) menyarankan pengambilan sampel berlebih pada kelas minor dilakukan dengan membuat sampel sintetis yakni sampel baru yang dibangkitkan dibanding melakukan duplikasi data. SMOTE dilakukan dengan menambah</span></p>
<p><span class="font2">jumlah data pada kelas minoritas dengan cara membangkitkan data baru berdasarkan </span><span class="font2" style="font-style:italic;">k </span><span class="font2">tetangga terdekat</span><span class="font2" style="font-style:italic;">.</span><span class="font2"> Data pada kelas minoritas dilakukan </span><span class="font2" style="font-style:italic;">oversampling</span><span class="font2"> dengan mengambil data pada kelas minoritas dan menambah sampel sintetis di sepanjang garis yang menghubungkan salah satu atau semua </span><span class="font2" style="font-style:italic;">k</span><span class="font2"> tetangga terdekat data kelas minoritas tersebut. Jumlah tetangga </span><span class="font2" style="font-style:italic;">k </span><span class="font2">dipilih secara acak.</span></p>
<p><span class="font2">Formula untuk membangkitkan data sintetis dengan SMOTE adalah sebagai berikut:</span></p>
<p><span class="font2" style="font-style:italic;">K</span><span class="font0" style="font-style:italic;">new</span><span class="font2"> = </span><span class="font2" style="font-style:italic;">X</span><span class="font0" style="font-style:italic;">i <sup>+</sup> </span><span class="font2" style="font-style:italic;">(X</span><span class="font0" style="font-style:italic;">k <sup>-</sup> </span><span class="font2" style="font-style:italic;">X</span><span class="font0" style="font-style:italic;">i</span><span class="font2" style="font-style:italic;">) × S</span></p>
<p><span class="font2">di mana X<sub>new</sub> = data sintetis baru, </span><span class="font2" style="font-style:italic;">X<sub>i</sub></span><span class="font2"> = data dari kelas minoritas, </span><span class="font2" style="font-style:italic;">X<sub>k</sub></span><span class="font2"> = data dari </span><span class="font2" style="font-style:italic;">k</span><span class="font2"> tetangga terdekat yang memiliki jarak terdekat dengan </span><span class="font2" style="font-style:italic;">X<sub>i</sub></span><span class="font2">, dan </span><span class="font2" style="font-style:italic;">S</span><span class="font2"> = bilangan acak antara 0 dan 1. Perbedaan jarak dalam menentukan tetangga terdekat pada data numerik dilakukan dengan menggunakan jarak </span><span class="font2" style="font-style:italic;">Euclid</span><span class="font2">.</span></p>
<p><span class="font2">Pada makalah algoritma SMOTE asli, beberapa modifikasi telah diusulkan dalam literatur. Pendekatan SMOTE tidak menangani kumpulan data dengan semua fitur yang berskala nominal dan hal tersebut dikembangkan untuk menangani kumpulan data dengan fitur berskala campuran yakni nominal dan kontinu. Chawla, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2002) mengusulkan </span><span class="font2" style="font-style:italic;">synthetic minority oversampling technique nominal</span><span class="font2"> (SMOTE-N) untuk kumpulan data dengan semua fitur berskala nominal, dan </span><span class="font2" style="font-style:italic;">synthetic minority oversampling technique nominal continuous</span><span class="font2"> (SMOTE-NC) untuk kumpulan data berskala nominal dan kontinu.</span></p>
<p><span class="font2">Perhitungan tetangga terdekat kelas minoritas fitur nominal pada SMOTE-N dilakukan dengan menggunakan </span><span class="font2" style="font-style:italic;">value difference metric</span><span class="font2"> (VDM) dengan formula:</span></p>
<p><span class="font0" style="font-style:italic;">N</span></p>
<p><span class="font2" style="font-style:italic;">∆(X,Y} =w<sub>x</sub>w<sub>y</sub>∑δ(X</span><span class="font0" style="font-style:italic;">i</span><span class="font2" style="font-style:italic;">,y</span><span class="font0" style="font-style:italic;">i</span><span class="font2" style="font-style:italic;">y </span><span class="font0" style="font-style:italic;">i=i</span></p>
<p><span class="font2">di mana </span><span class="font2" style="font-style:italic;">w<sub>x</sub>w<sub>y</sub></span><span class="font2"> = bobot amatan, </span><span class="font2" style="font-style:italic;">N</span><span class="font2"> = banyaknya fitur penjelas, </span><span class="font2" style="font-style:italic;">r</span><span class="font2"> = bernilai 1 (jarak </span><span class="font2" style="font-style:italic;">Manhattan</span><span class="font2">) atau 2 (jarak </span><span class="font2" style="font-style:italic;">Euclid)</span><span class="font2">, dan </span><span class="font2" style="font-style:italic;">S(x<sub>i</sub>,y<sub>i</sub>)</span><span class="font2"> = jarak antar fitur nominal dengan formula:</span></p>
<p><span class="font0" style="font-style:italic;">n</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2" style="font-style:italic;">*x^ = ∑⅛-% </span><span class="font2">U-I &nbsp;&nbsp;&nbsp;&nbsp;Uol</span></p></li></ul>
<p><span class="font0" style="font-style:italic;">i=i</span><span class="font2"> <sup>1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</sup></span><span class="font2" style="font-style:italic;"><sup>2</sup></span></p>
<p><span class="font2">di mana </span><span class="font2" style="font-style:italic;">C<sub>1</sub> =</span><span class="font2"> banyaknya </span><span class="font2" style="font-style:italic;">x<sub>i</sub></span><span class="font2"> terjadi, </span><span class="font2" style="font-style:italic;">C<sub>1i</sub></span><span class="font2"> = banyaknya </span><span class="font2" style="font-style:italic;">x<sub>i</sub></span><span class="font2"> yang termasuk kelas </span><span class="font2" style="font-style:italic;">i, C<sub>2</sub> = </span><span class="font2">banyaknya </span><span class="font2" style="font-style:italic;">y<sub>i</sub></span><span class="font2"> terjadi, </span><span class="font2" style="font-style:italic;">C<sub>2i</sub></span><span class="font2"> = banyaknya </span><span class="font2" style="font-style:italic;">y<sub>i</sub></span><span class="font2"> yang</span></p>
<p><span class="font2">termasuk kelas </span><span class="font2" style="font-style:italic;">i, n</span><span class="font2"> = banyaknya kelas, dan </span><span class="font2" style="font-style:italic;">k</span><span class="font2"> = konstanta (biasanya 1).</span></p>
<p><span class="font2">SMOTE-NC dilakukan dengan menghitung median deviasi standar semua fitur/variabel kontinu untuk kelas minoritas. Perhitungan jarak </span><span class="font2" style="font-style:italic;">Euclid</span><span class="font2"> dilakukan dengan menggunakan fitur kontinu dan menyertakan median deviasi standar yang telah dihitung sebelumnya. Pembangkitan data sintetis untuk fitur kontinu dilakukan sama dengan metode SMOTE sedangkan untuk fitur nominal dilakukan dengan memilih nilai mayoritas dari </span><span class="font2" style="font-style:italic;">k</span><span class="font2"> tetangga terdekat (Chawla </span><span class="font2" style="font-style:italic;">et al.,</span><span class="font2"> 2002)</span><span class="font2" style="font-style:italic;">.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font2" style="font-weight:bold;"><a name="bookmark7"></a>3. &nbsp;&nbsp;&nbsp;PEMBAHASAN</span></h2></li></ul>
<p><span class="font2">Pada dasarnya SMOTE adalah salah satu penerapan dari metode </span><span class="font2" style="font-style:italic;">oversampling</span><span class="font2">. Sehingga salah satu kelebihan metode ini adalah tidak akan menyebabkan adanya informasi yang hilang, dikarenakan tidak ada pengurangan data seperti yang dilakukan pada metode </span><span class="font2" style="font-style:italic;">undersampling</span><span class="font2">.</span></p>
<p><span class="font2">Jika dibandingkan dengan </span><span class="font2" style="font-style:italic;">oversampling </span><span class="font2">yang dilakukan dengan duplikasi (</span><span class="font2" style="font-style:italic;">random oversampling</span><span class="font2">) kelebihan SMOTE dapat dilihat dengan mempertimbangkan efek pada wilayah keputusan dalam ruang fitur. Dengan duplikasi, wilayah keputusan untuk kelas minoritas dapat menjadi lebih kecil dan spesifik, hal ini dikarenakan sampel minoritas di wilayah tersebut diduplikasi yang menyebabkan adanya kecenderungan untuk </span><span class="font2" style="font-style:italic;">overfitting</span><span class="font2">. Sedangkan, dengan pembangkitan data sintetis (SMOTE) dapat menyebabkan pengklasifikasi membangun wilayah keputusan yang lebih besar dan kurang spesifik pada kelas minoritas.</span></p>
<p><span class="font2">SMOTE menyediakan sampel pada kelas minoritas yang lebih terkait untuk dipelajari oleh pengklasifikasi, sehingga pengklasifikasi memiliki cakupan yang lebih besar dalam mempelajari kelas minoritas. Hal tersebut menyebabkan pendekatan SMOTE mampu meningkatkan nilai akurasi pengklasifikasi pada kelas minoritas, jika dibandingkan </span><span class="font2" style="font-style:italic;">oversampling </span><span class="font2">dengan duplikasi (Chawla </span><span class="font2" style="font-style:italic;">et al</span><span class="font2">. 2002).</span></p>
<p><span class="font2">Meskipun SMOTE cukup efektif dalam meningkatkan akurasi klasifikasi pada kelas minoritas, namun masih terdapat permasalahan pada metode ini. Salah satu permasalahan utama pada SMOTE adalah terjadinya </span><span class="font2" style="font-style:italic;">overgeneralization</span><span class="font2"> (generalisasi yang berlebih). Data sintetis hasil SMOTE dapat memungkinkan tersebar pada wilayah kelas minoritas maupun mayoritas, di mana hal ini</span></p>
<p><span class="font2">dapat menyebabkan menurunnya kinerja pengklasifikasi (Santoso </span><span class="font2" style="font-style:italic;">et al.,</span><span class="font2"> 2017).</span></p>
<p><span class="font2">Sejak kelas mayoritas diabaikan oleh metode ini, data sintetis yang dihasilkan dapat dibuat di atas kelas mayoritas, hal ini menyebabkan terjadinya </span><span class="font2" style="font-style:italic;">overlapping</span><span class="font2"> (tumpang tindih). Ilustrasi </span><span class="font2" style="font-style:italic;">overlapping</span><span class="font2"> dapat dilihat pada Gambar 1. Data sintetis baru (dilambangkan dengan kotak berwarna kuning) terletak di wilayah yang didominasi oleh data pada kelas mayoritas mengindikasikan kondisi </span><span class="font2" style="font-style:italic;">overlapping</span><span class="font2"> dan mengaburkan batas antar kelas.</span></p><img src="https://jurnal.harianregional.com/media/81473-1.jpg" alt="" style="width:143pt;height:130pt;">
<p><span class="font1">Gambar 1. Ilustrasi permasalahan </span><span class="font1" style="font-style:italic;">overgeneralization </span><span class="font1">pada SMOTE</span></p>
<p><span class="font2">Berdasarkan formula untuk membangkitkan data sintetis dengan SMOTE, jarak antara data minoritas dan tetangga terdekatnya dikalikan dengan bilangan acak antara 0 dan 1. Jika bilangan acak mendekati 0 maka data sintetis akan serupa dengan data minoritas asal. Sebaliknya, jika bilangan acak mendekati 1 maka data sintetis akan serupa dengan tetangga terdekat. Masalah yang dapat terjadi adalah jika bilangan acak sekitar 0,5 memungkinkan data sintetis yang dihasilkan akan serupa dengan data mayoritas. Inilah penyebab generalisasi yang berlebihan (Santoso </span><span class="font2" style="font-style:italic;">et al</span><span class="font2">., 2017).</span></p>
<p><span class="font2">Tetangga terdekat pada metode SMOTE dicari dengan menghitung perbedaan jarak, yang biasanya dihitung menggunakan jarak </span><span class="font2" style="font-style:italic;">Euclid</span><span class="font2">. Namun, permasalahan muncul di mana sejumlah kecil fitur memiliki kepentingan atau nilai korelasi yang tinggi dengan fitur lainnya. Mencari tetangga terdekat dengan jarak </span><span class="font2" style="font-style:italic;">Euclid </span><span class="font2">tanpa mempertimbangkan kepentingan tersebut akan menghasilkan tetangga yang tidak representatif (Fahrudin </span><span class="font2" style="font-style:italic;">et al.,</span><span class="font2"> 2019).</span></p>
<p><span class="font2">Penggunaan jarak </span><span class="font2" style="font-style:italic;">Euclid</span><span class="font2"> juga menimbulkan permasalahan pada kasus kumpulan data berdimensi tinggi. Jarak </span><span class="font2" style="font-style:italic;">Euclid</span><span class="font2"> tidak sesuai digunakan pada kasus data berdimensi tinggi karena konsep kedekatan tidak terdefinisi</span></p>
<p><span class="font2">dengan baik. Jarak </span><span class="font2" style="font-style:italic;">Euclid</span><span class="font2"> mengasumsikan bahwa semua fitur memiliki kepentingan yang sama, sedangkan data berdimensi tinggi biasanya memiliki fitur yang tidak relevan yakni fitur yang tidak merepresentasikan permasalahan dalam pemodelan, di mana hal ini akan menimbulkan </span><span class="font2" style="font-style:italic;">noise</span><span class="font2"> pada algoritma (Maldonado, 2019).</span></p>
<p><span class="font2">Chawla, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2002) pada makalahnya menyarankan untuk mempertimbangkan lebih lanjut beberapa topik untuk meningkatkan kinerja metode SMOTE, secara khusus yakni mengenai strategi untuk menentukan tetangga terdekat. Permasalahan yang muncul pada metode SMOTE, mendorong berbagai penelitian lanjutan modifikasi SMOTE untuk menciptakan teknik yang lebih efektif dalam meningkatkan kinerja klasifikasi.</span></p>
<p><span class="font2">Beberapa perkembangan dari metode SMOTE diantaranya, Chawla, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2003) mengusulkan SMOTEBoost. Metode tersebut merupakan kombinasi dari metode SMOTE dan prosedur </span><span class="font2" style="font-style:italic;">boosting</span><span class="font2"> standar dengan tujuan untuk meningkatkan akurasi prediksi kelas minoritas tanpa mengorbankan akurasi pada keseluruhan data.</span></p>
<p><span class="font2">Data tidak seimbang sering menyebabkan klaster kelas tidak didefinisikan dengan baik karena beberapa data kelas mayoritas mungkin berada pada kelas minoritas sehingga menyebabkan </span><span class="font2" style="font-style:italic;">noise</span><span class="font2">. Batista, </span><span class="font2" style="font-style:italic;">et al</span><span class="font2">. (2004) memperbaiki kinerja metode SMOTE dengan menghapus data tersebut dengan </span><span class="font2" style="font-style:italic;">tomek links </span><span class="font2">(SMOTE-Tomeks Link), selain itu perbaikan juga dilakukan pada kedua kelas minoritas dan mayoritas dengan </span><span class="font2" style="font-style:italic;">edited nearest neighbor </span><span class="font2">(SMOTE-ENN). Motivasi utama kedua metode tersebut tidak hanya menyeimbangkan data latih, tetapi juga menghilangkan data </span><span class="font2" style="font-style:italic;">noise</span><span class="font2"> yang memungkinkan pembuatan model yang lebih sederhana dan kemampuan generalisasi yang lebih baik.</span></p>
<p><span class="font2">Han, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2005) mengembangkan metode SMOTE yang berfokus pada daerah perbatasan (</span><span class="font2" style="font-style:italic;">borderline</span><span class="font2">) antara batas data kelas minoritas dan mayoritas. Motivasi metode tersebut, karena sebagian besar algoritma klasifikasi berusaha mempelajari garis batas setiap kelas setepat mungkin pada proses pelatihan. Data yang berada pada garis batas atau di dekatnya akan cenderung salah diklasifikasikan dibandingkan data yang terletak jauh dari garis batas. Sehingga, pembangkitan data sintetis hanya dilakukan pada daerah perbatasan (SMOTE-Borderline). Pada metode ini, bilangan acak</span></p>
<p><span class="font2">yang digunakan untuk membangkitkan data yakni antara 0 sampai 0,5 dengan harapan data sintetis yang dihasilkan akan serupa dengan data minoritas dan menghindari terjadinya </span><span class="font2" style="font-style:italic;">overlapping</span><span class="font2">.</span></p>
<p><span class="font2">He, </span><span class="font2" style="font-style:italic;">et al</span><span class="font2">. (2008) mengusulkan metode </span><span class="font2" style="font-style:italic;">adaptuve synthetic sampling approach </span><span class="font2">(ADASYN). Metode tersebut menggunakan distribusi berbobot pada kelas minoritas sesuai dengan tingkat kesulitan pembelajaran. Data sintetis akan dihasilkan lebih banyak pada kelas minoritas yang sulit untuk dipelajari dibandingkan kelas minoritas yang mudah dipelajari.</span></p>
<p><span class="font2">Bunkhumpornpat, </span><span class="font2" style="font-style:italic;">et al</span><span class="font2">. (2009) lebih berfokus memperhatikan daerah aman (</span><span class="font2" style="font-style:italic;">safe</span><span class="font2">), merupakan data yang ditempatkan didaerah yang relatif homogen dengan kelas minoritas. Metode tersebut disebut </span><span class="font2" style="font-style:italic;">Safe Level</span><span class="font2"> SMOTE, di mana data sintetis dihasilkan lebih banyak pada daerah aman, sehingga menghasilkan kinerja akurasi yang lebih baik.</span></p>
<p><span class="font2">Selanjutnya Ramentol, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2011) menggunakan teori </span><span class="font2" style="font-style:italic;">Rough Set</span><span class="font2"> untuk meningkatkan data sintetis yang dihasilkan oleh metode SMOTE, metode ini disebut SMOTE-RSB. Douzas dan Bacao (2017) mengusulkan metode </span><span class="font2" style="font-style:italic;">geometric</span><span class="font2"> SMOTE (G-SMOTE) dengan membangkitkan data sintetis di ruang input geometris yakni </span><span class="font2" style="font-style:italic;">truncated hyper-spheroid</span><span class="font2">, di sekitar data minoritas yang terpilih.</span></p>
<p><span class="font2">Untuk mengatasi permasalahan pada kumpulan data dengan dimensi tinggi, Maldonado, </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2"> (2019) mengusulkan metode SMOTE-</span><span class="font2" style="font-style:italic;">Subset of Features</span><span class="font2"> (SMOTE-SF). Metode tersebut menambahkan pemeringkatan fitur sebelum didefinisikan sebagai tetangga, perhitungan jarak hanya menggunakan beberapa fitur. Menggunakan metrik baru berdasarkan jarak </span><span class="font2" style="font-style:italic;">Minkowski</span><span class="font2"> seperti jarak </span><span class="font2" style="font-style:italic;">Chebyshev</span><span class="font2"> dan </span><span class="font2" style="font-style:italic;">Manhattan</span><span class="font2">.</span></p>
<p><span class="font2">Perkembangan lain dari metode ini diusulkan oleh Fahrudin, </span><span class="font2" style="font-style:italic;">et al</span><span class="font2">. (2019) yakni AWH-SMOTE (</span><span class="font2" style="font-style:italic;">attribute weighted and kNN Hub</span><span class="font2">). Peningkatan kinerja dilakukan dengan mengidentifikasi </span><span class="font2" style="font-style:italic;">noise</span><span class="font2"> menggunakan </span><span class="font2" style="font-style:italic;">attribute weighted</span><span class="font2"> dan pengambilan sampel selektif dengan menghitung kemunculan data di semua kelas minoritas </span><span class="font2" style="font-style:italic;">kNN</span><span class="font2"> (</span><span class="font2" style="font-style:italic;">kNN hub</span><span class="font2">). Data minoritas dengan kemunculan jumlah data yang kecil memiliki nilai aman (</span><span class="font2" style="font-style:italic;">safe value</span><span class="font2">) yang rendah. Hal tersebut menunjukkan bahwa data minoritas tersebut jauh dari titik pusat kelas minoritas dan daerah tersebut memiliki banyak data mayoritas, sehingga data sintetis akan dibangun pada</span></p>
<p><span class="font2">daerah tersebut.</span></p>
<p><span class="font2">Selanjutnya, Guan, </span><span class="font2" style="font-style:italic;">et al</span><span class="font2">. &nbsp;&nbsp;(2020)</span></p>
<p><span class="font2">mengusulkan metode SMOTE-WENN yakni menggabungkan metode SMOTE dengan pendekatan pembersihan data </span><span class="font2" style="font-style:italic;">weighted edited nearest neighbor</span><span class="font2"> (WENN). WENN menghapus data kelas minoritas dan mayoritas yang tidak aman menggunakan </span><span class="font2" style="font-style:italic;">weighted distance function </span><span class="font2">dan kNN. </span><span class="font2" style="font-style:italic;">Weighted distance function </span><span class="font2">mempertimbangkan ketakseimbangan lokal dan </span><span class="font2" style="font-style:italic;">spacial sparsity</span><span class="font2">. Perkembangan dan modifikasi dari metode SMOTE masih terus dilakukan dalam penelitian untuk meningkatkan kinerja klasifikasi pada kasus data tidak seimbang.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font2" style="font-weight:bold;"><a name="bookmark9"></a>4. &nbsp;&nbsp;&nbsp;KESIMPULAN</span></h2></li></ul>
<p><span class="font2">Kelebihan dari metode SMOTE secara umum adalah tidak menyebabkan adanya informasi yang hilang, menghindari terjadinya </span><span class="font2" style="font-style:italic;">overfitting</span><span class="font2">, membangun wilayah keputusan yang lebih besar, serta mampu meningkatkan akurasi prediksi kelas minoritas. Kekurangan dari metode ini adalah, </span><span class="font2" style="font-style:italic;">overgeneralization</span><span class="font2"> yang menyebabkan terjadinya </span><span class="font2" style="font-style:italic;">overlapping</span><span class="font2">, tidak tepat digunakan pada kasus yang mempertimbangkan kepentingan fitur dan kumpulan data dengan dimensi tinggi. Untuk mengatasi permasalahan tersebut, beberapa perkembangan dan modifikasi dilakukan pada metode ini. Walaupun demikian, SMOTE tetap merupakan pelopor perkembangan teknik </span><span class="font2" style="font-style:italic;">oversampling</span><span class="font2"> yang menggunakan data sintetis.</span></p>
<h2><a name="bookmark10"></a><span class="font2" style="font-weight:bold;"><a name="bookmark11"></a>DAFTAR PUSTAKA</span></h2>
<p><span class="font2">Batista, G.E., Prati, R.C. and Monard, M.C., 2004. A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data. </span><span class="font2" style="font-style:italic;">ACM SIGKDD explorations newsletter</span><span class="font2">, 6(1), pp.20-29.</span></p>
<p><span class="font2">Brownlee, J., &nbsp;2020. Data Preparation for</span></p>
<p><span class="font2">Machine Learning. San Francisco: Machine Learning Mastery.</span></p>
<p><span class="font2">Bunkhumpornpat, C., Sinapiromsaran, K. and Lursinsap, C., 2009. Safe-Level-SMOTE: Safe-Level-Synthetic Minority OverSampling Technique for Handling the Class Imbalanced Problem. &nbsp;&nbsp;&nbsp;</span><span class="font2" style="font-style:italic;">Pacific-Asia</span></p>
<p><span class="font2" style="font-style:italic;">conference on knowledge discovery and data mining</span><span class="font2">, &nbsp;pp.475-482. Springer, Berlin,</span></p>
<p><span class="font2">Heidelberg.</span></p>
<p><span class="font2">Chawla, N.V., Bowyer, K.W., Hall, L.O. and Kegelmeyer, W.P., 2002. SMOTE: Synthetic</span></p>
<p><span class="font2">Minority Over-Sampling Technique. </span><span class="font2" style="font-style:italic;">Journal of artificial intelligence research</span><span class="font2">, &nbsp;&nbsp;16,</span></p>
<p><span class="font2">pp.321-357.</span></p>
<p><span class="font2">Chawla, N.V., Lazarevic, A., Hall, L.O. and Bowyer, K.W., &nbsp;2003. SMOTEBoost:</span></p>
<p><span class="font2">Improving Prediction of the Minority Class in Boosting. </span><span class="font2" style="font-style:italic;">European conference on principles of data mining and knowledge discovery</span><span class="font2">, pp.107-119. Springer, Berlin,</span></p>
<p><span class="font2">Heidelberg.</span></p>
<p><span class="font2">Douzas, G. and Bacao, F., 2017. Geometric SMOTE: &nbsp;Effective Oversampling for</span></p>
<p><span class="font2">Imbalanced Learning Through A Geometric Extension of SMOTE. </span><span class="font2" style="font-style:italic;">arXiv preprint arXiv:1709.07377</span><span class="font2">.</span></p>
<p><span class="font2">Fahrudin, T., Buliali, J.L. and Fatichah, C., 2019. Enhancing the Performance of SMOTE Algorithm by Using Attribute Weighting Scheme and New Selective Sampling Method for Imbalanced Data set. </span><span class="font2" style="font-style:italic;">Int J Innov Comput Inf Control</span><span class="font2">, 15, </span><span class="font3">pp.423-444.</span></p>
<p><span class="font3">Guan, H., Zhang, Y., Xian, M., Cheng, H.D. and Tang, X., 2021. SMOTE-WENN: Solving Class Imbalance and Small Sample Problems by Oversampling and Distance &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Scaling. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3" style="font-style:italic;">Applied</span></p>
<p><span class="font3" style="font-style:italic;">Intelligence</span><span class="font3">, 51(3), pp.1394-1409.</span></p>
<p><span class="font2">Han, H., Wang, W.Y. and Mao, B.H., 2005. Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning. </span><span class="font2" style="font-style:italic;">International conference on intelligent computing</span><span class="font2">, pp.878-887. Springer, Berlin, Heidelberg.</span></p>
<p><span class="font2">He, H., Bai, Y., Garcia, E.A. and Li, S., 2008. ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning. </span><span class="font2" style="font-style:italic;">IEEE international joint conference on neural networks (IEEE world congress on computational intelligence)</span><span class="font2">, pp.1322-1328.</span></p>
<p><span class="font2">Maldonado, S., López, J. and Vairetti, C., 2019. An Alternative SMOTE Oversampling Strategy &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;High-Dimensional</span></p>
<p><span class="font2">Datasets. </span><span class="font2" style="font-style:italic;">Applied Soft Computing</span><span class="font2">, &nbsp;76,</span></p>
<p><span class="font2">pp.380-389.</span></p>
<p><span class="font2">Ramentol, E., Caballero, Y., Bello, R. and Herrera, F., 2012. SMOTE-RSB*: A Hybrid Preprocessing Approach Based on Oversampling and Undersampling for High Imbalanced Data-Sets Using SMOTE and</span></p>
<p><span class="font2">Rough Sets Theory. </span><span class="font2" style="font-style:italic;">Knowledge and information systems</span><span class="font2">, 33(2), pp.245-265.</span></p>
<p><span class="font2">Ramyachitra, D. and Manikandan, P., 2014. Imbalanced Dataset &nbsp;Classification and</span></p>
<p><span class="font2">Solutions: A Review. </span><span class="font2" style="font-style:italic;">International Journal of Computing and Business Research (IJCBR)</span><span class="font2">, 5(4), pp.1-29.</span></p>
<p><span class="font2">Santoso, B., Wijayanto, H., Notodiputro, K.A. and Sartono, B., 2017. Synthetic Over</span></p>
<p><span class="font2">Sampling Methods for Handling Class Imbalanced Problems: A review. </span><span class="font2" style="font-style:italic;">IOP conference series: earth and environmental science</span><span class="font2">, 58(1). IOP Publishing.</span></p>
<p><span class="font2">Singh, P. dan Sharma, P. A. 2019. Analysis of Imbalanced Classification Algorithms: A Perspective View. </span><span class="font2" style="font-style:italic;">International Journal of Trend in Scientific Research and Development</span><span class="font2">, 3(2), pp.974-978.</span></p>
<p><span class="font1">240</span></p>