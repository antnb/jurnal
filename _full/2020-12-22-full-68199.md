---
layout: full_article
title: "Spatial Based Deep Learning Autonomous  Wheel Robot Using CNN"
author: "Eko Wahyu Prasetyo, Nambo Hidetaka, Dwi Arman Prasetya, Wahyu Dirgantara, Hari Fitria Windi"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-68199 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-68199"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-68199"  
comments: true
---

<p><span class="font0" style="font-weight:bold;">LONTAR KOMPUTER VOL. 11, NO. 3 DECEMBER 2020</span></p>
<p><span class="font0" style="font-weight:bold;">DOI : 10.24843/LKJITI.2020.v11.i03.p05</span></p>
<p><span class="font0" style="font-weight:bold;">Accredited B by RISTEKDIKTI Decree No. 51/E/KPT/2017</span></p>
<p><span class="font0" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font0" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font1" style="font-weight:bold;"><a name="bookmark1"></a>Spatial Based Deep Learning Autonomous Wheel Robot Using CNN</span></h1>
<p><span class="font0">Eko Wahyu Prasetyo<sup>a1</sup></span><span class="font0" style="font-weight:bold;">, </span><span class="font0">Hidetaka Nambo<sup>b2</sup></span><span class="font0" style="font-weight:bold;">, </span><span class="font0">Dwi Arman Prasetya<sup>a3</sup>, Wahyu Dirgantara<sup>a4</sup>, Hari Fitria Windi <sup>a5</sup></span></p>
<p><span class="font0"><sup>a</sup>Teknik Elektro, Universitas Merdeka Malang</span></p>
<p><span class="font0">Jalan Terusan Dieng No. 62-64 Malang, Jawa Timur, Indonesia</span></p>
<p><span class="font0" style="text-decoration:underline;"><sup>1</sup>prasetyoekowahyu7@gmail.com;</span><a href="mailto:arman.prasetya@unmer.ac.id"><span class="font0"><sup>3</sup></span><span class="font0" style="text-decoration:underline;">arman.prasetya@unmer.ac.id</span><span class="font0">;</span></a></p>
<p><a href="mailto:4wahyu.dirgantara@unmer.ac.id"><span class="font0" style="text-decoration:underline;"><sup>4</sup>wahyu.dirgantara@unmer.ac.id</span><span class="font0">;</span></a><a href="mailto:5harry.fw@unmer.ac.id"><span class="font0"> </span><span class="font0" style="text-decoration:underline;"><sup>5</sup>harry.fw@unmer.ac.id</span><span class="font0">;</span></a></p>
<p><span class="font0"><sup>b</sup> Artificial Intelligence, Kanazawa University Kakumamachi, Kanazawa, Ishikawa, Jepang </span><a href="mailto:2nambo@blitz.ec.t.kanazawa-u.ac.jp"><span class="font0" style="text-decoration:underline;"><sup>2</sup>nambo@blitz.ec.t.kanazawa-u.ac.jp</span><span class="font0">;</span></a></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font0" style="font-style:italic;">The development of technology is growing rapidly; one of the most popular among the scientist is robotics technology. Recently, the robot was created to resemble the function of the human brain. Robots can make decisions without being helped by humans, known as AI (Artificial Intelligent). Now, this technology is being developed so that it can be used in wheeled vehicles, where these vehicles can run without any obstacles. Furthermore, of research, Nvidia introduced an autonomous vehicle named Nvidia Dave-2, which became popular. It showed an accuracy rate of 90%. The CNN (Convolutional Neural Network) method is used in the track recognition process with input in the form of a trajectory that has been taken from several angles. The data is trained using Jupiter's notebook, and then the training results can be used to automate the movement of the robot on the track where the data has been retrieved. The results obtained are then used by the robot to determine the path it will take. Many images that are taken as data, precise the results will be, but the time to train the image data will also be longer. From the data that has been obtained, the highest train loss on the first epoch is 1.829455, and the highest test loss on the third epoch is 30.90127. This indicates better steering control, which means better stability.</span></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font0" style="font-style:italic;">Autonomous Wheel Robot, Nvidia, Artificial Intelligent, Convolutional Neural Network, Jupiter Note Book</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font0" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font0">Currently, the development of robotics is increasingly sophisticated, e.g., the use of a Preprepared trajectory on the Autonomous Wheel Robot. Furthermore, the Pre-prepared trajectory includes Artificial Intelligence (AI) [1] in the wheeled robot control system; therefore, it can move automatically. AI robot that was previously moving conventionally or driven by humans has started to be able to move automatically in this stage the robot can already be said machine learning. Machine learning and Artificial Intelligent have a difference where AI aims to increase the chances of success and not the accuration, while Machine Learning(ML) focuses on improving efficiency and no matter the success. AI's goal is to simulate natural intelligence in solving complex problems, whereas ML's goal is to learn from data to maximize machine performance[2]. AI is about making decisions, while ML allows systems to learn new things from data. AI will create systems to mimic humans and respond and behave accordingly. Other ML discs are involved in creating algorithms for self-learning.</span></p>
<p><span class="font0">In previous research, [3] developed an autonomous wheel robot using Raspberry Pi3 as a mini PC to process the data resource. It is used because it has a low price, but sometimes packet loss appears during the transmission data process in real-time because the ram is only 1 GB. Despite having a higher price, Nvidia Jetson Nanobot is equipped with an 8 GB ram, so the probability of</span></p>
<p><span class="font0">packet loss is smaller. In addition, the GPU owned by Nvidia also supports data image processing, so it can be processed faster.</span></p>
<p><span class="font0">The data that has been input and processed by the machine passes through two or more layers [4]. When more layers are used, the accuracy rate will also be increase [5]. This layer is a substitute for humans to make decisions independently without human assistance. One method of Deep learning is the Convolutional Neural Network or commonly abbreviated as CNN [6]. The CNN works by scan each section in the data to be used as a node. Each number in nodes is the result of matrix calculation. The robot can follow the track avoiding obstacles and doing work more efficiently and optimally.</span></p>
<p><span class="font0">Research related to autonomous driving in Artificial Intelligence laboratories has been done before [7], by simulating it using a program called the Carla simulator. It's an open-source one for autonomous car driving. Aims to continue the development to the next stage, this research focuses on making a prototype of an autonomous car that has three wheels; two regular wheels and one Omni wheel. The body is made of abs filaments that are printed using a 3D printer, the camera module as a place to pick up objects on the front. Thus the robot is expected to be able to operate on the ground, reading the area of the path and obstacles obtained through camera capture.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font0" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h2></li></ul>
<p><span class="font0">The method used in this study is the Resnet model of convolutional neural network (CNN) as follows :</span></p><img src="https://jurnal.harianregional.com/media/68199-1.png" alt="" style="width:376pt;height:171pt;">
<p><span class="font0" style="font-weight:bold;">Figure 1. </span><span class="font0">Design of research system developed</span></p>
<p><span class="font0">The design of the research system developed is illustrated in Figure 1. The camera retrieves digital imagery data passed to Nvidia Jetson Nanobot for processing, data from the received digital imagery will be processed with Nvidia Jetson Nanobot using Convolutional Neural Network method, this method is used to detect image data and train it, but this process is done separately in Personal Computer using Jupiter lab.</span></p>
<p><span class="font0">After Nvidia Jetson nano does training on the data that has been taken, the data will be reused as a reference to control the motor drivers who drive dc motors as actuators.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font0" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Deep learning</span></h2></li></ul>
<p><span class="font0">In the deep learning method, it is necessary to address significant problems in statistical machine learning [8]. The selection of a feature space that fits the representation learning approach becomes a problem in machine learning because the input space can be mapped to intermediate features. Deep neural networks have some difficulties [9], especially with high dimensional input spaces, e.g., images. This problem then encourages researchers to adopt a deep architecture, consisting of several layers with non-linear processing to solve the problem. Although there is already evidence of a successful case of a shallow network [10][11], the researchers found that</span></p>
<p><span class="font0">curse dimensionality becomes a problem in the case of multiple functions. Also, it was found that increasing the number of layers in the neural network can reduce the impact of backpropagation on the first layer. The descent of the gradient then tends to stop within the local minima or plateaus. However, this problem was solved in 2006 [12][13] through the introduction of layerwise unattended pre-training. In 2011, the Graphics Processing Unit (GPU) speed increased significantly, which made it possible to train Convolutional Neural Networks based architectures. Alexnet is won international competitions in 2011 and 2012. Then, in the following order of the following year, with the advancement of CPU and GPU, Deep learning and more for data-hungry deep learning techniques. The training and validation of motor sensor control models for urban driving in the real world were beyond the reach of most of the research groups [14]. Therefore, simulation testing is an alternative that can be done.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font0" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Neural Networks</span></h2></li></ul>
<p><span class="font0">The result of cross multiplication Feedforward neural networks or Multilayer Perceptrons (MLPs) [15] are the base of the Deep Learning Model. The main objective of the feed-forward network is to define the mapping of input </span><span class="font4">x </span><span class="font4" style="font-style:italic;">to y,y-f (x;</span><span class="font4"> 0) </span><span class="font0">categories and to estimate the value of the parameter θ, which is the result of the best function estimate [16][17].</span></p><img src="https://jurnal.harianregional.com/media/68199-2.png" alt="" style="width:266pt;height:155pt;">
<p><span class="font0" style="font-weight:bold;">Figure 2. </span><span class="font0">Example of MLPs With Hidden Layer</span></p>
<p><span class="font0">A feed-forward neural network has a structure consisting of many different functions. For example, Figure 2 consists of three different layer functions </span><span class="font4" style="font-style:italic;">f(1),f(2),dan</span><span class="font4"> f(3) </span><span class="font0">, forming. </span><span class="font4" style="font-style:italic;">f(x) = </span><span class="font4">f <sup>(3)</sup>(f<sup>(2)</sup> (f W (χ))))</span><span class="font0">For this case, </span><span class="font4" style="font-style:italic;">f</span><span class="font4"> (1) </span><span class="font0">referred to as the input layer, </span><span class="font4" style="font-style:italic;">f</span><span class="font4"> (2) </span><span class="font0">is the second layer, or the hidden layer, and then </span><span class="font4" style="font-style:italic;">f</span><span class="font4"> (3) </span><span class="font0">is the output layer referred to in Figure 2. The overall length of the chain is the depth of the model. From here, this process is called Deep Learning [18].</span></p>
<p><span class="font0">This can provide a feed-forward network as a transformation of a linear function </span><span class="font4">x </span><span class="font0">into a nonlinear function of </span><span class="font4">x</span><span class="font0">, or it can be expressed as </span><span class="font4" style="font-style:italic;">φ</span><span class="font0"> (</span><span class="font4">x</span><span class="font0">), where </span><span class="font4" style="font-style:italic;">φ</span><span class="font0"> is a non-linear transformation. So it can be said that </span><span class="font4" style="font-style:italic;">φ</span><span class="font0"> has a feature that describes </span><span class="font4">x </span><span class="font0">or provides a new representation of </span><span class="font4">x</span><span class="font0">. There are three general approaches [18]used to select </span><span class="font4" style="font-style:italic;">φ</span><span class="font0"> mapping. That is:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">a. &nbsp;&nbsp;&nbsp;Very generic based </span><span class="font4" style="font-style:italic;">φ</span><span class="font0"> approach.</span></p></li>
<li>
<p><span class="font0">b. &nbsp;&nbsp;&nbsp;Manually engineered </span><span class="font4" style="font-style:italic;">φ.</span></p></li>
<li>
<p><span class="font0">c. &nbsp;&nbsp;&nbsp;Parametrization of </span><span class="font4" style="font-style:italic;">φ</span><span class="font0"> with a representation of </span><span class="font4" style="font-style:italic;">φ(x; θ).</span></p></li></ul>
<p><span class="font0">The last third option uses the feed-forward network as an application to study deterministic mapping, stochastic mapping, functions with feedback, and probability distributions on a single vector [18]. Most of the Neural Network models are designed using this principle.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font0" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Convolutional Neural Network</span></h2></li></ul>
<p><span class="font0">CNN, introduced by LeCun, is mainly used to process data with a grid-like topology. It is simply neural networks that use convolutions instead of general multiplication. Usually, a convolutional network is composed of three-phase. The first phase, the convolutional layer, carries out convolution to produce a series of linear activations. In the second phase, the convoluted features</span></p>
<p><span class="font0">undergo a non-linear activation function, eventually, through the merged layer, which is called the downsampled feature [9][10][21]</span><span class="font4">.</span></p>
<div><img src="https://jurnal.harianregional.com/media/68199-3.png" alt="" style="width:174pt;height:105pt;">
<p><span class="font3">Input Layers</span></p>
<p><span class="font3">Hidden Layers</span></p>
<p><span class="font3">Output Layers</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68199-4.jpg" alt="" style="width:208pt;height:78pt;">
<p><span class="font0" style="font-weight:bold;">Figure 3. </span><span class="font0">MLPs and CNN architecture</span></p>
</div><br clear="all">
<p><span class="font0">With the general availability of data and escalating computing power, deep learning approaches as convolutional neural networks (CNN), as evident, outperform traditional approaches.[22] CNN consists of multiple layers, each of which has an Application Program Interface (API) or commonly called a simple application program interface. In Figure 3, CNN, with the initial input of a threedimensional block, will be transformed into a three-dimensional output with several differentiation functions that have or do not have parameters. CNN forms its neurons into three dimensions (length, width, and height) in one layer. The proposed system performance was evaluated based on mean square error (MSE) [23][24].</span></p>
<p><span class="font0">In CNN, there are two main processes, namely Feature Learning and Classification</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font0" style="font-weight:bold;"><a name="bookmark13"></a>2.3.1. &nbsp;&nbsp;&nbsp;Feature Learning</span></h2></li></ul>
<p><span class="font0">Feature Learning is the layers contained in Feature Learning, which is useful for translating input into features based on the characteristics of the input, which are in the form of numbers in vectors [25]. This feature extraction layer consists of a Convolutional Layer and a Pooling Layer.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">a. &nbsp;&nbsp;&nbsp;Convolutional Layer will calculate the output of neurons connected to the local area in the input [26].</span></p></li>
<li>
<p><span class="font0">b. &nbsp;&nbsp;&nbsp;The Rectified Linear Unit (ReLU) will abolish off the lost gradient by adjusting the element activation function as f (x) = max f, 0 (0, x) [27] element activation will be performed when on the verge of 0. Advantages and disadvantages of using ReLU can expedite the Stochastic gradient compared with Sigmoid / tanh function, RelU is linear Not using exponential operations such as sigmoid / tanh, by creating an activation matrix when the threshold is 0. ReLU training is carried out it becomes fragile and dies, a large gradient that flows through ReLU causes weight updates, neurons are no longer active on the data point. If this happens, the gradient that flows through the unit will forever be zero from that point.</span></p></li>
<li>
<p><span class="font0">c. &nbsp;&nbsp;&nbsp;The pooling layer is a layer that reduces the dimensions of the feature map or better known as the step for down sampling [28], that speeds up computation. Fewer parameters need to be updated, and overfitting is overcome. Pooling that is commonly used is Max Pooling and Average Pooling. Max Pooling to determine the maximum value of each filter shift, while Average Pooling will determine the average value.</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font0" style="font-weight:bold;"><a name="bookmark15"></a>2.3.2. &nbsp;&nbsp;&nbsp;Classification</span></h2></li></ul>
<p><span class="font0">Classification This layer is useful for classifying each neuron that features extracted previously. Consists of:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">a. &nbsp;&nbsp;&nbsp;Flatten is Reshape feature map into a vector, and then it can be used as input for the fully- connected layer [29].</span></p></li>
<li>
<p><span class="font0">b. &nbsp;&nbsp;&nbsp;Fully-connected The FC layer calculates the class score. Like a normal Neural Network and as the name suggests, every neuron in this layer will be connected to every number in the volume.</span></p></li>
<li>
<p><span class="font0">c. &nbsp;&nbsp;&nbsp;Softmax function calculates the probability of each target class over all possible target classes and will help to determine the target class for the input given. The advantage of using Softmax is that the probability of output ranges from zero to one, and the number of all probabilities will be equal to one. The softmax function used for the multiclassification model will return the probability that each class and the target class will have a high probability [30].</span></p></li></ul>
<p><span class="font0">In the convolution layer, the convolutional algorithm converts the image into a vector without losing spatial information, which MLPs cannot do. Mathematically, the discrete convolution operation between two functions f and g, denoted by the operator </span><span class="font4">*</span><span class="font0">, can be defined as:</span></p>
<p><a href="#bookmark16"><span class="font4" style="font-style:italic;">(f*g)(χ) = ∑<sub>t</sub>f(t)g(χ + t)</span><span class="font0">(1)</span></a></p>
<p><span class="font0">For a 2-dimensional image as input, the formula can be written as follows</span></p>
<p><a href="#bookmark17"><span class="font4" style="font-style:italic;">(I*K)(i,j) = ∑</span><span class="font2" style="font-style:italic;">N</span><span class="font4" style="font-style:italic;">∑nKm,n)K(i + mJ + n)</span><span class="font0">(2)</span></a></p>
<p><span class="font0">Since convolution is commutative, convolution can also be written as follows,</span></p>
<p><a href="#bookmark18"><span class="font4" style="font-style:italic;">(K * MJ) = ∑</span><span class="font2" style="font-style:italic;">N</span><span class="font4" style="font-style:italic;">∑nKi + m,j</span><span class="font4"> + </span><span class="font4" style="font-style:italic;">n)K(m,n)</span><span class="font0">(3)</span></a></p>
<p><span class="font0">From these equations (1 and 2), I is a two-dimensional input, while K is a two-dimensional convolutional kernel.</span></p><img src="https://jurnal.harianregional.com/media/68199-5.jpg" alt="" style="width:343pt;height:183pt;">
<p><span class="font0" style="font-weight:bold;">Figure 4. </span><span class="font0">2D convolution between 3 x 4 input and 2 x 2 kernel</span></p>
<p><span class="font0">The principle of 2D convolution is to shift the convolutional kernel on the input. At each index position shown in Figure 3, element-wise multiplications are computed, they are summed. Then the result value is as follows:</span></p>
<p><span class="font0">q= a.m + b.n + e.o + f.p &nbsp;r = b.m + c.n +f.o + g.p </span><span class="font4" style="font-style:italic;">s</span><span class="font0"> = </span><span class="font4" style="font-style:italic;">cm</span><span class="font0"> + </span><span class="font4" style="font-style:italic;">dn</span><span class="font0"> + </span><span class="font4" style="font-style:italic;">go</span><span class="font0"> + </span><span class="font4" style="font-style:italic;">gp</span></p>
<p><span class="font0">t = e.m + f.n + o.e + f.p &nbsp;&nbsp;&nbsp;u = f.m + g.n +j.o +k.p </span><span class="font4">v = g.m + h.n + k.o +l.p</span></p>
<p><span class="font0">Refer to Figure 4, the kernel slides by the number of strides. This helps the user in downsampling the image. There is also a parameter called padding, which we can set up to control the size of the output.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark19"></a><span class="font0" style="font-weight:bold;"><a name="bookmark20"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font0">This chapter will contain about testing the system on a device that is designed following the design to find out whether the tool is running as planned. Testing is carried out to compare the results of the theoretical design with the experimental results. From the test results.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font0" style="font-weight:bold;"><a name="bookmark22"></a>3.1. &nbsp;&nbsp;&nbsp;Result of Autonomous Wheel Robot</span></h2></li></ul>
<p><span class="font0">The robots developed in this project are autonomous wheel robots that have three wheels, for more details can be seen in the following image.</span></p>
<div><img src="https://jurnal.harianregional.com/media/68199-6.jpg" alt="" style="width:182pt;height:136pt;">
<p><span class="font0" style="font-weight:bold;">Figure 5 </span><span class="font0">Autonomous wheel robot</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68199-7.jpg" alt="" style="width:180pt;height:135pt;">
</div><br clear="all">
<p><span class="font0">Figure 5 is a robot developed using three wheels, one of its wheels uses an Omni wheel that can move 360 degrees, and the other two wheels using conventional wheels connected to the DC motor as an actuator of the developed robot, in addition to being the robot's data receiver uses a camera on the front to capture the data received, for the brain to move the robot using NVidia Jetson Nanobot, to process data that has been stored using Convolutional Neural Network method (CNN) so that the robot can move smoothly along the track.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark23"></a><span class="font0" style="font-weight:bold;"><a name="bookmark24"></a>3.2. &nbsp;&nbsp;&nbsp;Result of Camera Module Capture pi v2</span></h2></li></ul>
<p><span class="font0">The camera used is the pi v2 camera module with the resolution used to capture images is 256 x 256 pixels, which serve as an image capture of objects with detailed and bright results. This camera device is a track detector or track that will be processed on the NVIDIA Jetson Nanobot</span><span class="font0" style="font-style:italic;">.</span></p>
<div><img src="https://jurnal.harianregional.com/media/68199-8.jpg" alt="" style="width:290pt;height:186pt;">
<p><span class="font0" style="font-weight:bold;">Figure 6. </span><span class="font0">Display of Camera Testing at Jupiter Lab</span></p>
</div><br clear="all">
<p><span class="font0">Figure 6 Is a camera device that is connected to the Jetson Nanobot, which is placed on the front as a trajectory detector in the digital image camera testing device that is captured by this device, shown in Figure 6.</span></p>
<div><img src="https://jurnal.harianregional.com/media/68199-9.jpg" alt="" style="width:72pt;height:72pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68199-10.jpg" alt="" style="width:72pt;height:72pt;">
</div><br clear="all">
<div>
<p><span class="font4">a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b</span></p><img src="https://jurnal.harianregional.com/media/68199-11.jpg" alt="" style="width:189pt;height:72pt;">
<p><span class="font4">c &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d</span></p>
<p><span class="font0" style="font-weight:bold;">Figure 7. </span><span class="font0">a, b, c, d, image display</span></p>
</div><br clear="all">
<p><span class="font0">Figure 7. (a) is the cut that is used, but this cut is only the right-turning part, Figure 7. (b) is the cut used this cut is only a straight section but takes half the angle of the whole track, Figure 7. (c) is the piece of track used but this track cut is only the left-turning part, Figure 7. (d) is the cut of the straight track taken from the end of the track to the last point of the track Display of the digital image captured by the camera as well as data to be processed using the Convolutional Neural Network method to Detect Pathways.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark25"></a><span class="font0" style="font-weight:bold;"><a name="bookmark26"></a>3.3. &nbsp;&nbsp;&nbsp;Data training</span></h2></li></ul>
<p><span class="font0">In this test, using CNN Resnet 34 method as described in the previous chapter in this process that determines how high the level of accuracy will be obtained in this research here is the training data process.</span></p>
<div><img src="https://jurnal.harianregional.com/media/68199-12.jpg" alt="" style="width:173pt;height:121pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68199-13.jpg" alt="" style="width:173pt;height:129pt;">
<p><span class="font0" style="font-weight:bold;">Figure 8. </span><span class="font0">Track Used</span></p>
</div><br clear="all">
<p><span class="font0">Figure 8 shows a picture of the track used, after which the track is divided into parts. For example, see Figure 9.</span></p>
<div><img src="https://jurnal.harianregional.com/media/68199-14.jpg" alt="" style="width:135pt;height:94pt;">
<p><span class="font4">a</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68199-15.jpg" alt="" style="width:131pt;height:94pt;">
<p><span class="font4">b</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68199-16.jpg" alt="" style="width:298pt;height:94pt;">
<p><span class="font4">c</span></p>
<p><span class="font4">d</span></p>
<p><span class="font0" style="font-weight:bold;">Figure 9. </span><span class="font0">a, b, c, d, The Marked Path</span></p>
</div><br clear="all">
<p><span class="font0">Figure 9. (a) is the cut that is used, but this cut is only the right turning part, Figure 9. (b) is the cut used, but this cut is only a straight section but takes half the angle of the whole track, Figure 9. (c) is the cut used but this cut is only the left turning part, Figure 9. (d) is a cut of a straight track taken from the end of the track to the last point of the track showing the result of the track that has been marked using the Jupiter Lab Note Book. The green pointer is the place where the track is marked as a point for Nvidia to make a decision.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark27"></a><span class="font0" style="font-weight:bold;"><a name="bookmark28"></a>3.4. &nbsp;&nbsp;&nbsp;Model Test Results</span></h2></li></ul>
<p><span class="font0">Loss Function is a function in optimization problems to minimize the shortage or loss itself. The loss is damage or failure when training data, while the Number of Epoch is the number of a group of data repeatedly.</span></p><img src="https://jurnal.harianregional.com/media/68199-17.jpg" alt="" style="width:349pt;height:241pt;">
<p><span class="font0" style="font-weight:bold;">Figure 10. </span><span class="font0">Function Loss Using Resnet 34</span></p>
<p><span class="font0">Figure 10. is a graph that shows the results of training data where training is carried out 70 times using the Resnet_34 Model, where the hidden layers used are 34 hidden layers, there is a comparison between training loss and test loss where if you want accurate accuracy results, the results of the test loss must be valuable equal to or higher than the training loss chart. It shows that for epoch less than ≤ 5, loss function obtained quite high with the peak point is around epoch = 3 and down monotone close to zero after passing epoch = 5.</span></p>
<table border="1">
<tr><td colspan="3" style="vertical-align:top;">
<p><span class="font0" style="font-weight:bold;">Table 1. </span><span class="font0">Loss Function Resnet 34</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font4">Epoch</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Train_Loss</span></p></td><td style="vertical-align:bottom;">
<p><span class="font4">Test_Loss</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">1.829455</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.432907</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">1.19932</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">24.971218</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.193119</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">30.90127</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.121705</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">4.869393</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.059956</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">4.776933</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">6</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.062019</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.30119</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.05823</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.198895</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.049085</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">1.056565</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">9</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.053102</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.051256</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font4">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.056029</span></p></td><td style="vertical-align:middle;">
<p><span class="font4">0.036103</span></p></td></tr>
</table>
<p><span class="font0">Based on Table 1. above, it can be seen that the highest Train Loss is in the first epoch with a value of 1.829455, while the highest Test Loss is in the third epoch with a value of 30.90127 while the epoch value is below the third epoch on an average value between 0.1 to 4.8. In previous research[31], using Carla to simulate how the CNN method works and obtained results training loss 0.00271 and validation loss 0.051. while the results of this study were obtained train loss 1.829455 and test loss 30.90127 this shows the results obtained in accordance with expectations. The value represents that, the model needs a more considerable amount of data so that train loss ~ test loss.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark29"></a><span class="font0" style="font-weight:bold;"><a name="bookmark30"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font0">In this experiment, the Convolutional Neural Network deep learning method was used with the Resnet 34 models in the trajectory recognition process. To move smoothly, must take a picture of the trajectory from several angles not only take from one angle because the robot does not always move according to its path there must be a time when the robot moves out of the trajectory, and by the time it happens, the robot already has the data to make its own decision, the data image that has been stored next will be trained to get test loss and train loss values. From the data obtained, the highest train loss in the first epoch was 1.829455, and the highest test loss in the third epoch is 30.90127. The result obtained is then used by the robot to determine the path it will take. By adding the data, we want to train, we can reduce the level of loss that will be obtained, but the more data we train then, the longer it will take to train the data.</span></p>
<h2><a name="bookmark31"></a><span class="font0" style="font-weight:bold;"><a name="bookmark32"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font0">[1] &nbsp;&nbsp;&nbsp;D. A. Prasetya, P. T. Nguyen, R. Faizullin, I. Iswanto, and E. F. Armay, &quot;Resolving the shortest path problem using the haversine algorithm,&quot; </span><span class="font0" style="font-style:italic;">Journal of Critical Reviews</span><span class="font0">, vol. 7, no. 1, pp. 62–64, 2020, doi: 10.22159/jcr.07.01.11.</span></p></li>
<li>
<p><span class="font0">[2] &nbsp;&nbsp;&nbsp;S. . Chang </span><span class="font0" style="font-style:italic;">et al.</span><span class="font0">, &quot;Resonant scattering of energetic electrons in the plasmasphere by monotonic whistler-mode waves artificially generated by ionospheric modification,&quot; </span><span class="font0" style="font-style:italic;">Annales Geophysicae</span><span class="font0">, vol. 32, pp. 507–518, 2014.</span></p></li>
<li>
<p><span class="font0">[3] &nbsp;&nbsp;&nbsp;M. G. Bechtel, E. McEllhiney, M. Kim, and H. Yun, &quot;DeepPicar: A low-cost deep neural network-based autonomous car,&quot; </span><span class="font0" style="font-style:italic;">Proc. - 2018 IEEE International Conference on Embedded and Real-Time Computing Systems and Applications(RTCSA 2018)</span><span class="font0">, pp. 11–21, 2019, doi: 10.1109/RTCSA.2018.00011.</span></p></li>
<li>
<p><span class="font0">[4] &nbsp;&nbsp;&nbsp;C. L. Zhang and J. Wu, &quot;Improving CNN linear layers with power mean non-linearity,&quot; </span><span class="font0" style="font-style:italic;">Pattern Recognition</span><span class="font0">, vol. 89, pp. 12–21, 2019, doi: 10.1016/j.patcog.2018.12.029.</span></p></li>
<li>
<p><span class="font0">[5] &nbsp;&nbsp;&nbsp;D. A. Prasetya and I. Mujahidin, &quot;2.4 GHz Double Loop Antenna with Hybrid Branch-Line 90-Degree Coupler for Widespread Wireless Sensor,&quot; in </span><span class="font0" style="font-style:italic;">2020 10th Electrical Power, Electronics, Communications, Controls and Informatics Seminar (EECCIS)</span><span class="font0">, Aug. 2020, pp. 298–302, doi: 10.1109/EECCIS49483.2020.9263477.</span></p></li>
<li>
<p><span class="font0">[6] &nbsp;&nbsp;&nbsp;J. Sun, Y. Fu, S. Li, J. He, C. Xu, and L. Tan, &quot;Sequential human activity recognition based on deep convolutional network and extreme learning machine using wearable sensors,&quot; </span><span class="font0" style="font-style:italic;">Journal of Sensors</span><span class="font0">, vol. 2018, no. 1, 2018, doi: 10.1155/2018/8580959.</span></p></li>
<li>
<p><span class="font0">[7] &nbsp;&nbsp;&nbsp;W. Dharmawan and H. Nambo, &quot;End-to-End Xception Model Implementation on Carla Self Driving Car in Moderate Dense Environment,&quot; </span><span class="font0" style="font-style:italic;">AICCC 2019: Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing Conference</span><span class="font0">, pp. 139–143, &nbsp;2019, doi:</span></p></li></ul>
<p><span class="font0">10.1145/3375959.3375969.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[8] &nbsp;&nbsp;&nbsp;Z. Q. Zhao, P. Zheng, S. T. Xu, and X. Wu, &quot;Object Detection with Deep Learning: A Review,&quot; </span><span class="font0" style="font-style:italic;">IEEE Transactions on Neural Networks and Learning Systems</span><span class="font0">, vol. 30, no. 11, pp. 3212– 3232, 2019, doi: 10.1109/TNNLS.2018.2876865.</span></p></li>
<li>
<p><span class="font0">[9] &nbsp;&nbsp;&nbsp;A. R. Pathak, M. Pandey, and S. Rautaray, &quot;Application of Deep Learning for Object Detection,&quot; </span><span class="font0" style="font-style:italic;">Procedia Computer Science</span><span class="font0">, vol. 132, no. Iccids, pp. 1706–1717, 2018, doi: 10.1016/j.procs.2018.05.144.</span></p></li>
<li>
<p><span class="font0">[10] &nbsp;&nbsp;&nbsp;N. Akhtar and A. Mian, &quot;Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey,&quot; </span><span class="font0" style="font-style:italic;">IEEE Access</span><span class="font0">, vol. 6, pp. 14410–14430, &nbsp;&nbsp;&nbsp;2018, doi:</span></p></li></ul>
<p><span class="font0">10.1109/ACCESS.2018.2807385.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[11] &nbsp;&nbsp;&nbsp;N. F. Ardiansyah, A. Rabi’, D. Minggu, and W. Dirgantara, “Computer Vision Untuk Pengenalan Obyek Pada Peluncuran Roket Kendaraan Tempur,” </span><span class="font0" style="font-style:italic;">JASIEK (Jurnal Apl. Sains, Informasi, Elektron. dan Komputer)</span><span class="font0">, vol. 1, no. 1, 2019, doi: 10.26905/jasiek.v1i1.3142.</span></p></li>
<li>
<p><span class="font0">[12] &nbsp;&nbsp;&nbsp;H. Yu, D. C. Samuels, Y. yong Zhao, and Y. Guo, &quot;Architectures and accuracy of artificial neural network for disease classification from omics data,&quot; </span><span class="font0" style="font-style:italic;">BMC Genomics</span><span class="font0">, vol. 20, no. 1, pp. 1–12, 2019, doi: 10.1186/s12864-019-5546-z.</span></p></li>
<li>
<p><span class="font0">[13] &nbsp;&nbsp;&nbsp;A. A. Elsharif, I. M. Dheir, A. Soliman, A. Mettleq, and S. S. Abu-naser, &quot;Potato Classification Using Deep Learning,&quot; </span><span class="font0" style="font-style:italic;">Advances in Animal Biosciences</span><span class="font0">, vol. 3, no. 12, pp. 1–8, 2019.</span></p></li>
<li>
<p><span class="font0">[14] &nbsp;&nbsp;&nbsp;A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, &quot;CARLA: An Open Urban Driving Simulator,&quot; </span><span class="font0" style="font-style:italic;">1st Conference on Robot Learning (CoRL 2017)</span><span class="font0">, no. CoRL, pp. 1–16, 2017, [Online]. Available: </span><a href="http://arxiv.org/abs/1711.03938"><span class="font0">http://arxiv.org/abs/1711.03938</span></a><span class="font0">.</span></p></li>
<li>
<p><span class="font0">[15] &nbsp;&nbsp;&nbsp;W. Xiang, D. M. Lopez, P. Musau, and T. T. Johnson, &quot;Reachable Set Estimation and Verification for Neural Network Models of Nonlinear Dynamic Systems,&quot; </span><span class="font0" style="font-style:italic;">Safe, Autonomous and Intelligent Vehicles</span><span class="font0">, pp. 123–144, 2019, doi: 10.1007/978-3-319-97301-2_7.</span></p></li>
<li>
<p><span class="font0">[16] &nbsp;&nbsp;&nbsp;A. A. Heidari, H. Faris, I. Aljarah, and S. Mirjalili, &quot;An efficient hybrid multilayer perceptron neural network with grasshopper optimization,&quot; </span><span class="font0" style="font-style:italic;">Soft Computing</span><span class="font0">, vol. 23, no. 17, pp. 7941– 7958, 2019, doi: 10.1007/s00500-018-3424-2.</span></p></li>
<li>
<p><span class="font0">[17] &nbsp;&nbsp;&nbsp;W. A. H. M. Ghanem, A. Jantan, S. A. A. Ghaleb, and A. B. Nasser, &quot;An Efficient Intrusion Detection Model Based on Hybridization of Artificial Bee Colony and Dragonfly Algorithms for Training Multilayer Perceptrons,&quot; </span><span class="font0" style="font-style:italic;">IEEE Access</span><span class="font0">, vol. 8, pp. 130452–130475, 2020, doi: 10.1109/access.2020.3009533.</span></p></li>
<li>
<p><span class="font0">[18] &nbsp;&nbsp;&nbsp;J. Heaton, &quot;Ian Goodfellow, Yoshua Bengio, and Aaron Courville: Deep learning, &quot;&nbsp;</span><span class="font0" style="font-style:italic;">Genetic Programming and Evolvable Machines</span><span class="font0">, vol. 19, no. 1–2, pp. 305–307, 2018, doi: 10.1007/s10710-017-9314-z.</span></p></li>
<li>
<p><span class="font0">[19] &nbsp;&nbsp;&nbsp;W. Dharmawan, &quot;End-to-End Sequential Input with Time Distributed Model for Carla Self Driving Car in Moderate Dense Environment,&quot; 2019.</span></p></li>
<li>
<p><span class="font0">[20] &nbsp;&nbsp;&nbsp;C. Zhao, B. Ni, J. Zhang, Q. Zhao, W. Zhang, and Q. Tian, &quot;Variational convolutional neural network pruning,&quot; </span><span class="font0" style="font-style:italic;">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="font0">, vol. 2019-June, pp. 2775–2784, 2019, doi: 10.1109/CVPR.2019.00289.</span></p></li>
<li>
<p><span class="font0">[21] &nbsp;&nbsp;&nbsp;Y. Liu, B. Fan, S. Xiang, and C. Pan, &quot;Relation-shape convolutional neural network for point cloud analysis,&quot; </span><span class="font0" style="font-style:italic;">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="font0">, vol. 2019-June, pp. 8887–8896, 2019, doi: 10.1109/CVPR.2019.00910.</span></p></li>
<li>
<p><span class="font0">[22] &nbsp;&nbsp;&nbsp;A. Amidi, S. Amidi, D. Vlachakis, V. Megalooikonomou, N. Paragios, and E. I. Zacharaki, &quot;EnzyNet: enzyme classification using 3D convolutional neural networks on spatial representation,&quot; </span><span class="font0" style="font-style:italic;">Bioinformatics and Genomics</span><span class="font0">, pp. 1–11, 2017.</span></p></li>
<li>
<p><span class="font0">[23] &nbsp;&nbsp;&nbsp;D. A. Prasetya, T. Yasuno, H. Suzuki, and A. Kuwahara, &quot;Cooperative Control System of Multiple Mobile Robots Using Particle Swarm Optimization with Obstacle Avoidance for Tracking Target,&quot; </span><span class="font0" style="font-style:italic;">Journal of Signal Processing</span><span class="font0">, vol. 17, no. 5, pp. 199–206, 2013.</span></p></li>
<li>
<p><span class="font0">[24] &nbsp;&nbsp;&nbsp;A. P. Sari, H. Suzuki, T. Kitajima, T. Yasuno, and D. A. Prasetya, &quot;Prediction Model of Wind Speed and Direction Using Deep Neural Network,&quot; </span><span class="font0" style="font-style:italic;">JEEMECS (Journal of Electrical Engineering, Mechatronic and Computer Science)</span><span class="font0">, vol. 3, no. 1, pp. 1–10, 2020, doi: 10.26905/jeemecs.v3i1.3946.</span></p></li>
<li>
<p><span class="font0">[25] &nbsp;&nbsp;&nbsp;A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox, &quot;Discriminative unsupervised feature learning with convolutional neural networks,&quot; </span><span class="font0" style="font-style:italic;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="font0">, vol. 1, no. January, pp. 766–774, 2014.</span></p></li>
<li>
<p><span class="font0">[26] &nbsp;&nbsp;&nbsp;S. Wang, J. Sun, I. Mehmood, C. Pan, Y. Chen, and Y. D. Zhang, &quot;Cerebral micro-bleeding identification based on a nine-layer convolutional neural network with stochastic pooling,&quot; </span><span class="font0" style="font-style:italic;">Concurrency and Computation Practice and Experience</span><span class="font0">, vol. 32, no. 1, pp. 1–16, 2020, doi: 10.1002/cpe.5130.</span></p></li>
<li>
<p><span class="font0">[27] &nbsp;&nbsp;&nbsp;A. F. Agarap, &quot;Deep Learning using Rectified Linear Units (ReLU),&quot; </span><span class="font0" style="font-style:italic;">Neural and Evolutionary Computing</span><span class="font0">, no. 1, pp. 2–8, 2018.</span></p></li>
<li>
<p><span class="font0">[28] &nbsp;&nbsp;&nbsp;L. Jing, M. Zhao, P. Li, and X. Xu, &quot;A convolutional neural network based feature learning and fault diagnosis method for the condition monitoring of gearbox,&quot; </span><span class="font0" style="font-style:italic;">Measurement. Journal of the International Measurement Confederation (IMEKO)</span><span class="font0">, vol. 111, pp. 1–10, 2017, doi: 10.1016/j.measurement.2017.07.017.</span></p></li>
<li>
<p><span class="font0">[29] &nbsp;&nbsp;&nbsp;M. Yu </span><span class="font0" style="font-style:italic;">et al.</span><span class="font0">, &quot;Gradiveq: Vector quantization for bandwidth-efficient gradient aggregation in distributed CNN training,&quot; </span><span class="font0" style="font-style:italic;">Advances In Neural Information Processing Systems 31 (NIPS 2018)</span><span class="font0">, vol. 2018-Decem, no. NeurIPS, pp. 5123–5133, 2018.</span></p></li>
<li>
<p><span class="font0">[30] &nbsp;&nbsp;&nbsp;S. Chen, C. Zhang, M. Dong, J. Le, and M. Rao, “Chen_Using_Ranking-CNN_for_CVPR_2017_paper.pdf,” </span><span class="font0" style="font-style:italic;">Cvpr</span><span class="font0">, pp. 5183–5192, 2017.</span></p></li>
<li>
<p><span class="font0">[31] &nbsp;&nbsp;&nbsp;C. Science, &quot;End-to-End Spatial Based Deep Neural Network on Self-Driving Car,&quot; 2020.</span></p></li></ul>
<p><span class="font0">177</span></p>