---
layout: full_article
title: "Model Object Detection Neural Network Berbasis Hand Gesture Recognition sebagai Kontrol Prostesis Tangan"
author: "I Made Esa Darmayasa Adi Putra, I Made Putra Arya Winata, Ilham Fauzi, Karuna Sindhu Krishna Prasad, I Wayan Widhiada"
categories: jte
canonical_url: https://jurnal.harianregional.com/jte/full-98236 
citation_abstract_html_url: "https://jurnal.harianregional.com/jte/id-98236"
citation_pdf_url: "https://jurnal.harianregional.com/jte/full-98236"  
comments: true
---

<p><span class="font10" style="font-style:italic;">Majalah Ilmiah Teknologi Elektro, Vol.22, No.2, Juli-Desember 2023</span></p>
<p><span class="font10" style="font-style:italic;">DOI: </span><a href="https://doi.org/10.24843/MITE.2023.v22i02.P05"><span class="font10" style="font-style:italic;">https://doi.org/10.24843/MITE.2023.v22i02.P05</span></a><span class="font10"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;185</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font12" style="font-weight:bold;"><a name="bookmark1"></a>Model </span><span class="font12" style="font-weight:bold;font-style:italic;">Object Detection Neural Network</span><span class="font12" style="font-weight:bold;"> Berbasis </span><span class="font12" style="font-weight:bold;font-style:italic;">Hand Gesture Recognition</span><span class="font12" style="font-weight:bold;"> sebagai Kontrol Prostesis Tangan</span></h1>
<h2><a name="bookmark2"></a><span class="font11"><a name="bookmark3"></a>I.M.P. Arya Winata<sup>1</sup>, Karuna K.S Prasad<sup>2</sup>, Ilham Fauzi<sup>3</sup>, I.M.E. Darmayasa Adiputra<sup>4</sup>, I.W. Widhiada<sup>5</sup></span></h2>
<p><span class="font9" style="font-style:italic;">[Submission: 13-02-2023, Accepted: 29-03-2023]</span></p>
<p><span class="font9" style="font-weight:bold;font-style:italic;">Abstract</span><span class="font9" style="font-weight:bold;">— </span><span class="font9" style="font-weight:bold;font-style:italic;">Most people with disabilities due to accident injuries are patients after an arm amputation which can cause psychological disorders and even major trauma. The urgency of hand prosthesis functionality is increasingly needed. Hand gesture recognition (HGR) can be used to control hand prostheses, judging from the similarity in shape of objects/objects that tend to make the same hand movements. This development uses three common types of movement, namely pinch, pick, and grab. Developing a neural network model capable of implementing this concept is necessary. The neural network model developed uses the YOLOV7 and YOLOV7tiny pre-trained networks with datasets collected through the public image data scrapping method. The dataset is 317 images and 2278 object labels with a training ratio of 80:20 testing. The training process uses the Pytorch framework with 300 epochs. The results of the loss values for each epoch show that the model is trainable in the given dataset. The training results are then evaluated by evaluating number of parameters, frame per seconds (FPS) and mean average precision (mAP) using a testing dataset. The overall results show the highest evaluation metrics in the model, with YOLOV7 pretrained with parameter number of 36,9 million, FPS of 161, and mAP of 98,11%. The model has the potential to be developed and implemented as a support for the control functionality of hand prostheses.</span></p>
<p><span class="font9" style="font-weight:bold;font-style:italic;">Intisari</span><span class="font9" style="font-weight:bold;">— Penyandang disabilitas akibat cedera kecelakaan sebagian besar merupakan pasien pasca amputasi lengan yang dapat menyebabkan gangguan psikologis, bahkan trauma besar. Dengan demikian, urgensi fungsionalitas prostesis tangan semakin dibutuhkan. </span><span class="font9" style="font-weight:bold;font-style:italic;">Hand gesture recognition (HGR)</span><span class="font9" style="font-weight:bold;"> dapat dimanfaatkan sebagai kontrol prostesis tangan, meninjau dari kesamaan bentuk objek/benda memberikan kecenderungan terhadap gerakan tangan yang sama. Pengembangan ini menggunakan tiga jenis gerakan yang umum, yaitu </span><span class="font9" style="font-weight:bold;font-style:italic;">pinch, pick</span><span class="font9" style="font-weight:bold;">, dan </span><span class="font9" style="font-weight:bold;font-style:italic;">grab</span><span class="font9" style="font-weight:bold;">. Diperlukan pengembangan model </span><span class="font9" style="font-weight:bold;font-style:italic;">neural network</span><span class="font9" style="font-weight:bold;"> yang mampu mengimplementasikan konsep tersebut. Model </span><span class="font9" style="font-weight:bold;font-style:italic;">neural network</span><span class="font9" style="font-weight:bold;"> yang dikembangkan menggunakan </span><span class="font9" style="font-weight:bold;font-style:italic;">pretrained network YOLOV7</span><span class="font9" style="font-weight:bold;"> dan </span><span class="font9" style="font-weight:bold;font-style:italic;">YOLOV7tiny</span><span class="font9" style="font-weight:bold;"> dengan </span><span class="font9" style="font-weight:bold;font-style:italic;">dataset</span><span class="font9" style="font-weight:bold;"> yang dikumpulkan melalui metode </span><span class="font9" style="font-weight:bold;font-style:italic;">scrapping data</span><span class="font9" style="font-weight:bold;"> gambar publik. </span><span class="font9" style="font-weight:bold;font-style:italic;">Dataset</span><span class="font9" style="font-weight:bold;"> yang diperoleh sejumlah 317 gambar, 2278 label objek dengan rasio </span><span class="font9" style="font-weight:bold;font-style:italic;">training</span><span class="font9" style="font-weight:bold;"> dengan </span><span class="font9" style="font-weight:bold;font-style:italic;">testing</span><span class="font9" style="font-weight:bold;"> 80:20. Proses </span><span class="font9" style="font-weight:bold;font-style:italic;">training</span><span class="font9" style="font-weight:bold;"> menggunakan </span><span class="font9" style="font-weight:bold;font-style:italic;">framework Pytorch</span><span class="font9" style="font-weight:bold;"> dengan 300 </span><span class="font9" style="font-weight:bold;font-style:italic;">epoch</span><span class="font9" style="font-weight:bold;">. Hasil nilai </span><span class="font9" style="font-weight:bold;font-style:italic;">loss</span><span class="font9" style="font-weight:bold;"> tiap </span><span class="font9" style="font-weight:bold;font-style:italic;">epoch </span><span class="font9" style="font-weight:bold;">menunjukkan model </span><span class="font9" style="font-weight:bold;font-style:italic;">trainable</span><span class="font9" style="font-weight:bold;"> pada </span><span class="font9" style="font-weight:bold;font-style:italic;">dataset</span><span class="font9" style="font-weight:bold;"> yang diberikan. Hasil </span><span class="font9" style="font-weight:bold;font-style:italic;">training</span><span class="font9" style="font-weight:bold;"> selanjutnya dievaluasi dengan </span><span class="font9" style="font-weight:bold;font-style:italic;">evaluation metrics</span><span class="font9" style="font-weight:bold;"> jumlah</span></p>
<p><span class="font9" style="font-weight:bold;">parameter</span><span class="font9" style="font-weight:bold;font-style:italic;">, frame per seconds (FPS)</span><span class="font9" style="font-weight:bold;"> dan </span><span class="font9" style="font-weight:bold;font-style:italic;">mean average precision (mAP)</span><span class="font9" style="font-weight:bold;"> menggunakan </span><span class="font9" style="font-weight:bold;font-style:italic;">testing dataset</span><span class="font9" style="font-weight:bold;">. Hasil </span><span class="font9" style="font-weight:bold;font-style:italic;">overall</span><span class="font9" style="font-weight:bold;"> menunjukkan </span><span class="font9" style="font-weight:bold;font-style:italic;">evaluation metrics</span><span class="font9" style="font-weight:bold;"> tertinggi pada model dengan </span><span class="font9" style="font-weight:bold;font-style:italic;">pretrained YOLOV7</span><span class="font9" style="font-weight:bold;"> dengan jumlah parameter 36,9 juta, </span><span class="font9" style="font-weight:bold;font-style:italic;">FPS</span><span class="font9" style="font-weight:bold;"> 161, dan </span><span class="font9" style="font-weight:bold;font-style:italic;">mAP </span><span class="font9" style="font-weight:bold;">98,11%. Dengan hasil ini, model memiliki potensi untuk dikembangkan dan diimplementasikan sebagai penunjang fungsionalitas kontrol prostesis tangan.</span></p>
<p><span class="font9" style="font-weight:bold;font-style:italic;">Kata Kunci</span><span class="font9" style="font-weight:bold;">— </span><span class="font9" style="font-weight:bold;font-style:italic;">hand gesture recognition</span><span class="font9" style="font-weight:bold;">, kontrol prostesis tangan, </span><span class="font9" style="font-weight:bold;font-style:italic;">object detection, neural network</span><span class="font9" style="font-weight:bold;">.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font10"><a name="bookmark5"></a>I.</span><span class="font10" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Pendahuluan</span></h3></li></ul>
<p><span class="font10">Tidak sedikit penyandang disabilitas yang merupakan pasien pasca amputasi lengan dan tangan akibat cedera dari kecelakaan besar. Pada tahun 2022, Asia Tenggara merupakan salah satu kawasan dengan angka amputasi tinggi [1]. Pasien pasca amputasi lengan dan tangan harus menyesuaikan keterbatasan fisiknya dalam berkegiatan. Hal ini dapat mempengaruhi kondisi psikologis pasien, khususnya pasien dengan trauma besar yang belum mampu menerima diri sepenuhnya. Tingkat depresi dan kecemasan pada pasien pasca amputasi menunjukkan nilai yang relatif tinggi antara 20,6% hingga 63% untuk depresi dan 25,45% hingga 57% untuk kecemasan [2]. Hal ini diperparah dengan fakta bahwa aanggota gerak menjadi bagian tubuh yang mengalami cedera terbanyak akibat kecelakaan seperti yang ditunjukkan pada Gambar 1: [3]. Merujuk permasalahan ini, suatu alat bantu prostesis dibutuhkan untuk memperbaiki kualitas hidup pasien pasca amputasi lengan dan tangan.</span></p>
<h2><a name="bookmark6"></a><span class="font3"><a name="bookmark7"></a>Proporsi Bagian Tubuh yang Terkena Cedera Akibat Kecelakaan</span></h2>
<p><span class="font2">80</span></p>
<p><span class="font2">60</span></p>
<p><span class="font2">40</span></p>
<p><span class="font2">20</span></p>
<p><span class="font2">0</span></p>
<p><span class="font1">Kepala &nbsp;&nbsp;&nbsp;Dada Punggung Perut Anggota Anggota</span></p>
<p><span class="font1">gerak atas gerak bawah</span></p>
<p><span class="font8">Gambar 1: Proporsi bagian tubuh yang terkena cedera akibat kecelakaan</span></p>
<p><span class="font10">Dalam pengembangan prostesis tangan, aspek fungsional menjadi salah satu perhatian yang penting. Hal ini disebabkan karena prostesis harus mampu mensubstitusi semirip mungkin dengan tangan asli yang memiliki gerakan kompleks dan mengambil andil penting dalam kegiatan sehari-hari manusia</span></p>
<div><img src="https://jurnal.harianregional.com/media/98236-1.png" alt="" style="width:179pt;height:65pt;">
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font10">[4] . Merujuk pada permasalahan tersebut, dibutuhkan suatu konsep yang mampu memberikan pemodelan jelas terhadap gerakan tangan pada prostesis tangan.</span></p></li></ul>
<p><span class="font10">Dalam aplikasinya, gerakan tangan yang presisi selalu memiliki tantangan dengan adanya background noise yang menyebabkan perlunya desain sistem pengenalan gerakan yang efisien. Oleh karena itu, ketepatan sistem </span><span class="font10" style="font-style:italic;">hand gesture recognition</span><span class="font10"> (</span><span class="font10" style="font-style:italic;">HGR</span><span class="font10">) masih memberikan beberapa tantangan bagi para peneliti. Salah satu pendekatan umum untuk rekognisi gerakan tangan statis adalah dengan berbasis computer vision melalui akuisisi data, segmentasi wilayah tangan, ekstraksi fitur, dan klasifikasi gerakan berdasarkan fitur yang teridentifikasi [4,5,6]. Proses akuisisi data untuk pengenalan gerakan tangan berbasis penglihatan dilakukan dengan menggunakan sensor seperti kamera web [7].</span></p>
<p><span class="font10">Merujuk pada kondisi tersebut, </span><span class="font10" style="font-style:italic;">HGR</span><span class="font10"> memiliki potensi untuk diimplementasikan pada kontrol prostesis tangan, mengingat jenis gerakan tangan cenderung akan memiliki pola tertentu. Pola yang dimaksud ini salah satunya merujuk pada bentuk objek yang hendak diambil. Dengan kata lain, bentuk objek/benda yang mirip akan memiliki kecenderungan gerakan tangan yang sama untuk mengambilnya [8]. Pada penelitian ini, pengembangan model </span><span class="font10" style="font-style:italic;">neural network</span><span class="font10"> dilakukan untuk mencapai fungsionalitas </span><span class="font10" style="font-style:italic;">HGR</span><span class="font10"> tersebut. Hal ini dikarenakan model </span><span class="font10" style="font-style:italic;">neural</span><span class="font10"> network telah menunjukkan kemampuan yang sangat baik dalam menginterpretasikan gambar ke dalam suatu klasifikasi [9]. Terlebih lagi, model </span><span class="font10" style="font-style:italic;">neural network</span><span class="font10"> dalam hal deteksi objek (</span><span class="font10" style="font-style:italic;">object detection</span><span class="font10">) telah dikembangkan secara lebih spesifik. Pada penelitian ini, pengembangan model </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> akan dikaji.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark8"></a><span class="font10"><a name="bookmark9"></a>II.</span><span class="font10" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Penelitian Terkait</span></h3></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font10" style="font-style:italic;">A. &nbsp;&nbsp;&nbsp;Object Detection Neural Network</span></p></li></ul>
<p><span class="font10">Makna dari </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> adalah membuat algoritma dalam mengidentifikasi dan mengklasifikasi objek dari sebuah </span><span class="font10" style="font-style:italic;">frame</span><span class="font10">/gambar. </span><span class="font10" style="font-style:italic;">Object detection</span><span class="font10"> adalah area penting dari </span><span class="font10" style="font-style:italic;">computer vision</span><span class="font10"> dan memiliki aplikasi penting dalam penelitian ilmiah dan produksi industri praktis, seperti deteksi wajah, deteksi teks, deteksi manusia, deteksi kendaraan, dan deteksi citra medis. Perkembangan dari </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> ini dipengaruhi oleh beberapa faktor, seperti kemampuan komputasi dan </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10"> dalam penerapan </span><span class="font10" style="font-style:italic;">artificial neural network</span><span class="font10">. Hal ini menyebabkan, algoritma </span><span class="font10" style="font-style:italic;">object detection </span><span class="font10">tradisional masih sering digunakan, dalam bidang </span><span class="font10" style="font-style:italic;">computer vision</span><span class="font10">, seperti: </span><span class="font10" style="font-style:italic;">deformable parts model</span><span class="font10"> (DPM) dan </span><span class="font10" style="font-style:italic;">selective search</span><span class="font10"> [10,11]. Arsitektur dasar dari algoritma </span><span class="font10" style="font-style:italic;">object detection </span><span class="font10">tradisional tersebut telah mampu memberikan interpretasi dasar yang dapat dimanfaatkan dalam bidang bidang yang disebutkan sebelumnya namun tentu masih dapat dikembangkan lagi.</span></p>
<p><span class="font10">Peningkatan kemampuan komputasi dan ketersediaan jumlah dataset gambar memberikan dampak yang baik dalam pengembangan </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> dengan basis </span><span class="font10" style="font-style:italic;">Deep Convolution Neural Network</span><span class="font10"> (DCNN). Sebelum adanya DCNN, sebuah model CNN diajukan dengan sebutan AlexNet, mampu memberikan nilai terbaik pada </span><span class="font10" style="font-style:italic;">metrics evaluation top-5 score</span><span class="font10"> dengan menggunakan ImageNet dataset [12]. </span><span class="font10" style="font-style:italic;">Top-5 score</span><span class="font10"> ini merupakan metode evaluasi model dengan melihat nilai sebenarnya (</span><span class="font10" style="font-style:italic;">ground truth</span><span class="font10">) terdapat pada 5 prediksi teratas oleh model. Setelah melihat dampak dari penerapan AlexNet</span></p>
<p><span class="font10">pada dataset tersebut, pengembangan dari </span><span class="font10" style="font-style:italic;">object detection </span><span class="font10">semakin pesat dalam penerapan konsep </span><span class="font10" style="font-style:italic;">neural network</span><span class="font10">. Pengembangan selanjutnya adalah model RCNN (Region based-CNN) [13], yang mengajukan konsep </span><span class="font10" style="font-style:italic;">selective search </span><span class="font10">dalam menentukan objek (</span><span class="font10" style="font-style:italic;">region proposal</span><span class="font10">) dan dilanjutkan dengan klasifikasi oleh CNN sebagai </span><span class="font10" style="font-style:italic;">feature extraction</span><span class="font10"> pada setiap objek yang didapat. Metode ini dianggap sebagai multistage detector karena melalui dua tahapan utama, yaitu penentuan region objek dan training klasifikasi objek. Sebagai optimalisasi, sistem </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> dengan konsep singlestage detector diajukan dengan sebutan YOLO (You Only Look Once: Unified, Real-Time Object Detection) [14,15]. </span><span class="font10" style="font-style:italic;">Singlestage detector</span><span class="font10"> YOLO ini dibuat dengan konsep tanpa adanya </span><span class="font10" style="font-style:italic;">region proposal</span><span class="font10"> seperti pada </span><span class="font10" style="font-style:italic;">multi-stage detector</span><span class="font10">, melainkan hanya dengan membagi gambar menjadi beberapa grid dan menentukan sebuah objek terdapat pada grid yang mana. Konsep ini memberikan hasil implementasi </span><span class="font10" style="font-style:italic;">object detection </span><span class="font10">yang sangat cepat.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font10" style="font-style:italic;">B. &nbsp;&nbsp;&nbsp;Real-Time Inference Object Detection</span></p></li></ul>
<p><span class="font10">Object detection telah menawarkan akurasi tinggi dalam melakukan penentuan dan klasifikasi objek sehingga hal ini dapat dimanfaatkan dalam proses otomatisasi. Namun, sebagai suatu solusi dalam otomatisasi, model </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> harus mampu beradaptasi pada faktor </span><span class="font10" style="font-style:italic;">affordability</span><span class="font10"> dan </span><span class="font10" style="font-style:italic;">compatibility </span><span class="font10">[16]. </span><span class="font10" style="font-style:italic;">Affordability</span><span class="font10"> menjelaskan bahwa model </span><span class="font10" style="font-style:italic;">object detection </span><span class="font10">yang baik mampu dijangkau oleh masyarakat dalam hal biaya dan material, sehingga mampu dikomersialisasi secara lebih komprehensif. Sementara itu, </span><span class="font10" style="font-style:italic;">compatibility</span><span class="font10"> memiliki peran yang lebih penting, yaitu mampu menyesuaikan model agar dapat berjalan pada komputer yang sesuai dengan kebutuhan bentuk, yang cenderung kecil. Hal ini disebabkan oleh kecenderungan implementasi </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> digunakan dalam menentukan posisi dan jenis objek secara langsung (</span><span class="font10" style="font-style:italic;">real-time</span><span class="font10">), seperti aplikasi pada pendeteksi penggunaan masker medis [17]. Sehingga menjadi sebuah tantangan ketika implementasi ini diterapkan di tempat deteksi secara langsung yang membutuhkan penyesuaian pada bentuk yang relatif kecil.</span></p>
<p><span class="font10">Merujuk pada faktor </span><span class="font10" style="font-style:italic;">compatibility</span><span class="font10">, model </span><span class="font10" style="font-style:italic;">object detection </span><span class="font10">juga harus mampu memberikan otomatisasi yang berseuaian dengan yang dibutuhkan, terutama pada waktu </span><span class="font10" style="font-style:italic;">inference</span><span class="font10"> model. Waktu </span><span class="font10" style="font-style:italic;">inference</span><span class="font10"> ini akan berdampak pada kelancaran dari menginterpreatasikan hasil posisi dan klasifikasi objek. Waktu </span><span class="font10" style="font-style:italic;">inference</span><span class="font10"> pada model </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> besar dipengaruhi oleh jumlah parameter yang dimuat oleh model. Hal ini dikarenakan jumlah parameter menunjukkan jumlah komputasi yang dilakukan oleh model dalam interpretasi sebuah gambar. Terkait kajian antara </span><span class="font10" style="font-style:italic;">multi-stage detector</span><span class="font10"> dengan </span><span class="font10" style="font-style:italic;">single-stage detector</span><span class="font10"> pernah dilakukan dalam parameter inference time. Secara garis besar, </span><span class="font10" style="font-style:italic;">single-stage detector</span><span class="font10"> memberikan waktu </span><span class="font10" style="font-style:italic;">inference</span><span class="font10"> yang lebih cepat dibandingkan dengan multi-stage detector. Sehingga penerapan </span><span class="font10" style="font-style:italic;">single-stage detector</span><span class="font10"> lebih banyak dilakukan pada bidang deteksi objek yang cenderung dinamis/bergerak.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark10"></a><span class="font10"><a name="bookmark11"></a>III.</span><span class="font10" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Metode</span></h3></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font10" style="font-style:italic;">A. &nbsp;&nbsp;&nbsp;Kerangka Berpikir</span></p></li></ul>
<p><span class="font10">Pada prinsipnya, </span><span class="font10" style="font-style:italic;">object detection</span><span class="font10"> yang hendak diterapkan memberikan keputusan untuk melakukan </span><span class="font10" style="font-style:italic;">HGR</span><span class="font10"> yang sering</span></p>
<p><span class="font10" style="font-style:italic;">Majalah Ilmiah Teknologi Elektro, Vol.22, No.2, Juli-Desember DOI: </span><a href="https://doi.org/10.24843/MITE.2023.v22i02.P05"><span class="font10" style="font-style:italic;">https://doi.org/10.24843/MITE.2023.v22i02.P05</span></a><span class="font10" style="font-style:italic;"> </span><span class="font10">digunakan dalam kegiatan-kegiatan dasar. Dari sisi medis, terdapat lima gerakan dasar jari yang menjadi fokus pengembangan presisi gerakan prostesis lengan dan tangan, yakni: (1) gerakan mendekatkan jari-jari ke telapak tangan (fleksi), (2) gerakan meluruskan jari (ekstensi), (3) gerakan mendekatkan jari-jari ke jari tengah (adduksi), (4) gerakan menjauhkan jari-jari dari jari tengah (abduksi), dan (5) gerakan menjauhkan ibu jari dari jari lainnya (oposisi) Kelima gerakan dasar ini dikombinasikan menjadi tiga jenis </span><span class="font10" style="font-style:italic;">HGR</span><span class="font10"> sebagaimana ditunjukkan pada Gambar 2: , antara lain: 1) gerakan mencubit (</span><span class="font10" style="font-style:italic;">pinch)</span><span class="font10">, 2) gerakan memegang pensil (</span><span class="font10" style="font-style:italic;">pick)</span><span class="font10">, dan 3) gerakan mengambil dengan seluruh telapak tangan </span><span class="font10" style="font-style:italic;">(grab)</span><span class="font10"> [18]. Dalam mendeteksi objek nantinya, prostesis dibubuhkan dengan kamera dengan rangkaian divisualisasikan oleh Gambar 3: sehingga secara langsung dapat menentukan jenis </span><span class="font10" style="font-style:italic;">HGR </span><span class="font10">berdasarkan bentuk dari objek.</span></p>
<div><img src="https://jurnal.harianregional.com/media/98236-2.jpg" alt="" style="width:201pt;height:263pt;">
<p><span class="font8">Gambar 2: Mekanisme model </span><span class="font8" style="font-style:italic;">object detection</span><span class="font8"> berbasis </span><span class="font8" style="font-style:italic;">HGR</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/98236-3.jpg" alt="" style="width:170pt;height:111pt;">
<p><span class="font8">Gambar 3: Implementasi computer vision pada prostesis tangan</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font10" style="font-style:italic;">B. &nbsp;&nbsp;&nbsp;Pengumpulan dan Processing Dataset</span></p></li></ul>
<p><span class="font10">Pengumpulan </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10"> dilakukan dengan menggunakan metode </span><span class="font10" style="font-style:italic;">data scrapping</span><span class="font10"> berupa gambar (</span><span class="font10" style="font-style:italic;">image</span><span class="font10">) yang diperoleh</span></p>
<p><span class="font10" style="font-style:italic;">2023</span></p>
<div>
<p><span class="font10">dari data publik. Hal ini dilakukan untuk memberikan gambaran umum pada objek yang hendak ditargetkan pada saat implementasi sehingga menyerupai keadaan sebenarnya saat </span><span class="font10" style="font-style:italic;">deployment</span><span class="font10">. Secara lebih spesifik, </span><span class="font10" style="font-style:italic;">image data scrapping </span><span class="font10">dilakukan dengan menggunakan modul </span><span class="font10" style="font-style:italic;">Selenium</span><span class="font10"> pada </span><span class="font10" style="font-style:italic;">Python</span><span class="font10">, yang memberikan otomatisasi untuk mengunduh gambar publik pada </span><span class="font10" style="font-style:italic;">Google Image</span><span class="font10"> serta mengumpulkannya sebagai </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10"> [19]. </span><span class="font10" style="font-style:italic;">Dataset</span><span class="font10"> yang telah terkumpul selanjutnya dilakukan </span><span class="font10" style="font-style:italic;">labelling</span><span class="font10"> berupa </span><span class="font10" style="font-style:italic;">bounding box</span><span class="font10"> sebagai </span><span class="font10" style="font-style:italic;">object localization</span><span class="font10"> pada gambar serta identifikasi objek sebagai tiga jenis </span><span class="font10" style="font-style:italic;">HGR</span><span class="font10"> yang diharapkan pada lengan prostesis. Keseluruhan proses </span><span class="font10" style="font-style:italic;">labelling</span><span class="font10"> dilakukan dengan menggunakan </span><span class="font10" style="font-style:italic;">open-source framework LabelImg</span><span class="font10"> yang dikembangkan secara spesifik untuk membantu memberikan </span><span class="font10" style="font-style:italic;">annotation</span><span class="font10"> pada </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10">, berupa </span><span class="font10" style="font-style:italic;">labels file</span><span class="font10"> (Gambar 4:) [20]. </span><span class="font10" style="font-style:italic;">Dataset</span><span class="font10"> dan </span><span class="font10" style="font-style:italic;">labels file</span><span class="font10"> dilakukan proses </span><span class="font10" style="font-style:italic;">splitting</span><span class="font10"> sebagai usaha untuk mengevaluasi (</span><span class="font10" style="font-style:italic;">testing</span><span class="font10">) model yang diajukan nanti pada data yang belum digunakan </span><span class="font10" style="font-style:italic;">training </span><span class="font10">oleh model. Rasio antara data </span><span class="font10" style="font-style:italic;">training</span><span class="font10"> dengan </span><span class="font10" style="font-style:italic;">testing</span><span class="font10"> yang digunakan yaitu 80:20. Secara kuantitas, jumlah </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10"> yang digunakan adalah total 317 gambar dengan 2278 label objek untuk ketiga jenis label.</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/98236-4.jpg" alt="" style="width:232pt;height:267pt;">
<p><span class="font8">Gambar 4: Ilustrasi demo dari proses persiapan dataset pada </span><span class="font8" style="font-style:italic;">framework LabelImg</span><span class="font8"> [11]</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font10" style="font-style:italic;">C. &nbsp;&nbsp;&nbsp;Penyusunan Model Neural Network Object Detection</span></p></li></ul>
<p><span class="font10" style="font-style:italic;">Berbasis HGR</span></p>
<p><span class="font10">Dalam membangun model </span><span class="font10" style="font-style:italic;">neural network object detection</span><span class="font10">, </span><span class="font10" style="font-style:italic;">pretrained model YOLOV7</span><span class="font10"> digunakan sebagai basis dalam </span><span class="font10" style="font-style:italic;">transfer learning</span><span class="font10"> untuk </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10"> yang telah dikumpulkan. </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10"> menggunakan algoritma </span><span class="font10" style="font-style:italic;">convolutional</span><span class="font10"> pada arsitekturnya sehingga memudahkan dalam </span><span class="font10" style="font-style:italic;">feature extraction</span><span class="font10">. Arsitektur model </span><span class="font10" style="font-style:italic;">YOLO</span><span class="font10"> dapat dilihat pada Gambar 5:</span></p>
<p><span class="font10">p-ISSN:1693 – 2951; e-ISSN: </span><span class="font0">2503-2372</span></p>
<div><img src="https://jurnal.harianregional.com/media/98236-5.png" alt="" style="width:62pt;height:38pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/98236-6.jpg" alt="" style="width:251pt;height:135pt;">
<p><span class="font8">Gambar 5: Arsitektur model </span><span class="font8" style="font-style:italic;">YOLO</span><span class="font8"> [15]</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">precision</span><span class="font10"> dan </span><span class="font10" style="font-style:italic;">recall</span><span class="font10"> dihasilkan melalui </span><span class="font10" style="font-style:italic;">cross-validation actual </span><span class="font10">dan </span><span class="font10" style="font-style:italic;">predicted value</span><span class="font10"> yang ditunjukkan pada Tabel 1:</span></p>
</div><br clear="all">
<div>
<p><span class="font8">TABEL I</span></p>
<p><span class="font10" style="font-style:italic;font-variant:small-caps;">C</span><span class="font6" style="font-style:italic;font-variant:small-caps;">ross</span><span class="font10" style="font-style:italic;font-variant:small-caps;">-</span><span class="font6" style="font-style:italic;font-variant:small-caps;">validation</span><span class="font7"> NILAI SEBENARNYA </span><span class="font8">(</span><span class="font7" style="font-style:italic;">ACTUAL </span><span class="font6" style="font-style:italic;font-variant:small-caps;">value</span><span class="font10" style="font-variant:small-caps;">)</span><span class="font7"> DAN NILAI PREDIKSI </span><span class="font8">(</span><span class="font7" style="font-style:italic;">PREDICTED VALUE</span><span class="font8">)</span></p>
<table border="1">
<tr><td colspan="2" rowspan="2" style="vertical-align:top;"></td><td colspan="2" style="vertical-align:top;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">Actual Value</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">True</span></p></td><td style="vertical-align:top;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">False</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">Predicted</span></p>
<p><span class="font9" style="font-weight:bold;font-style:italic;">Value</span></p></td><td style="vertical-align:middle;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">True</span></p></td><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">True Positive</span></p>
<p><span class="font9">(TP)</span></p></td><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">False Positive</span></p>
<p><span class="font9">(FP)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">False</span></p></td><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">False Negative </span><span class="font9">(FN)</span></p></td><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">True Negative </span><span class="font9">(TN)</span></p></td></tr>
</table>
</div><br clear="all">
<p><span class="font10">Selain itu, </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10"> merupakan model yang termasuk </span><span class="font10" style="font-style:italic;">one-stage object detector</span><span class="font10"> yang mampu memberikan secara langsung nilai posisi objek dan klasifikasi objek secara langsung pada data gambar. Merujuk pada keunggulan ini, pengembangan model </span><span class="font10" style="font-style:italic;">neural network</span><span class="font10"> untuk prostesis lengan ini dilakukan dengan menggunakan basis </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10"> beserta turunannya [14]. Pemilihan dari </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10"> beserta turunannya turut dipertimbangkan mengingat proses dari training dataset yang mengkonsumsi waktu relatif besar serta menghemat dari proses komputasi yang dilakukan. Hal tersebut dikaitkan dengan beberapa aspek, antara lain: </span><span class="font10" style="font-style:italic;">average precision</span><span class="font10"> (</span><span class="font10" style="font-style:italic;">AP</span><span class="font10">) serta parameter yang dapat di-</span><span class="font10" style="font-style:italic;">training</span><span class="font10"> (</span><span class="font10" style="font-style:italic;">trainable parameters</span><span class="font10">) pada tiap jenis model turunan </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10">. Selain itu, terdapat pula aspek lainnya seperti waktu inferensi model yang secara langsung dipengaruhi oleh jumlah </span><span class="font10" style="font-style:italic;">trainable parameter</span><span class="font10">. Gambar 6: menunjukkan grafik hubungan kedua aspek tersebut komparasi dari model turunan YOLOV7 dan model lainnya.</span></p><img src="https://jurnal.harianregional.com/media/98236-7.jpg" alt="" style="width:234pt;height:198pt;">
<p><span class="font8">Gambar 6: Perbandingan </span><span class="font8" style="font-style:italic;">YOLOV7</span><span class="font8"> dengan model </span><span class="font8" style="font-style:italic;">object detection</span><span class="font8"> lain yang memperoleh </span><span class="font8" style="font-style:italic;">state-of-the-arts</span><span class="font8"> pada aspek performansi</span></p>
<ul style="list-style:none;"><li>
<p><span class="font10" style="font-style:italic;">D. &nbsp;&nbsp;&nbsp;Evaluasi Model Neural Network Object Detection</span></p></li></ul>
<p><span class="font10">Sebagai bahan evaluasi, perbandingan </span><span class="font10" style="font-style:italic;">evaluation metrics </span><span class="font10">pada beberapa turunan </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10"> turut dilakukan untuk mengobservasi model yang sesuai dengan </span><span class="font10" style="font-style:italic;">task</span><span class="font10"> yang diberikan. Beberapa </span><span class="font10" style="font-style:italic;">evaluation metrics</span><span class="font10"> yang dimaksud berupa </span><span class="font10" style="font-style:italic;">precision</span><span class="font10">, </span><span class="font10" style="font-style:italic;">recall</span><span class="font10">, dan </span><span class="font10" style="font-style:italic;">mean average precision (mAP)</span><span class="font10">. Secara konsep,</span></p>
<p><span class="font10" style="font-style:italic;">Precision</span><span class="font10"> dan </span><span class="font10" style="font-style:italic;">recall</span><span class="font10"> selanjutnya dirumuskan dalam (1) dan (2) secara berturut-turut. Sementara itu, </span><span class="font10" style="font-style:italic;">mAP</span><span class="font10"> merupakan hasil dari analisis rata-rata yang dihasilkan dari hubungan </span><span class="font10" style="font-style:italic;">precisionrecall</span><span class="font10"> akan digunakan sebagai akurasi model keseluruhan [22].</span></p>
<p><a href="#bookmark12"><span class="font10" style="font-style:italic;">Precision</span><span class="font10"> =.....(1)</span></a></p>
<p><a href="#bookmark13"><span class="font4" style="font-style:italic;">TP+FP</span></a></p>
<p><a href="#bookmark14"><span class="font10" style="font-style:italic;">Recall </span><span class="font5" style="font-style:italic;font-variant:small-caps;">=-<sup>t</sup>-</span><span class="font10">(2)</span></a></p>
<p><a href="#bookmark15"><span class="font4" style="font-style:italic;">TP+PN</span></a></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark16"></a><span class="font10"><a name="bookmark17"></a>IV.</span><span class="font10" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Hasil dan Pembahasan</span></h3></li></ul>
<p><span class="font10">Model </span><span class="font10" style="font-style:italic;">neural network</span><span class="font10"> dilakukan </span><span class="font10" style="font-style:italic;">training</span><span class="font10"> dengan konfigurasi </span><span class="font10" style="font-style:italic;">set-up</span><span class="font10"> komputer sebagaimana ditunjukkan Tabel 2:</span></p>
<p><span class="font8">TABEL III</span></p>
<p><span class="font10" style="font-variant:small-caps;">K</span><span class="font8" style="font-variant:small-caps;">onfigurasi </span><span class="font8" style="font-style:italic;">T</span><span class="font7" style="font-style:italic;">RAINING </span><span class="font8" style="font-style:italic;">M</span><span class="font7" style="font-style:italic;">ODEL </span><span class="font8" style="font-style:italic;">O</span><span class="font7" style="font-style:italic;">BJECT </span><span class="font8" style="font-style:italic;">D</span><span class="font7" style="font-style:italic;">ETECTION </span><span class="font10" style="font-variant:small-caps;">P</span><span class="font8" style="font-variant:small-caps;">rostesis </span><span class="font10" style="font-variant:small-caps;">T</span><span class="font8" style="font-variant:small-caps;">angan</span></p>
<p><span class="font10" style="font-variant:small-caps;">B</span><span class="font8" style="font-variant:small-caps;">erbasis </span><span class="font8" style="font-style:italic;">HGR</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font9" style="font-weight:bold;">Parameter</span></p></td><td style="vertical-align:middle;">
<p><span class="font9" style="font-weight:bold;">Configuration</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">CPU</span></p></td><td style="vertical-align:top;">
<p><span class="font9">10<sup>th</sup> Gen Intel® Core i5-10400F</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">GPU</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">NVIDIA GeForce GTX1650 Super (4G)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">Operating System</span></p></td><td style="vertical-align:top;">
<p><span class="font9">Windows10</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">CUDA</span></p></td><td style="vertical-align:top;">
<p><span class="font9">11.7</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">Python</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">3.8.9</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">Torch</span></p></td><td style="vertical-align:top;">
<p><span class="font9">1.13.1</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">Momentum</span></p></td><td style="vertical-align:top;">
<p><span class="font9">0.937</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">Weight decay</span></p></td><td style="vertical-align:top;">
<p><span class="font9">0.0005</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">Batch size</span></p></td><td style="vertical-align:top;">
<p><span class="font9">4</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">Learning rate</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">0.01</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">Image size</span></p></td><td style="vertical-align:top;">
<p><span class="font9">640 x 640</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-style:italic;">Epochs</span></p></td><td style="vertical-align:top;">
<p><span class="font9">300</span></p></td></tr>
</table>
<p><span class="font10" style="font-style:italic;">Training</span><span class="font10"> ini berlangsung selama 6 jam dengan masing-masing model </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10"> dan </span><span class="font10" style="font-style:italic;">YOLOV7tiny</span><span class="font10"> berdurasi 3 jam. Prosedur </span><span class="font10" style="font-style:italic;">training</span><span class="font10"> dengan </span><span class="font10" style="font-style:italic;">set up</span><span class="font10"> yang ditunjukkan pada Tabel 2: menghasilkan grafik </span><span class="font10" style="font-style:italic;">loss function</span><span class="font10"> tiap epoch-nya yang menunjukkan bahwa model </span><span class="font10" style="font-style:italic;">trainable</span><span class="font10"> pada </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10"> yang digunakan (Gambar 7:).</span></p>
<p><span class="font10" style="font-style:italic;">Majalah Ilmiah Teknologi Elektro, Vol.22, No.2, Juli-Desember 2023 DOI: </span><a href="https://doi.org/10.24843/MITE.2023.v22i02.P05"><span class="font10" style="font-style:italic;">https://doi.org/10.24843/MITE.2023.v22i02.P05</span></a></p><img src="https://jurnal.harianregional.com/media/98236-8.jpg" alt="" style="width:251pt;height:119pt;"><img src="https://jurnal.harianregional.com/media/98236-9.jpg" alt="" style="width:254pt;height:114pt;">
<p><span class="font8">(b)</span></p>
<p><span class="font8">Gambar 7: Grafik nilai </span><span class="font8" style="font-style:italic;">loss</span><span class="font8"> model tiap </span><span class="font8" style="font-style:italic;">epoch training</span><span class="font8"> pada (a) </span><span class="font8" style="font-style:italic;">YOLOV7</span><span class="font8"> dan (b) </span><span class="font8" style="font-style:italic;">YOLOV7tiny</span></p>
<p><span class="font10">Untuk memperoleh evaluasi yang komprehensif dan objektif pada performa dari model yang dibuat, prosedur dilanjutkan dengan menggunakan </span><span class="font10" style="font-style:italic;">confussion matrix</span><span class="font10">. Sebagai luaran evaluasi, </span><span class="font10" style="font-style:italic;">confusion matrix model</span><span class="font10"> hasil training pada </span><span class="font10" style="font-style:italic;">testing dataset</span><span class="font10"> ditunjukkan pada Gambar 8:</span></p><img src="https://jurnal.harianregional.com/media/98236-10.jpg" alt="" style="width:260pt;height:217pt;">
<p><span class="font8">(a)</span></p><img src="https://jurnal.harianregional.com/media/98236-11.jpg" alt="" style="width:248pt;height:215pt;">
<p><span class="font8">(b)</span></p>
<p><span class="font8">Gambar 8: </span><span class="font8" style="font-style:italic;">Confusion matrix</span><span class="font8"> hasil </span><span class="font8" style="font-style:italic;">training model</span><span class="font8"> (a) </span><span class="font8" style="font-style:italic;">YOLOV7</span><span class="font8"> dan (b) </span><span class="font8" style="font-style:italic;">YOLOV7tiny</span><span class="font8"> pada </span><span class="font8" style="font-style:italic;">dataset HGR</span></p>
<p><span class="font10">Melalui </span><span class="font10" style="font-style:italic;">framework Pytorch</span><span class="font10">, hasil </span><span class="font10" style="font-style:italic;">confusion matrix</span><span class="font10"> tersebut dapat dianalisis </span><span class="font10" style="font-style:italic;">evaluation metrics</span><span class="font10">-nya yang secara detail ditunjukkan pada Tabel 3:</span></p>
<p><span class="font8">TABEL IIIII</span></p>
<p><span class="font10" style="font-variant:small-caps;">N</span><span class="font8" style="font-variant:small-caps;">ilai </span><span class="font8" style="font-style:italic;">E</span><span class="font7" style="font-style:italic;">VALUATION </span><span class="font8" style="font-style:italic;">M</span><span class="font7" style="font-style:italic;">ETRICS </span><span class="font10" style="font-variant:small-caps;">P</span><span class="font8" style="font-variant:small-caps;">ada </span><span class="font10" style="font-variant:small-caps;">T</span><span class="font8" style="font-variant:small-caps;">esting </span><span class="font8" style="font-style:italic;">D</span><span class="font7" style="font-style:italic;">ATASET</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font9" style="font-weight:bold;">Model </span><span class="font9" style="font-weight:bold;font-style:italic;">Pretrained</span></p></td><td style="vertical-align:middle;">
<p><span class="font9" style="font-weight:bold;">Jumlah Parameter (juta)</span></p></td><td style="vertical-align:top;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">Frame per second (FPS)</span></p></td><td style="vertical-align:top;">
<p><span class="font9" style="font-weight:bold;font-style:italic;">mAP (%)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">YOLOV7</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">36,9</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">161</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">98,11</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font9" style="font-style:italic;">YOLOV7-tiny</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">6,2</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">286</span></p></td><td style="vertical-align:middle;">
<p><span class="font9">72,43</span></p></td></tr>
</table>
<p><span class="font10">Tabel 3: menunjukkan bahwa model </span><span class="font10" style="font-style:italic;">YOLOV7</span><span class="font10"> menghasilkan nilai yang lebih baik pada </span><span class="font10" style="font-style:italic;">mAP</span><span class="font10"> relatif terhadap </span><span class="font10" style="font-style:italic;">YOLOV7tiny</span><span class="font10">. Hal ini disebabkan karena model </span><span class="font10" style="font-style:italic;">pretrained YOLOV7</span><span class="font10"> memiliki lebih banyak </span><span class="font10" style="font-style:italic;">trainable parameter</span><span class="font10">, sejumlah 36,9 juta parameter, sementara </span><span class="font10" style="font-style:italic;">YOLOV7tiny</span><span class="font10"> hanya memiliki 6,2 juta </span><span class="font10" style="font-style:italic;">trainable parameter</span><span class="font10"> [14]. Tentu dengan adanya perbedaan jumlah parameter ini membuat </span><span class="font10" style="font-style:italic;">YOLOV7tiny</span><span class="font10"> membuat kesalahan interpretasi yang lebih banyak. Namun, pengembangan </span><span class="font10" style="font-style:italic;">YOLOV7tiny</span><span class="font10"> tetap harus dipertimbangkan apabila </span><span class="font10" style="font-style:italic;">deployment model</span><span class="font10"> dilakukan karena model yang lebih sedikit parameternya akan meningkatkan kecepatan proses gambar, dengan kata lain </span><span class="font10" style="font-style:italic;">FPS (frame per second)</span><span class="font10"> dari proses gambar semakin meningkat. Hal ini tentu berdampak baik untuk kondisi </span><span class="font10" style="font-style:italic;">real-time</span><span class="font10"> [21]. Selain itu, berdasarkan kelayakan implementasi, nilai dari </span><span class="font10" style="font-style:italic;">mAP YOLOV7</span><span class="font10"> dapat dikatakan telah memenuhi kategori sangat baik (</span><span class="font10" style="font-style:italic;">excellent</span><span class="font10">) karena memiliki nilai mAP di atas 90%. Sementara itu, </span><span class="font10" style="font-style:italic;">YOLOV7tiny</span><span class="font10">, dengan</span></p>
<div><img src="https://jurnal.harianregional.com/media/98236-12.png" alt="" style="width:179pt;height:65pt;">
</div><br clear="all">
<p><span class="font10">ambang batas 80%, belum memenuhi sebagai model yang optimal untuk diimplementasikan [22]. Sehingga, </span><span class="font10" style="font-style:italic;">YOLOV7 </span><span class="font10">akan lebih baik dan layak saat implementasi model </span><span class="font10" style="font-style:italic;">object detection neural network</span><span class="font10"> pada prostesis tangan.</span></p>
<p><span class="font10">Peninjauan terhadap masing-masing jenis label juga dapat diamati melalui Tabel 3:. Label yang selalu memiliki nilai </span><span class="font10" style="font-style:italic;">metrics</span><span class="font10"> yang paling rendah adalah label </span><span class="font10" style="font-style:italic;">pinch</span><span class="font10">. Hal ini dikarenakan model ini masih memiliki jumlah label yang sedikit dan secara pengamatan kasat mata memiliki bentuk yang hampir sejenis dengan objek pada label </span><span class="font10" style="font-style:italic;">pinch</span><span class="font10">, berbentuk panjang dengan lebar relatif kecil (Gambar 9:). Peningkatan akurasi model berpotensi dapat dikembangkan dengan metode </span><span class="font10" style="font-style:italic;">segmentation</span><span class="font10"> yang memberikan analisis lebih mendetail terhadap bentuk objek [23].</span></p>
<p><span class="font10">Terkait aspek dataset, pengembangan model juga dapat diperkuat akurasinya dengan penambahan pada variasi dataset yang mampu merepresentasikan objek sesuai dengan konsep </span><span class="font10" style="font-style:italic;">HGR</span><span class="font10">. Pada dataset yang digunakan saat ini dapat dikatakan masih realtif kurang dalam distribusi jenis objek karena pada saat pengumpulan dataset proses </span><span class="font10" style="font-style:italic;">web scrapping</span><span class="font10">, objek yang diperoleh terbatas pada keyword yang diberikan.</span></p>
<p><span class="font10">]</span></p><img src="https://jurnal.harianregional.com/media/98236-13.jpg" alt="" style="width:251pt;height:161pt;"><img src="https://jurnal.harianregional.com/media/98236-14.jpg" alt="" style="width:251pt;height:190pt;">
<p><span class="font8">Gambar 9. Hasil prediksi model </span><span class="font8" style="font-style:italic;">object detection</span><span class="font8"> pada </span><span class="font8" style="font-style:italic;">batch testing dataset</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark18"></a><span class="font10"><a name="bookmark19"></a>V.</span><span class="font10" style="font-variant:small-caps;"> &nbsp;&nbsp;&nbsp;Kesimpulan</span></h3></li></ul>
<p><span class="font10">Dengan hasil yang dicapai, pengembangan model </span><span class="font10" style="font-style:italic;">object detection neural network</span><span class="font10"> yang telah dilakukan menunjukkan bahwa pendekatan ini sangat memungkinkan untuk</span></p>
<p><span class="font10">diimplementasikan, secara khususnya dengan model </span><span class="font10" style="font-style:italic;">YOLOV7 </span><span class="font10">dan turunannya. Hal ini secara langsung ditunjukkan oleh nilai </span><span class="font10" style="font-style:italic;">evaluation metrics</span><span class="font10"> yang dihasilkan pada </span><span class="font10" style="font-style:italic;">dataset</span><span class="font10"> objek yang selanjutnya memiliki relasi dengan </span><span class="font10" style="font-style:italic;">HGR</span><span class="font10"> yang bersesuaian. Sebagai tahapan pengembangan lanjutan, pengumpulan </span><span class="font10" style="font-style:italic;">dataset </span><span class="font10">yang lebih banyak perlu dilakukan agar mampu merepresentasikan keadaan sebagaimana saat di-</span><span class="font10" style="font-style:italic;">deploy</span><span class="font10"> serta implementasi metode </span><span class="font10" style="font-style:italic;">segmentation</span><span class="font10"> untuk mencapai ketelitian deteksi dan klasifikasi objek.</span></p>
<h3><a name="bookmark20"></a><span class="font10" style="font-variant:small-caps;"><a name="bookmark21"></a>Ucapan Terima Kasih</span></h3>
<p><span class="font10">Ucapan terima kasih diberikan kepada Universitas Udayana melalui Lembaga Penelitian dan Pengabdian kepada Masyarakat (LPPM) atas pemberian hibah pada kegiatan Merdeka Belajar Kampus Merdeka (MBKM) Riset dengan Nomor Keputusan 921/UN14/HK/2022 serta Mitra MBKM Riset Tim Prostesis Tangan Puspadi Bali atas bantuan dan arahan terhadap pengembangan prostesis tangan dari perspektif praktisi. Sehingga, penelitian ini dapat berjalan dengan semestinya.</span></p>
<h3><a name="bookmark22"></a><span class="font10" style="font-variant:small-caps;"><a name="bookmark23"></a>Referensi</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font8">[1] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I. A. Satam, “Review Studying of the Latest Development of</span></p></li></ul>
<p><span class="font8">Prosthetic Limbs Technologies,” </span><span class="font8" style="font-style:italic;">Int. J. Sci. Eng. Res.</span><span class="font8">, vol. 12, no. 12, pp. 721–731, 2022.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[2] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P. S. Mckechnie and A. John, “Anxiety and depression following</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font8">traumatic limb amputation : A systematic review,” </span><span class="font8" style="font-style:italic;">Injury</span><span class="font8">, vol. 45, no. 12, pp. 1859–1866, 2014, doi: 10.1016/j.injury.2014.09.015.</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font8">[3] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kemenkes RI, “Hasil Riset Kesehatan Dasar Tahun 2018,”</span></p></li></ul>
<p><span class="font8" style="font-style:italic;">Kementrian Kesehat. RI</span><span class="font8">, vol. 53, no. 9, pp. 1689–1699, 2018.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[4] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D. Dosen, S., Prahm, C., Amsüss, S., Vujaklija, I., &amp;&nbsp;Farina,</span></p></li></ul>
<p><span class="font8">“Prosthetic Feedback Systems,” in </span><span class="font8" style="font-style:italic;">Bionic Limb Reconstruction</span><span class="font8">, Springer Nature Switzerland AG, 2021, pp. 147–170.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[5] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Z. Ren, J. Yuan, J. Meng, and Z. Zhang, “Robust Part-Based Hand</span></p></li></ul>
<p><span class="font8">Gesture Recognition Using Kinect Sensor,” </span><span class="font8" style="font-style:italic;">IEEE Trans. Multimed</span><span class="font8">, vol. 15, no. 5, pp. 1–11, 2013.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[6] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B. Feng, F. He, X. Wang, Y. Wu, and H. Wang, “Depth-Projection</span></p></li></ul>
<p><span class="font8">Map-Based Bag of Contour Fragments for Robust Hand Gesture Recognition,” </span><span class="font8" style="font-style:italic;">IEEE Trans. Hum.-Mach. Syst</span><span class="font8">, vol. 47, pp. 1–13, 2016.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[7] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P. K. Pisharady and M. Saerbeck, “Recent methods and databases in</span></p></li></ul>
<p><span class="font8">vision-based hand gesture recognition: A review,” </span><span class="font8" style="font-style:italic;">Comput. Vis. Image Underst.</span><span class="font8">, vol. 141, pp. 152–165, &nbsp;&nbsp;2015, doi:</span></p>
<p><span class="font8">10.1016/j.cviu.2015.08.004.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[8] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M. Garcia, J. Cruz, C. Garza, P. DeLucia, and J. Yang, “The Effect</span></p></li></ul>
<p><span class="font8">of Object Surfaces and Shapes on Hand Grip Function for Heavy Objects,” in </span><span class="font8" style="font-style:italic;">Proceedings of the AHFE 2018 International Conferences on Human Factors and Simulation and Digital Human Modeling and Applied Optimization</span><span class="font8">, 2018, pp. 446–452.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[9] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I. P. G. S. Andisana, M. Sudarma, and I. M. O. Widyantara,</span></p></li></ul>
<p><span class="font8">“Pengenalan Dan Klasifikasi Citra Tekstil Tradisional Berbasis Web Menggunakan Deteksi Tepi Canny, Local Color Histogram Dan CoOccurrence Matrix,” </span><span class="font8" style="font-style:italic;">Maj. Ilm. Teknol. Elektro</span><span class="font8">, vol. 17, no. 3, p. 401, 2018, doi: 10.24843/mite.2018.v17i03.p15.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[10] &nbsp;&nbsp;&nbsp;&nbsp;X. Zhang, M. Shen, X. Li, and F. Feng, “A deformable CNN-based</span></p></li></ul>
<p><span class="font8">triplet model for fine-grained sketch-based image retrieval,” </span><span class="font8" style="font-style:italic;">Pattern Recognit.</span><span class="font8">, &nbsp;&nbsp;&nbsp;&nbsp;vol. &nbsp;&nbsp;&nbsp;&nbsp;125, &nbsp;&nbsp;&nbsp;&nbsp;p. &nbsp;&nbsp;&nbsp;&nbsp;108508, &nbsp;&nbsp;&nbsp;&nbsp;2022, &nbsp;&nbsp;&nbsp;&nbsp;doi:</span></p>
<p><span class="font8">10.1016/j.patcog.2021.108508.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[11] &nbsp;&nbsp;&nbsp;&nbsp;N. Dong, Y. Zhang, M. Ding, and G. H. Lee, “Incremental-DETR:</span></p></li></ul>
<p><span class="font8">Incremental Few-Shot Object Detection via Self-Supervised Learning,” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2022, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Online]. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Available:</span></p>
<p><a href="http://arxiv.org/abs/2205.04042"><span class="font8">http://arxiv.org/abs/2205.04042</span></a><span class="font8">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[12] &nbsp;&nbsp;&nbsp;&nbsp;M. Z. Alom </span><span class="font8" style="font-style:italic;">et al.</span><span class="font8">, “The History Began from AlexNet: A</span></p></li></ul>
<p><span class="font8">Comprehensive Survey on Deep Learning Approaches,” 2018, [Online]. Available: </span><a href="http://arxiv.org/abs/1803.01164"><span class="font8">http://arxiv.org/abs/1803.01164</span></a><span class="font8">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[13] &nbsp;&nbsp;&nbsp;P. Bharati and A. Pramanik, “Deep Learning Techniques—R-CNN</span></p></li></ul>
<p><span class="font8">to Mask R-CNN: A Survey,” in </span><span class="font8" style="font-style:italic;">Advances in Intelligent Systems and Computing</span><span class="font8">, 2020, vol. 999, p. 7, doi: 10.1142/S0218001402001976.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[14] &nbsp;&nbsp;&nbsp;&nbsp;C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, “YOLOv7:</span></p></li></ul>
<p><span class="font8">Trainable bag-of-freebies sets new state-of-the-art for real-time object &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;detectors,” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2022. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Online]. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Available:</span></p>
<p><span class="font10" style="font-style:italic;">Majalah Ilmiah Teknologi Elektro, Vol.22, No.2, Juli-Desember 2023 DOI: </span><a href="https://doi.org/10.24843/MITE.2023.v22i02.P05http://arxiv.org/abs/2207.02696"><span class="font10" style="font-style:italic;">https://doi.org/10.24843/MITE.2023.v22i02.P05</span><span class="font8">http://arxiv.org/abs/2207.02696</span></a><span class="font8">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[15] &nbsp;&nbsp;&nbsp;J. Solawetz, “YOLOv7 - A Breakdown of How it Works,” </span><span class="font8" style="font-style:italic;">Roboflow</span><span class="font8">,</span></p></li></ul>
<p><span class="font8">2022. </span><a href="https://blog.roboflow.com/yolov7-breakdown/"><span class="font8">https://blog.roboflow.com/yolov7-breakdown/</span></a><span class="font8"> (accessed Jan. 10, 2023).</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[16] &nbsp;&nbsp;&nbsp;&nbsp;S. Mittal, “A Survey on optimized implementation of deep learning</span></p></li></ul>
<p><span class="font8">models on the NVIDIA Jetson platform,” </span><span class="font8" style="font-style:italic;">J. Syst. Archit.</span><span class="font8">, vol. 97, no. December &nbsp;&nbsp;&nbsp;&nbsp;2018, &nbsp;&nbsp;&nbsp;&nbsp;pp. &nbsp;&nbsp;&nbsp;&nbsp;428–442, &nbsp;&nbsp;&nbsp;&nbsp;2019, &nbsp;&nbsp;&nbsp;&nbsp;doi:</span></p>
<p><span class="font8">10.1016/j.sysarc.2019.01.011.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[17] &nbsp;&nbsp;&nbsp;&nbsp;C. G. I. Raditya, P. A. S. Dharma, K. A. Widyatmika, I. N. Suparta,</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font8">I. &nbsp;&nbsp;&nbsp;M. S. Yasa, and A. A. N. G. Sapteka, “Pendeteksi Penggunaan Masker Wajah dengan ESP32Cam Menggunakan OpenCV dan Tensorflow,” </span><span class="font8" style="font-style:italic;">Maj. Ilm. Teknol. Elektro</span><span class="font8">, vol. 21, no. 2, p. 155, 2022, doi: 10.24843/mite.2022.v21i02.p01.</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font8">[18] &nbsp;&nbsp;&nbsp;&nbsp;A. Schafer, G. Reis, and D. Stricker, “Comparing Controller With the</span></p></li></ul>
<p><span class="font8">Hand Gestures Pinch and Grab for Picking Up and Placing Virtual Objects,” in </span><span class="font8" style="font-style:italic;">IEEE VR</span><span class="font8">, 2022, pp. 1–2, doi: 10.3390/mti4040091.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[19] &nbsp;&nbsp;&nbsp;&nbsp;K. U. Manjari, S. Rousha, D. Sumanth, and J. Sirisha Devi,</span></p></li></ul>
<p><span class="font8">“Extractive Text Summarization from Web pages using Selenium and TF-IDF algorithm,” </span><span class="font8" style="font-style:italic;">Proc. 4th Int. Conf. Trends Electron. Informatics, ICOEI 2020</span><span class="font8">, pp. 648–652, &nbsp;&nbsp;2020, doi:</span></p>
<p><span class="font8">10.1109/ICOEI48184.2020.9142938.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[20] &nbsp;&nbsp;&nbsp;&nbsp;Tzutalin, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“LabelImg,” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font8" style="font-style:italic;">Git &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;code</span><span class="font8">, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015.</span></p></li></ul>
<p><a href="https://github.com/heartexlabs/labelImg"><span class="font8">https://github.com/heartexlabs/labelImg</span></a><span class="font8"> (accessed Jan. 10, 2023).</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[21] &nbsp;&nbsp;&nbsp;&nbsp;J. Zheng, H. Wu, H. Zhang, Z. Wang, and W. Xu, “Insulator-Defect</span></p></li></ul>
<p><span class="font8">Detection Algorithm Based on Improved YOLOv7,” </span><span class="font8" style="font-style:italic;">Sensors</span><span class="font8">, vol. 22, no. 22, pp. 1–23, 2022, doi: 10.3390/s22228801.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[22] &nbsp;&nbsp;&nbsp;&nbsp;K. M. Kuo, P. C. Talley, C. H. Huang, and L. C. Cheng, “Predicting</span></p></li></ul>
<p><span class="font8">hospital-acquired pneumonia among schizophrenic patients: A machine learning approach,” </span><span class="font8" style="font-style:italic;">BMC Med. Inform. Decis. Mak.</span><span class="font8">, vol. 19, no. 1, pp. 1–8, 2019, doi: 10.1186/s12911-019-0792-1.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[23] &nbsp;&nbsp;&nbsp;&nbsp;S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,</span></p></li></ul>
<p><span class="font8">and J. Gao, “Deep Learning Based Text Classification: A Comprehensive Review,” </span><span class="font8" style="font-style:italic;">ACM Comput. Surv</span><span class="font8">, vol. 54, no. 3, p. Article &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;62, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2021, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Online]. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Available:</span></p>
<p><a href="http://arxiv.org/abs/2004.03705"><span class="font8">http://arxiv.org/abs/2004.03705</span></a><span class="font8">.</span></p><img src="https://jurnal.harianregional.com/media/98236-15.png" alt="" style="width:179pt;height:65pt;">
<p><span class="font10">{Halaman ini sengaja dikosongkan}</span></p>
<p><span class="font10">ISSN 1693 – 2951</span></p>
<p><span class="font11">I.M.P. Arya Winata</span><span class="font10">: Model </span><span class="font10" style="font-style:italic;">Object Detection Neural…</span></p>