---
layout: full_article
title: "The Effect of Feature Selection on Music Genre Classification"
author: "I Nyoman Yusha Tresnatama Giri, Luh Arida Ayu Rahning Putri"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-64437 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-64437"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-64437"  
comments: true
---

<p><span class="font3">p-ISSN: 2301-5373</span></p>
<p><span class="font3">e-ISSN: 2654-5101</span></p>
<p><span class="font3">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font3">Volume 9 No. 4, May 2021</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font4" style="font-weight:bold;"><a name="bookmark1"></a>The Effect of Feature Selection on Music Genre Classification</span></h1>
<p><span class="font3">I Nyoman Yusha Tresnatama Giri<sup>a1</sup>, Luh Arida Ayu Rahning Putri<sup>a2</sup></span></p>
<p><span class="font3"><sup>a</sup>Informatics Department, Udayana University South Kuta, Badung, Bali, Indonesia </span><a href="mailto:1yusatresnatama11@gmail.com"><span class="font3"><sup>1</sup>yusatresnatama11@gmail.com</span></a><span class="font3"> </span><a href="mailto:2rahningputri@unud.ac.id"><span class="font3"><sup>2</sup>rahningputri@unud.ac.id</span></a></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font3" style="font-style:italic;">One of the things that affects classification results is the correlation of features to the class of a data. This research was conducted to determine the effect of the reduction of features (independent variable) that have the weakest correlation or have a distant relationship with the class (dependent variable). Bivariate Pearson Correlation is used as a feature selection method and K-Nearest Neighbor is used as a classification method. Results of the test showing that, 75.1% average accuracy was obtained for classification without feature selection, while using feature selection, average accuracy was obtained in the range of 75% - 79.3%. The average accuracy obtained by the selection of features tends to be higher compared to the accuracy obtained without selection of features.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font3" style="font-style:italic;">Feature Selection, Classification, Accuracy, Bivariate Pearson correlation, K-Nearest Neighbor</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark2"></a><span class="font3" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h3></li></ul>
<p><span class="font3">Genre in music is a classification performed by a person based on how similar the rhythm, harmony, and other various contents from the music is. Manual classification like this takes a lot of time because we need to listen to the music one by one, so a more effective classification is needed.</span></p>
<p><span class="font3">Before it can be classified, music needs to be extracted first in order to obtain feature data from the music itself. From the extraction results, a classification is carried out using classification method with the aim of obtaining the results of the music’s genre. K-Nearest Neighbor is a method that is easy to learn and in terms of learning, this method has the nature of supervised learning, which means this method uses training data as information to calculate and classify the test data [1]. There are various factors that can affect the results of classification, one of which is the features itself. The relationship between features will certainly have an impact on the classification results [2]. Bivariate Pearson correlation is a feature selection method which used to determine strength of the relationship between the independent variable (features) and the dependent variable (genres). Feature that have a weak correlation will be eliminated so that new dataset can be obtained with the eliminated features which will gives a different accuracy if re-classified.</span></p>
<p><span class="font3">In a previous studies related to the music genre classification has been conducted by [3] using 5 feature extraction methods (MFCC, chroma frequencies, spectral centroid, spectral roll-off, and zero crossing rate) to classify songs to 9 different genres. In this study, there are 3 classification methods used with different accuracy results namely K-Nearest Neighbor reaching 64%, Linear Support Vector Machine reaching 60%, and Poly Support Vector Machine reaching 78%.</span></p>
<p><span class="font3">In other studies conducted by [2], classification of liver disorders diagnostic results was conducted by using neural network backpropagation with MATLAB as classification method and bivariate Pearson correlation with IBM SPSS Statistic 25 tools as feature selection method. In testing data analysis conducted by this study, data that does not perform feature selection produces an accuracy value that tends to be greater than data that performs feature selection ranging between</span></p>
<p><span class="font3">64% to 100%. However, the accuracy value obtained in data that performs feature selection is more stable which is ranging between 68.57% to 71.42%.</span></p>
<p><span class="font3">Based on both studies above, authors want to know the accuracy changes that occur both before and after feature selection of music genre classification. The features that will be used in this study are produced by 5 feature extraction methods which results a total of 28 vector features. Bivariate Pearson correlation will be used as selection feature method that aims to find out which features or vector features will be selected based on the correlation relationship between features and classes (genres). The classification method in this study is K-Nearest Neighbor.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font3" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h3></li></ul>
<p><span class="font3">In this research, the audio data that will be used is GTZAN dataset which has 9 genres with 100 songs for each genre. The process of how the system will works can be seen in Figure 1.</span></p>
<div><img src="https://jurnal.harianregional.com/media/64437-1.jpg" alt="" style="width:244pt;height:181pt;">
<p><span class="font3" style="font-weight:bold;">Figure 1. </span><span class="font3">Flowchart of music genre classification with feature selection</span></p>
</div><br clear="all">
<p><span class="font3">The process starts from feature extraction to get the features of all songs from GTZAN dataset. The features obtained will be saved into a dataset. The feature selection was carried out using the Bivariate Pearson correlation method which then will be classified using K-Nearest Neighbor to produce an output in the form of accuracy.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark6"></a><span class="font3" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Feature Extraction</span></h3></li></ul>
<p><span class="font3">The goal of this step is to get a new dataset in the form of numeric values of each feature extracted from each song. There are 5 extraction features that will be used namely MFCC, chroma frequencies, spectral centroid, spectral roll-off, and zero crossing rate. Several libraries are involved for feature extraction namely scipy, librosa, and pandas.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark8"></a><span class="font3" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Feature Selection</span></h3></li></ul>
<p><span class="font3">Feature selection aims to find the relevance value of a feature of the class label and ignore feature that do not contribute anything to data classification' result. Bivariate Pearson correlation is used to analyze the significance value of each feature on the dataset. If the significance value of a feature is close to 0, it means that feature has a strong relationship to the class (genre) which might influence the result of classification. If the feature significance value is close to 1, it means the feature has a weak relationship to the class (genre) [2]. The sample correlation coefficient between two variables (feature (</span><span class="font3" style="font-style:italic;">x</span><span class="font3">) and genre (</span><span class="font3" style="font-style:italic;">y</span><span class="font3">)) is denoted </span><span class="font3" style="font-style:italic;">r</span><span class="font3"> or </span><span class="font3" style="font-style:italic;">r</span><span class="font1" style="font-style:italic;">xy</span><span class="font3">, and can be computed as equation (1).</span></p>
<h2><a name="bookmark10"></a><span class="font7" style="font-style:italic;"><a name="bookmark11"></a>cov(x,y)</span></h2>
<div>
<p><span class="font3">(1)</span></p>
</div><br clear="all">
<p><span class="font5" style="font-style:italic;"><sup>r</sup>xy</span><span class="font3"> = &nbsp;</span><span class="font5" style="font-style:italic;">i </span><span class="font3">, τ ∕ &nbsp;&nbsp;&nbsp;</span><span class="font3" style="text-decoration:line-through;">„ F</span></p>
<h2><a name="bookmark12"></a><span class="font7" style="font-style:italic;"><a name="bookmark13"></a>√var(x). √var(y)</span></h2>
<p><span class="font3">Where </span><span class="font3" style="font-style:italic;">cov(x, y)</span><span class="font3"> is the sample covariance of </span><span class="font3" style="font-style:italic;">x</span><span class="font3"> and </span><span class="font3" style="font-style:italic;">y</span><span class="font3">, </span><span class="font3" style="font-style:italic;">var(x)</span><span class="font3"> is the sample variance of </span><span class="font3" style="font-style:italic;">x</span><span class="font3">, and </span><span class="font3" style="font-style:italic;">var(y) </span><span class="font3">is the sample variance of </span><span class="font3" style="font-style:italic;">y</span><span class="font3"> [4]. IBM SPSS Statistic 25 is used to help calculate Bivariate Pearson correlation method above.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark14"></a><span class="font3" style="font-weight:bold;"><a name="bookmark15"></a>2.3. &nbsp;&nbsp;&nbsp;Classification</span></h3></li></ul>
<p><span class="font3">Classification is done after we have a dataset either before or after performing the feature selection. The classification steps by using the K-Nearest Neighbor can be done by the following steps:</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark16"></a><span class="font3"><a name="bookmark17"></a>a.</span><span class="font3" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Dataset input</span></h3></li></ul>
<p><span class="font3">Dataset that has been obtained through feature extraction will be inputted for early classification’s process.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark18"></a><span class="font3"><a name="bookmark19"></a>b.</span><span class="font3" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Pre-processing</span></h3></li></ul>
<p><span class="font3">In pre-processing, the data will be normalized with min-max normalization to get a balance of values between each feature from dataset [5]. The calculation of min-max normalization can be done with equations (2).</span></p>
<p><span class="font6" style="font-style:italic;">minRange + (x — minValue)(maxRange - minRange) norm(x) =-------------——---- . <sub>τ</sub>. ,--------------</span></p>
<div>
<p><span class="font3">(2)</span></p>
</div><br clear="all">
<p><span class="font6" style="font-style:italic;">maxValue — minValue</span></p>
<p><span class="font3">Where </span><span class="font3" style="font-style:italic;">norm(x)</span><span class="font3"> is the function to normalize data </span><span class="font3" style="font-style:italic;">x</span><span class="font3">, </span><span class="font3" style="font-style:italic;">minRange</span><span class="font3"> is the minimum limit we provide while </span><span class="font3" style="font-style:italic;">maxRange</span><span class="font3"> is the maximum limit. </span><span class="font3" style="font-style:italic;">minValue</span><span class="font3"> and </span><span class="font3" style="font-style:italic;">maxValue</span><span class="font3"> are both smallest and largest values of all data before normalized. </span><span class="font3" style="font-style:italic;">x</span><span class="font3"> is the data before normalized.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark20"></a><span class="font3"><a name="bookmark21"></a>c.</span><span class="font3" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Euclidean distance calculation</span></h3></li></ul>
<p><span class="font3">There are many distance calculations available in the K-Nearest Neighbor method, one of which is Euclidean. The purpose of the calculation is to define the distance between the two points, which is the point in the training data (</span><span class="font3" style="font-style:italic;">x</span><span class="font3">), and the point in testing data (</span><span class="font3" style="font-style:italic;">y</span><span class="font3">). Calculation of euclidean distance can be done with equations (3).</span></p>
<div>
<p><span class="font6" style="font-style:italic;">d(xi,yi) =</span></p><img src="https://jurnal.harianregional.com/media/64437-2.png" alt="" style="width:63pt;height:45pt;">
</div><br clear="all">
<p><span class="font3">(3)</span></p>
<p><span class="font3">Where </span><span class="font3" style="font-style:italic;">d</span><span class="font3"> is the distance between the point on the training data </span><span class="font3" style="font-style:italic;">x</span><span class="font3"> and the testing data </span><span class="font3" style="font-style:italic;">y</span><span class="font3"> to be classified. </span><span class="font3" style="font-style:italic;">x</span><span class="font3">, </span><span class="font3" style="font-style:italic;">y</span><span class="font3">, and </span><span class="font3" style="font-style:italic;">i</span><span class="font3"> represent attribute and </span><span class="font3" style="font-style:italic;">n</span><span class="font3"> is the dimension of the attribute.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark22"></a><span class="font3"><a name="bookmark23"></a>d.</span><span class="font3" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Nearest distance sorting</span></h3></li></ul>
<p><span class="font3">Sorting process is done after all distances have obtained. In the K-Nearest Neighbor method, sorting is performed based on the smallest (closest) distance value.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark24"></a><span class="font3"><a name="bookmark25"></a>e.</span><span class="font3" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Class determine</span></h3></li></ul>
<p><span class="font3">The process of determining a class, in this case a genre on music. The output of genre results is obtained from the test data through this step.</span></p>
<p><span class="font3">In the classification step of this study will be done with the help of scikit-learn library which has several functions namely K-NN Classifier function, metrics for accuracy calculation, and most importantly is data training and data testing will use K-fold cross validation function from this library.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></p>
<ul style="list-style:none;">
<li>
<p><span class="font3" style="font-weight:bold;">3.1. &nbsp;Feature Extraction Result</span></p></li></ul></li></ul>
<p><span class="font3">Total 28 vector features have been obtained from the extraction feature which consist of 13 MFCC vector features, 12 chroma frequencies vector features, and 1 vector feature each from spectral centroid, spectral roll-off, and zero crossing rate.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">3.2. &nbsp;&nbsp;&nbsp;Feature Selection Result</span></p>
<div>
<p><span class="font3">Correlation test results using IBM SPSS Statistic 25 can be seen in Figure 2.</span></p><img src="https://jurnal.harianregional.com/media/64437-3.jpg" alt="" style="width:451pt;height:26pt;">
<p><span class="font3" style="font-weight:bold;">Figure 2. </span><span class="font3">Correlation result of MFCC feature against genre</span></p>
</div><br clear="all"></li></ul>
<p><span class="font3">In the image above, there are several features in Pearson correlation that have an asterisk symbol. 1 asterisk symbol (*) means its correlation significance is below 0.05 and the 2 asterisk symbol (**) means its correlation significance is below 0.01 [4]. Some features that do not have an asterisk symbol indicate that the independent variable (feature) has a weak correlation to dependent variable (genre). In the MFCC feature, there are 8 vector features that have an asterisk symbol which means have a strong correlation.</span></p>
<p><span class="font0">chroma<sub>-</sub>set1 chroma<sub>-</sub>set1 chroma<sub>-</sub>set1</span></p>
<p><a href="#bookmark26"><span class="font0">chroma<sub>-</sub>set1 chroma<sub>-</sub>se12 chroma<sub>-</sub>set3 chroma<sub>-</sub>se14 chroma<sub>-</sub>set5 chroma<sub>-</sub>se16 chroma<sub>-</sub>set7 chroma<sub>-</sub>se18 chroma<sub>-</sub>set9 0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I2</span></a></p>
<ul style="list-style:none;"><li>
<p><a href="#bookmark27"><span class="font0">-.048 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.196” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.019 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.057 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.037 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-.077’ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.142&quot; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.008 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.O9θ&quot; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.016 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-.075’.096”</span></a></p></li></ul>
<p><a href="#bookmark28"><span class="font0">.150 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.000 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.572 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.087 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.262 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.020 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.000 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.820 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.007 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.622 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.024.004</span></a></p>
<p><a href="#bookmark29"><span class="font0">900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;900900</span></a></p>
<p><span class="font3" style="font-weight:bold;">Figure 3. </span><span class="font3">Correlation result of Chroma Frequency feature against genre</span></p>
<p><span class="font3">In chroma frequencies correlation result, there are 6 vector features that have an asterisk symbol which also means have a strong correlations</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font3">centroid</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">rolloff</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">zero cross</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">XX</span></p>
<p><span class="font2">.408</span></p></td><td style="vertical-align:top;">
<p><span class="font2">XX .339</span></p></td><td style="vertical-align:top;">
<p><span class="font2">XX .375</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">.000</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">.000</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">.000</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">900</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">900</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">900</span></p></td></tr>
</table>
<p><span class="font3" style="font-weight:bold;">Figure 4. </span><span class="font3">Correlation result of Spectral Centroid, Spectral Roll-Off, and Zero Crossing Rate against genre</span></p>
<p><span class="font3">Spectral centroid, spectral roll-off, and zero crossing rate features have a strong correlation. From the correlation result of all features, every features that has a significance value above 0.05 will be eliminated because values above this significance’s threshold have no significant impact on classification results.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">3.3. &nbsp;&nbsp;&nbsp;Classification Result</span></p></li></ul>
<p><span class="font3">From the correlation results, 4 different scenarios will be tested in classification step:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">a. &nbsp;Classification without feature selection</span></p></li>
<li>
<p><span class="font3">b. &nbsp;Classification with vector features selection of chroma frequencies</span></p></li>
<li>
<p><span class="font3">c. &nbsp;Classification with vector features selection of MFCC</span></p></li>
<li>
<p><span class="font3">d. &nbsp;Classification with vector features selection of both chroma and MFCC.</span></p></li></ul>
<p><span class="font3">The test results for each scenario using K-Nearest Neighbor method with K=13 and K-fold cross validation by 9 fold to distribute the test data can be seen in the table below:</span></p>
<p><span class="font3" style="font-weight:bold;">Table 1. </span><span class="font3">Accuracy of classification results on each scenario</span></p>
<p><span class="font3" style="font-weight:bold;">Scenario</span></p>
<p><span class="font3">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.70</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.77</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.74</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.84</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 2</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.76</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.76</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.78</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.79</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.84</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.83</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.85</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.82</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 4</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.69</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.73</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.65</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.72</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.64</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.75</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.68</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.74</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 6</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.77</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.75</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.76</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.79</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 7</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.82</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.81</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.81</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 8</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.75</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.77</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.73</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.77</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Fold 9</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.73</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.75</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.70</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">Average</span></p></td><td style="vertical-align:top;">
<p><span class="font3">0.75</span></p></td><td style="vertical-align:top;">
<p><span class="font3">0.77</span></p></td><td style="vertical-align:top;">
<p><span class="font3">0.75</span></p></td><td style="vertical-align:top;">
<p><span class="font3">0.79</span></p></td></tr>
</table><img src="https://jurnal.harianregional.com/media/64437-4.jpg" alt="" style="width:295pt;height:175pt;">
<p><span class="font3" style="font-weight:bold;">Figure 5. </span><span class="font3">Bar chart of each scenario’s result</span></p>
<p><span class="font3">Table 1. Shows a table of the accuracy obtained from the classification results of 4 types of scenarios while Figure 5 is a bar graph of the average accuracy of each scenario. Based on the graph above, the average accuracy from 4 different type scenarios ranging between 75.0% -79.3%. The lowest average accuracy is 75.0% for the scenario 3 which tests the MFCC vector features selection while the highest average accuracy is 79.3% for the scenario 4 which tests both MFCC and chroma frequencies vector features. The average accuracy of the scenario 1 is 75% which tests without involving feature selection.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark30"></a><span class="font3" style="font-weight:bold;"><a name="bookmark31"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h3></li></ul>
<p><span class="font3">Bivariate Pearson correlation is a method that used for feature selection. In this study, feature selection using Bivariate Pearson correlation tends to give a direct impact on the accuracy results of music genre classification. From the study that has been successfully carried out to test 4 scenarios, it shows that scenario which tests classification with feature selection tends to produce higher average accuracy with the highest average accuracy is 79.3% obtained by selecting vector features from both MFCC and chroma frequencies (scenario 4). For further research development, it is recommended to use other feature extraction methods, especially methods that have a single feature vector such as spectral centroid, spectral roll-off, and zero crossing rate.</span></p>
<h3><a name="bookmark32"></a><span class="font3" style="font-weight:bold;"><a name="bookmark33"></a>References</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;T. H. Simanjuntak, W. F. Mahmudy, Sutrisno, “Implementasi </span><span class="font3" style="font-style:italic;">Modified K-Nearest Neighbor </span><span class="font3">Dengan Otomatisasi Nilai K Pada Pengklasifikasian Penyakit Tanaman Kedelai” </span><span class="font3" style="font-style:italic;">Pengembangan Teknologi Informasi dan Ilmu Komputer</span><span class="font3">, Vol. 1, No. 2, p. 75-79, 2017.</span></p></li>
<li>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;K. D. Prebiana, I. G. S. Astawa, “Influence Optimization Feature Against Liver Disorders Diagnostic Results Using Artificial Neural Network” </span><span class="font3" style="font-style:italic;">Jurnal Elektronik Ilmu Komputer Udayana</span><span class="font3">, Vol. 8, No.3, p. 261-267, 2020.</span></p></li>
<li>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;N. M. Patil, M. U. Nemade, “Music Genre Classification Using MFCC, K-NN, and SVM Classifier” </span><span class="font3" style="font-style:italic;">Computer Engineering in Research Trends</span><span class="font3">, Vol. 4, Issue 2, p. 43-47, 2017.</span></p></li>
<li>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;K. Yeager, P. Bhattacharya, V. Reynolds, “Kent State University Library”, 22 September 2020. [Online]. Available:</span><a href="https://libguides.library.kent.edu/SPSS"><span class="font3"> https://libguides.library.kent.edu/SPSS.</span></a><span class="font3"> [Accessed 19 September 2020]</span></p></li>
<li>
<p><span class="font3">[5] &nbsp;&nbsp;&nbsp;D. A. Nasution, H. H. Khotimah, N. Chamidah, “Perbandingan Normalisasi Data Untuk Klasifikasi Wine Menggunakan Algoritma K-NN” </span><span class="font3" style="font-style:italic;">Computer Engineering System and Science</span><span class="font3">, Vol. 4, No. 1, p. 78-82, 2019.</span></p></li></ul>
<p><span class="font3">554</span></p>