---
layout: full_article
title: "Modified KNN-LVQ for Stairs Down Detection Based on Digital Image"
author: "Ahmad Wali Satria Bahari Johan, Sekar Widyasari Putri, Granita Hajar, Ardian Yusuf Wicaksono"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-75212 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-75212"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-75212"  
comments: true
---

<p><span class="font3" style="font-weight:bold;">LONTAR KOMPUTER VOL. 12, NO. 3 DECEMBER 2021</span></p>
<p><span class="font3" style="font-weight:bold;">DOI : 10.24843/LKJITI.2021.v12.i03.p02</span></p>
<p><span class="font3" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 30/E/KPT/2018</span></p>
<p><span class="font3" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font3" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font5" style="font-weight:bold;"><a name="bookmark1"></a>Modified KNN-LVQ for Stairs Down Detection Based on Digital Image</span></h1>
<p><span class="font3">Ahmad Wali Satria Bahari Johan<sup>a1</sup>, Sekar Widyasari Putri<sup>b2</sup>, Granita Hajar<sup>c3</sup>, Ardian Yusuf Wicaksono<sup>a4</sup></span></p>
<p><span class="font3"><sup>a</sup>Informatics, Faculty of Information Technology and Industry, Institut Teknologi Telkom Surabaya</span></p>
<p><span class="font3">Surabaya, Indonesia</span></p>
<p><span class="font1"><sup>1</sup></span><span class="font3">ahmadsatria13@ittelkom-sby.ac.id</span><span class="font2">(Corresponding author)</span></p>
<p><a href="mailto:4ardian@ittelkom-sby.ac.id"><span class="font1"><sup>4</sup></span><span class="font3">ardian@ittelkom-sby.ac.id</span></a></p>
<p><span class="font3"><sup>b</sup>Digital Business, Faculty of Information Technology and Industry, Institut Teknologi Telkom Surabaya</span></p>
<p><span class="font3">Surabaya, Indonesia</span></p>
<p><a href="mailto:2sekar@ittelkom-sby.ac.id"><span class="font1"><sup>2</sup></span><span class="font3">sekar@ittelkom-sby.ac.id</span></a></p>
<p><span class="font3"><sup>c</sup>Logistics Engineering, Faculty of Information Technology and Industry, Institut Teknologi Telkom Surabaya</span></p>
<p><span class="font3">Surabaya, Indonesia</span></p>
<p><a href="mailto:3granita@ittelkom-sby.ac.id"><span class="font1"><sup>3</sup></span><span class="font3">granita@ittelkom-sby.ac.id</span></a></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font3" style="font-style:italic;">Persons with visual impairments need a tool that can detect obstacles around them. The obstacles that exist can endanger their activities. The obstacle that is quite dangerous for the visually impaired is the stairs down. The stairs down can cause accidents for blind people if they are not aware of their existence. Therefore we need a system that can identify the presence of stairs down. This study uses digital image processing technology in recognizing the stairs down. Digital images are used as input objects which will be extracted using the Gray Level Co-occurrence Matrix method and then classified using the KNN-LVQ hybrid method. The proposed algorithm is tested to determine the accuracy and computational speed obtained. Hybrid KNN-LVQ gets an accuracy of 95%. While the average computing speed obtained is 0.07248 (s).</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Keywords:</span><span class="font3" style="font-style:italic;">Visual Impairments, GLCM, KNN, LVQ, Digital Image</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font3" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font3">Disability is a condition where a person has limitations in his physical condition. One type of disability is blindness, which is someone who has limited vision. They need tools to facilitate their activities. In this case, blind people often use sticks to detect objects around them and help them move. However, the stick itself has a weakness, where blind people have difficulty recognizing the types of objects around them. The ability to identify obstacles is also necessary for blind people. Where several obstacles can endanger their safety. Therefore, blind people need technology that can help them detect obstacles around them. One such obstacle is the down of the stairs. Where the stairs down is quite dangerous for anyone who falls.</span></p>
<p><span class="font3">Over the past few years, several technologies have been developed to help the visually impaired in their movements. Ultrasonic sensors have often been used to provide navigation or object detection. The research by Arnesh Sen Kaustav Sen Jayoti Das has developed a system to avoid an obstacle by using ultrasonic sensors. The ultrasonic sensors are paired up on the chest, knee, and toe[1]. However, that technique has limitations, where the sensor cannot detect objects that cannot be touched, such as stairs down. Because the sensor emits ultrasonic waves in a straight line and requires many sensors that attach to the body to detect obstacles, some technologies</span></p><img src="https://jurnal.harianregional.com/media/75212-1.jpg" alt="" style="width:170pt;height:114pt;">
<p><span class="font3" style="font-weight:bold;">Figure 1. </span><span class="font3">Camera Position</span></p>
<p><span class="font3">are being developed to help blind people currently based on an image to identify the obstacle. That has done by Alessandro Grassi by using a single smartphone camera. In that research, the system can identify obstacles such as traffic lights and doors[2]. Fitri Utaminingrum develops a wheelchair that able to detect stairs down by using a camera. That research uses GLCM and LVQ for the algorithm to classify between stairs down and floor. Fitri Utaminingrum develops a wheelchair that can detect stairs down by using a camera. That research uses GLCM and LVQ for the algorithm to classify between stairs down, and floor [3].</span></p>
<p><span class="font3">Several studies described illustrate that some technologies can help blind people in carrying out their activities. In this study, we use digital image processing technology to detect stairs down. This study uses Gray Level Co-occurrence Matrix feature extraction to calculate the feature values of the input image. The classification method that we propose is a hybrid K nearest neighbor algorithm and Learning vector quantization. KNN can find the closest distance between testing data and training data. Learning vector quantization has advantages in terms of computational speed. Learning vector quantization can carry out the training and testing process quickly, where fast computing time is very influential in the comfort of blind people.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font3" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span><br><br><span class="font3" style="font-weight:bold;"><a name="bookmark6"></a>2.1. &nbsp;&nbsp;&nbsp;Data Source</span></h2></li></ul>
<p><span class="font3">The source of this research dataset was taken from several different buildings. Each building has the characteristics of stairs with different ceramics. Some buildings have the characteristics of stairs with large tiled floors, which have a unique value that is different from small tile floors. A camera is placed on the chest, as shown in Fig 1. Two classes are used to detect stairs down. The two classes are stairs down and floors. The image taken from the camera is 480x640 pixels. In the picture, ROI will be taken as an indicator of the stairs down or floor. The ROI taken is 400x150 pixels and is at the bottom of the image. ROI image that is used during the training process is taken manually. Researchers take ROI, which has characteristics as stairs down, and ROI, which has floor characteristics. For taking ROI training, there is no provision for coordinates, but the size taken is 400x150. Meanwhile, when the testing process, ROI is taken automatically, the coordinate position used is fixed. The coordinates of the test ROI are at coordinates (40,400) to (440,550). Figure 2 shows the position of the ROI taken during testing. Figure 3(a) shows the ROI of the stairs, and Figure 3(b) shows the ROI of the floors. In this research, the data source used is divided into 2. The data source consists of images used during the training process (200 images) and a set of images used during the testing process (40 images).</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark7"></a><span class="font3" style="font-weight:bold;"><a name="bookmark8"></a>2.2. &nbsp;&nbsp;&nbsp;Gray Level Co-occurrence Matrix</span></h2></li></ul>
<p><span class="font3">Gray Level Co-occurrence Matrix (GLCM) is a method that is often used in conducting texture analysis or feature extraction[4][5]. GLCM analyzes a pixel in a digital image and determines the level of gray that occurs. The image to be performed feature extraction using Gray Level Cooccurrence matrix must be converted into a grayscale image. Gray Level Co-Occurrence Matrix has two parameters, namely distance, and angle. Characteristics obtained from the matrix pixel</span></p>
<div><img src="https://jurnal.harianregional.com/media/75212-2.jpg" alt="" style="width:213pt;height:244pt;">
<p><span class="font3" style="font-weight:bold;">Figure 2. </span><span class="font3">Input Image</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/75212-3.jpg" alt="" style="width:425pt;height:87pt;">
<p><span class="font3" style="font-weight:bold;">Figure 3. </span><span class="font3">(a) ROI of Stairs down (b) ROI of Floor</span></p>
</div><br clear="all">
<p><span class="font3">values, which have a certain value and form a pattern angle [6][7][8]. The angles in GLCM are </span><span class="font8">θ </span><span class="font3">= 0</span><span class="font1"><sup>o</sup></span><span class="font3">, 45</span><span class="font1"><sup>o</sup></span><span class="font3">, 90</span><span class="font1"><sup>o</sup></span><span class="font3">, and 135</span><span class="font1"><sup>o</sup> </span><span class="font3">and the distance values are </span><span class="font3" style="font-style:italic;">d</span><span class="font3">=1, 2, 3, and 4. Figure 4 is the angular orientation on GLCM. GLCM carries out several stages to perform feature extraction. The first stage is to form the initial GLCM matrix from a pair of 2 pixels based on a predetermined angle and distance. Then form a symmetric matrix by adding the GLCM matrix with the transpose matrix. Then normalize the GLCM matrix by dividing each matrix element by the number of pixel pairs. Six features will be generated from the GLCM feature extraction process. The following are six features used in this study[9]:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">1. &nbsp;&nbsp;&nbsp;Contrast: This feature calculates the difference in the gray level of an image. The high or low contrast value depends on the amount of difference in the gray level in the image. The contrast value is obtained by equation 1.</span></p></li></ul>
<p><span class="font7">L</span></p>
<p><span class="font8" style="font-style:italic;">Contrast </span><span class="font2" style="font-style:italic;">=</span><span class="font8"> ^ (a </span><span class="font2">— </span><span class="font8">b)</span><span class="font1"><sup>2 </sup></span><span class="font7" style="font-style:italic;">a,b</span><span class="font6" style="font-weight:bold;font-style:italic;">=0</span></p>
<div>
<p><span class="font3">(1)</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font3">2. &nbsp;&nbsp;&nbsp;Homogeneity: This feature calculates the value of gray homogeneity in an image. The homogeneity value will be higher if the gray level is almost the same. The homogeneity values are obtained by equation 2.</span></p>
<div>
<p><span class="font8" style="font-style:italic;">Homogeneity</span><span class="font8"> =</span></p>
</div><br clear="all">
<div>
<p><span class="font7">L</span></p>
<p><span class="font3"><sup>X</sup></span></p>
<p><span class="font7" style="font-style:italic;">a,b</span><span class="font6" style="font-weight:bold;font-style:italic;">=0</span></p>
</div><br clear="all">
<div>
<p><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">a, b</span><span class="font2" style="font-style:italic;">)</span><span class="font8" style="font-style:italic;">x</span><span class="font3" style="font-style:italic;"><sup>2</sup></span></p>
<p><span class="font8">1</span><span class="font2">|</span><span class="font8">a </span><span class="font2">— </span><span class="font8">b</span><span class="font2">|</span></p>
</div><br clear="all">
<div>
<p><span class="font3">(2)</span></p>
</div><br clear="all"></li></ul><img src="https://jurnal.harianregional.com/media/75212-4.jpg" alt="" style="width:149pt;height:80pt;">
<p><span class="font3" style="font-weight:bold;">Figure 4. </span><span class="font3">Angle Orientation</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">3. &nbsp;&nbsp;&nbsp;Correlation: This feature shows how the pixel reference correlates with its neighbors. The correlation values are obtained by equation 3.</span></p></li></ul>
<p><span class="font8" style="font-style:italic;">Correlation</span><span class="font8"> = </span><span class="font2" style="font-style:italic;">XX </span><span class="font8" style="font-style:italic;">P</span><span class="font7" style="font-style:italic;"><sub>i</sub>,<sub>j</sub> </span><span class="font2" style="text-decoration:line-through;"><sup>(</sup>a - ^ - ^)</span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)</span></p>
<p><span class="font7" style="font-style:italic;">a,b</span><span class="font6" style="font-weight:bold;font-style:italic;">=0</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">4. &nbsp;&nbsp;&nbsp;EEnergy: This feature calculates the level of gray distribution in an image. The energy values are obtained by equation 4.</span></p></li></ul>
<p><span class="font7">L</span></p>
<p><span class="font8" style="font-style:italic;">Energy = ∑ P</span><span class="font1"><sup>2</sup></span><span class="font8">(α,b) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(4)</span></p>
<p><span class="font7" style="font-style:italic;">a,b</span><span class="font6" style="font-weight:bold;font-style:italic;">=0</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">5. &nbsp;&nbsp;&nbsp;Dissimilarity: The dissimilarity values are obtained by equation 5.</span></p></li></ul>
<p><span class="font7">L</span></p>
<p><span class="font8" style="font-style:italic;">Dissimilarity </span><span class="font2" style="font-style:italic;">= &nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font8" style="font-style:italic;">P</span><span class="font7">a,b</span><span class="font2">|</span><span class="font8" style="font-style:italic;">a</span><span class="font2"> - </span><span class="font8" style="font-style:italic;">b</span><span class="font2">| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(5)</span></p>
<p><span class="font7" style="font-style:italic;">a,b</span><span class="font6" style="font-weight:bold;font-style:italic;">=0</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">6. &nbsp;&nbsp;&nbsp;AAngular Second Moment: The Angular second moment values are obtained by equation 6.</span></p></li></ul>
<p><span class="font7" style="font-style:italic;">L</span></p>
<p><span class="font8" style="font-style:italic;">ASM </span><span class="font2" style="font-style:italic;">=</span><span class="font2"> X </span><span class="font8">P</span><span class="font7">(i,j)</span><span class="font0">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(6)</span></p>
<p><span class="font7" style="font-style:italic;">a,b</span><span class="font6" style="font-weight:bold;font-style:italic;">=0</span></p>
<p><span class="font3">where :</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;L </span><span class="font2" style="font-style:italic;">=</span><span class="font3"> Number of gray levels in the image as specified by Number of levels</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;a,b </span><span class="font2" style="font-style:italic;">=</span><span class="font3"> Pixel coordinate</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;P</span><span class="font3"> = Element a,b of the normalized symmetrical GLCM</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;ϑ</span><span class="font3"> = the GLCM mean (being an estimate of the intensity of all pixels in the relationships that contributed to the GLCM)</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8"> &nbsp;&nbsp;&nbsp;σ</span><span class="font1"><sup>2</sup> </span><span class="font3">= the variance of the intensities of all reference pixels in the relationships that contributed to the GLCM</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark9"></a><span class="font3" style="font-weight:bold;"><a name="bookmark10"></a>2.3. &nbsp;&nbsp;&nbsp;K Nearest Neighbor</span></h2></li></ul>
<p><span class="font3">K nearest neighbor (KNN) is a supervised learning algorithm, in which this algorithm generates a classification based on the majority of the k-value categories provided in the training data [10] [11]. The purpose of this algorithm is to classify new objects based on attributes and samples from training data. The K Nearest neighbor algorithm uses neighborhood classification as the predicted value for the new instance value.[12]. Training data is placed in a place that will be used during the classification process. The unknown sample class is determined by a majority vote of</span></p>
<div><img src="https://jurnal.harianregional.com/media/75212-5.jpg" alt="" style="width:106pt;height:121pt;">
<p><span class="font3" style="font-weight:bold;">Figure 5. </span><span class="font3">KNN concept</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/75212-6.jpg" alt="" style="width:255pt;height:141pt;">
<p><span class="font3" style="font-weight:bold;">Figure 6. </span><span class="font3">LVQ Architecture</span></p>
</div><br clear="all">
<p><span class="font3">its neighboring samples in the training pattern space [13][14]. The most influential parameter on K nearest neighbor is the k-value. Where k-value is a parameter of how many nearest neighbors of the object are classified. Figure 5 is an example of the KNN concept. Where the k-value used is 3. Then the algorithm will find the three closest neighbors using the Euclidian distance equation. Equation 7 is a way to find the nearest neighbor using Euclidian distances. After getting the three closest neighbors, the next step is to calculate the majority of the class in the three neighbors. Where the majority class will be selected as the result of the classification.</span></p>
<div>
<p><span class="font8" style="font-style:italic;">D</span><span class="font6" style="font-weight:bold;font-style:italic;">(</span><span class="font8" style="font-style:italic;">a,b)</span></p>
</div><br clear="all">
<div>
<p><span class="font7">n</span></p>
<p><span class="font2">X</span><span class="font8">(a</span><span class="font7">k </span><span class="font2">- </span><span class="font8" style="font-style:italic;">b</span><span class="font7" style="font-style:italic;">k</span><span class="font8">)</span><span class="font1"><sup>2</sup></span></p>
<p><span class="font7">k=1</span></p>
</div><br clear="all">
<div>
<p><span class="font3">(7)</span></p>
</div><br clear="all">
<p><span class="font3">Where :</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;n</span><span class="font3"> = number of data</span></p></li>
<li>
<p><span class="font3" style="font-style:italic;">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;D</span><span class="font6" style="font-weight:bold;font-style:italic;">(</span><span class="font8" style="font-style:italic;">a, b</span><span class="font2" style="font-style:italic;">)</span><span class="font3"> = closest euclidean distance</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;a</span><span class="font3"> = data 1</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;b</span><span class="font3"> = data 2</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;k</span><span class="font3"> = feature to - n</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark11"></a><span class="font3" style="font-weight:bold;"><a name="bookmark12"></a>2.4. &nbsp;&nbsp;&nbsp;Learning Vector Quantization</span></h2></li></ul>
<p><span class="font3">Learning Vector Quantization (LVQ) is part of the classification of artificial neural networks with supervised competitive learning [15]. LVQ works by using a clustering method where the tar-get/class has been defined by the architecture [16]. The LVQ learning model is trained significantly faster than other algorithms such as Back Propagation Neural Network. It can summarize</span></p><img src="https://jurnal.harianregional.com/media/75212-7.jpg" alt="" style="width:234pt;height:145pt;">
<p><span class="font3" style="font-weight:bold;">Figure 7. </span><span class="font3">Hybrid KNN-LVQ</span></p>
<p><span class="font3">or reduce large datasets to a small number of vectors. The competitive layer will automatically learn to classify the input vectors. The classes obtained from this competitive layer only depend on the distance between the input vectors. If the input vectors are close to the same, the competitive layer will classify the two input vectors into the same class [17][18]. Figure 6 is an LVQ architecture with two classes. The following are some of the steps in running LVQ [19]:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">1. &nbsp;&nbsp;&nbsp;Initialization of initial weight (</span><span class="font8">W</span><span class="font7">j</span><span class="font3">) and value of learning rate (</span><span class="font8">α</span><span class="font3">). The weights used are equal to the number of classes. Where each weight represents its respective class.</span></p></li>
<li>
<p><span class="font3">2. &nbsp;&nbsp;&nbsp;Determine the number of training iterations</span></p></li>
<li>
<p><span class="font3">3. &nbsp;&nbsp;&nbsp;Find the closest distance (</span><span class="font8">j</span><span class="font3">) using equation 8.</span></p></li></ul>
<p><a href="#bookmark13"><span class="font2" style="font-style:italic;"><sup>j</sup> =</span><span class="font4"> <sup>|</sup></span><span class="font2"><sup>x</sup> - </span><span class="font8" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">j</span><span class="font2"> I</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font3">4. &nbsp;&nbsp;&nbsp;Update the selected weight value as the minimum value. If the selected condition is the same as the target, an update is carried out using equation 9. If the selected weight is not the same as the target, then the update uses equation 10.</span></p></li></ul>
<p><a href="#bookmark14"><span class="font8" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">j </span><span class="font2" style="font-style:italic;">= </span><span class="font8" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">j </span><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">old)</span><span class="font8"> + </span><span class="font8" style="font-style:italic;">α</span><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">x </span><span class="font2" style="font-style:italic;">— </span><span class="font8" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">j </span><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">old))</span><span class="font3">(9)</span></a></p>
<p><a href="#bookmark15"><span class="font8" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">j </span><span class="font2" style="font-style:italic;">= </span><span class="font8" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">j </span><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">old) </span><span class="font2" style="font-style:italic;">— </span><span class="font8" style="font-style:italic;">α</span><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">x </span><span class="font2" style="font-style:italic;">— </span><span class="font8" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">j </span><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">old))</span><span class="font3">(10)</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font3">5. &nbsp;&nbsp;&nbsp;Stop the training process until the specified number of iterations</span></p></li></ul>
<p><span class="font3">Where :</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;W</span><span class="font7" style="font-style:italic;">j</span><span class="font3"> = Weight of LVQ</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;W</span><span class="font7" style="font-style:italic;">j </span><span class="font2" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">old)</span><span class="font3"> = Old weight</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;j</span><span class="font3"> = Distance value</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;x</span><span class="font3"> = Training data</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;α</span><span class="font3"> = Learning rate</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font3" style="font-weight:bold;"><a name="bookmark17"></a>2.5. &nbsp;&nbsp;&nbsp;Hybrid KNN-LVQ</span></h2></li></ul>
<p><span class="font3">The limitation of the KNN classifier is a false classification of test images when the majority of the nearest neighbors have closely matched features[20]. The computational time of KNN depends on the amount of training data used. The more training data, the longer it takes. To overcome this problem, KNN could be combined with another classifier[21][22]. Here we combine KNN with</span></p>
<p><span class="font3" style="font-weight:bold;">LONTAR KOMPUTER VOL. 12, NO. 3 DECEMBER 2021 DOI : 10.24843/LKJITI.2021.v12.i03.p02</span></p>
<p><span class="font3" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 30/E/KPT/2018</span></p>
<p><span class="font3">LVQ to classifying between floor and stairs down. Where LVQ has the advantage of speeding up the training and testing process. The idea is that the program runs KNN to get 30 data from 200 training data that has the closest value to the test data so that there are only 30 selected training data to be continued in the LVQ process. Where training is carried out on the LVQ to update the initial weight. The initial weight used before training is the average value of the GLCM features of each class. Two weights represent the class of stairs down and floor. The LVQ training process was carried out 100 times. After the training process is complete, it is continued to test data testing with LVQ using the latest weights from the training process. The KNN-LVQ hybrid concept is depicted in figure 7.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font3" style="font-weight:bold;"><a name="bookmark19"></a>3. &nbsp;&nbsp;&nbsp;Result</span><br><br><span class="font3" style="font-weight:bold;"><a name="bookmark20"></a>3.1. &nbsp;&nbsp;&nbsp;Peformance Measures</span></h2></li></ul>
<p><span class="font3">There are 240 pieces of image data used in this study. Where the image will be used for training data and partly used as test data. Training and testing data use different images. In the training process using the KNN-LVQ hybrid algorithm, there are as many as 200 images consisting of images indicating the stairs down and images indicating the floor. In the testing process, 40 images consist of images of stairs down and floors. In the testing process to be carried out, we look for the accuracy value obtained using equation 11 [23].TP (true positive) shows the appropriate prediction results, namely the stairs down. TN (true negative) indicates an incorrect prediction result. Where the test data is the stairs down, but the results of the floor prediction. FP (false positive) indicates the correct prediction result, namely the floor. FN (false negative) indicates an incorrect prediction result. Where the test data is the floor, but the prediction results are stairs down.</span></p>
<div>
<p><span class="font8" style="font-style:italic;">A </span><span class="font2" style="font-style:italic;">=</span></p>
</div><br clear="all">
<div>
<p><span class="font8" style="font-style:italic;">TP</span><span class="font8"> + </span><span class="font8" style="font-style:italic;">TN</span></p>
</div><br clear="all">
<div>
<p><span class="font8" style="font-style:italic;">TP</span><span class="font8"> + </span><span class="font8" style="font-style:italic;">FP</span><span class="font8"> + </span><span class="font8" style="font-style:italic;">TN</span><span class="font8"> + </span><span class="font8" style="font-style:italic;">FN</span></p>
</div><br clear="all">
<div>
<p><span class="font3">(11)</span></p>
</div><br clear="all">
<p><span class="font3">Where :</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;A</span><span class="font3"> = Accuracy</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;T P</span><span class="font3"> = True positive</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;T N</span><span class="font3"> = True negative</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;F P</span><span class="font3"> = False positive</span></p></li>
<li>
<p><span class="font3">•</span><span class="font8" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;F N</span><span class="font3"> = False negative</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font3" style="font-weight:bold;"><a name="bookmark22"></a>3.2. &nbsp;&nbsp;&nbsp;Testing for Classification Accuracy</span></h2></li></ul>
<p><span class="font3">Testing is carried out by comparing the accuracy results obtained from 3 classification methods. The first method is K nearest neighbor, then Learning Vector Quantization. Next is our proposed method, namely Hybrid KNN-LVQ. This test is carried out with the same parameters. The GLCM distance and angle parameters used are </span><span class="font3" style="font-style:italic;">d</span><span class="font3"> =1 and </span><span class="font8" style="font-style:italic;">θ</span><span class="font3"> = 0</span><span class="font1"><sup>o</sup></span><span class="font3">. The GLCM parameter is used when performing feature extraction. So that this test is carried out with the same test data and the same feature value. As for LVQ, the training iterations carried out were 100 times and the learning rate used was </span><span class="font8" style="font-style:italic;">α</span><span class="font3"> = 0.5. Iteration and learning rate parameters are used during the LVQ and Hybrid KNN-LVQ classification processes. Tests were carried out using 40 data consisting of 20 data features of stairs down and 20-floor feature data. This test is shown in Table 1. From the tests’ results, the K nearest neighbor algorithm gets the lowest accuracy, which is 90%. The Learning Vector Quantization Algorithm gets an accuracy of 92.5%. While the method that we propose can get a better accuracy result that is 95%. These results indicate that the classification process carried out by Learning Vector Quantization gets better results when the training data is selected using K Nearest Neighbor. By using the process of finding the nearest neighbor on K nearest neighbor, we can obtain a dataset for training that is more in line with the given testing data.</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font3">Method</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Stairs down</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Floor</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Accuracy</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">LVQ</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">18</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">19</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">92.5%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3">KNN</span></p></td><td style="vertical-align:top;">
<p><span class="font3">18</span></p></td><td style="vertical-align:top;">
<p><span class="font3">18</span></p></td><td style="vertical-align:top;">
<p><span class="font3">90%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3">Hybrid KNN-LVQ</span></p></td><td style="vertical-align:top;">
<p><span class="font3">19</span></p></td><td style="vertical-align:top;">
<p><span class="font3">19</span></p></td><td style="vertical-align:top;">
<p><span class="font3">95%</span></p></td></tr>
</table>
<p><span class="font3" style="font-weight:bold;">Table 2. </span><span class="font3">Result of Computation Time Testing</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font3">No</span></p></td><td style="vertical-align:top;">
<p><span class="font3">LVQ</span></p></td><td style="vertical-align:top;">
<p><span class="font3">KNN</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Hybrid LVQ-KNN</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02600 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02499 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.06601 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02899 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.06100 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.07702 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02599 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.05937 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.04302 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.03001 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.10656 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.11347 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.03000 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02700 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.08907 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">6</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02600 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.05902 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.05480 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">7</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02900 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.03400 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.08600 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">8</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02500 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.03100 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.08454 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.03200 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02600 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.06385 (s)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">10</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.02500 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.05001 (s)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.04700 (s)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3">Average</span></p></td><td style="vertical-align:top;">
<p><span class="font3">0.02779 (s)</span></p></td><td style="vertical-align:top;">
<p><span class="font3">0.04789 (s)</span></p></td><td style="vertical-align:top;">
<p><span class="font3">0.07248 (s)</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark23"></a><span class="font3" style="font-weight:bold;"><a name="bookmark24"></a>3.3. &nbsp;&nbsp;&nbsp;Testing of Computation Time</span></h2></li></ul>
<p><span class="font3">Computational time testing is carried out to determine the average speed of each algorithm in classifying. This is quite influential on the comfort for users of the stair down detection system. It takes computing time as quickly as possible in the detection. This test uses ten pictures of the same stairs down. In table 2, it can be seen that LVQ produces the fastest computation time of 0.02779 (s). While the KNN-LVQ hybrid produces the slowest computation time, which is 0.07248 (s). This is because the KNN-LVQ hybrid performs two processes, namely the KNN process to obtain K-30 as training data for the training process and test on LVQ.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark25"></a><span class="font3" style="font-weight:bold;"><a name="bookmark26"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font3">TThis research aims to create a system that can detect the presence of stairs down. The GLCM feature extraction method was used to generate six feature values. The six features are contrast, homogeneity, energy, angular second moment, correlation, and dissimilarity. We propose a combination of classification algorithms in determining the class on the test image, where there are two classes, namely stairs down and floors. The combined algorithms are K nearest neighbor and Learning vector quantization. We call this merger Hybrid KNN-LVQ, where KNN works to get K-30. K-30 is the 30 data that is closest in value to the test data. Furthermore, the LVQ process conducts training on 30 selected data to update the weights and the testing process on the test data. From the results of the tests conducted, the hybrid KNN-LVQ method produced a better accuracy of 95%. However, the KNN-LVQ method has a longer computation time than the comparison algorithm, as shown in Table 2.</span></p>
<h2><a name="bookmark27"></a><span class="font3" style="font-weight:bold;"><a name="bookmark28"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;A. Sen, K. Sen, and J. Das, “Ultrasonic blind stick for completely blind people to avoid any kind of obstacles,” in </span><span class="font3" style="font-style:italic;">2018 IEEE SENSORS</span><span class="font3">, 2018, pp. 1–4.</span></p></li>
<li>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;A. Grassi and C. Guaragnella, “Defocussing estimation for obstacle detection on single camera smartphone assisted navigation for vision impaired people,” in </span><span class="font3" style="font-style:italic;">2014 IEEE International Symposium on Innovations in Intelligent Systems and Applications (INISTA) Proceedings</span><span class="font3">, 2014, pp. 309–312.</span></p></li>
<li>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;A. W. S. Bahari Johan, F. Utaminingrum, and T. K. Shih, “Stairs descent identification for smart wheelchair by using glcm and learning vector quantization,” in </span><span class="font3" style="font-style:italic;">2019 Twelfth International Conference on Ubi-Media Computing (Ubi-Media)</span><span class="font3">, 2019, pp. 64–68.</span></p></li>
<li>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;R. Yusof and N. R. Rosli, “Tropical wood species recognition system based on gabor filter as image multiplier,” in </span><span class="font3" style="font-style:italic;">2013 International Conference on Signal-Image Technology InternetBased Systems</span><span class="font3">, 2013, pp. 737–743.</span></p></li>
<li>
<p><span class="font3">[5] &nbsp;&nbsp;&nbsp;C. Malegori, L. Franzetti, R. Guidetti, E. Casiraghi, and R. Rossi, “Glcm, an image analysis technique for early detection of biofilm,” </span><span class="font3" style="font-style:italic;">Journal of Food Engineering</span><span class="font3">, vol. 185, pp. 48–55, 2016.</span></p></li>
<li>
<p><span class="font3">[6] &nbsp;&nbsp;&nbsp;M. Saleck, A. ElMoutaouakkil, and M. Moucouf, “Tumor detection in mammography images using fuzzy c-means and glcm texture features,” in </span><span class="font3" style="font-style:italic;">2017 14th International Conference on Computer Graphics, Imaging and Visualization (CGiV)</span><span class="font3">. Los Alamitos, CA, USA: IEEE Computer Society, may 2017, pp. 122–125.</span></p></li>
<li>
<p><span class="font3">[7] &nbsp;&nbsp;&nbsp;Z. khan and S. Alotaibi, “Computerised segmentation of medical images using neural networks and glcm,” in </span><span class="font3" style="font-style:italic;">2019 International Conference on Advances in the Emerging Computing Technologies (AECT)</span><span class="font3">. Los Alamitos, CA, USA: IEEE Computer Society, feb 2020, pp. 1–5.</span></p></li>
<li>
<p><span class="font3">[8] &nbsp;&nbsp;&nbsp;S. Barburiceanu, R. Terebes, and S. Meza, “3d texture feature extraction and classification using glcm and lbp-based descriptors,” </span><span class="font3" style="font-style:italic;">Applied Sciences</span><span class="font3">, vol. 11, no. 5, 2021. [Online]. Available: </span><a href="https://www.mdpi.com/2076-3417/11/5/2332"><span class="font3">https://www.mdpi.com/2076-3417/11/5/2332</span></a></p></li>
<li>
<p><span class="font3">[9] &nbsp;&nbsp;&nbsp;T. S. A. Sukiman, M. Zarlis, and S. Suwilo, “Feature extraction method glcm and lvq in digital image-based face recognition,” </span><span class="font3" style="font-style:italic;">Applied Sciences</span><span class="font3">, vol. 4, no. 1, 2019.</span></p></li>
<li>
<p><span class="font3">[10] &nbsp;&nbsp;&nbsp;M. Kenyhercz and N. Passalacqua, “Chapter 9 - missing data imputation methods and their performance with biodistance analyses,” in </span><span class="font3" style="font-style:italic;">Biological Distance Analysis</span><span class="font3">, M. A. Pilloud and J. T. Hefner, Eds. San Diego: Academic Press, 2016, pp. 181–194.</span></p></li>
<li>
<p><span class="font3">[11] &nbsp;&nbsp;&nbsp;“Chapter 9 - object categorization using adaptive graph-based semi-supervised learning,” in </span><span class="font3" style="font-style:italic;">Handbook of Neural Computation</span><span class="font3">, P. Samui, S. Sekhar, and V. E. Balas, Eds. Academic Press, 2017, pp. 167–179.</span></p></li>
<li>
<p><span class="font3">[12] &nbsp;&nbsp;&nbsp;K. Taunk, S. De, S. Verma, and A. Swetapadma, “A brief review of nearest neighbor algorithm for learning and classification,” in </span><span class="font3" style="font-style:italic;">2019 International Conference on Intelligent Computing and Control Systems (ICCS)</span><span class="font3">, 2019, pp. 1255–1260.</span></p></li>
<li>
<p><span class="font3">[13] &nbsp;&nbsp;&nbsp;X. Zhu and T. Sugawara, “Meta-reward model based on trajectory data with k-nearest neighbors method,” in </span><span class="font3" style="font-style:italic;">2020 International Joint Conference on Neural Networks (IJCNN)</span><span class="font3">, 2020, pp. 1–8.</span></p></li>
<li>
<p><span class="font3">[14] &nbsp;&nbsp;&nbsp;A. K. Gupta, “Time portability evaluation of rcnn technique of od object detection — machine learning (artificial intelligence),” in </span><span class="font3" style="font-style:italic;">2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)</span><span class="font3">, 2017, pp. 3127–3133.</span></p></li>
<li>
<p><span class="font3">[15] &nbsp;&nbsp;&nbsp;P. Melin, J. Amezcua, F. Valdez, and O. Castillo, “A new neural network model based on the lvq algorithm for multi-class classification of arrhythmias,” </span><span class="font3" style="font-style:italic;">Information Sciences</span><span class="font3">, vol. 279, pp. 483–497, 2014.</span></p></li>
<li>
<p><span class="font3">[16] &nbsp;&nbsp;&nbsp;S. Qiu, L. Gao, and J. Wang, “Classification and regression of elm, lvq and svm for e-nose data of strawberry juice,” </span><span class="font3" style="font-style:italic;">Journal of Food Engineering</span><span class="font3">, vol. 144, pp. 77–85, 2015.</span></p></li>
<li>
<p><span class="font3">[17] &nbsp;&nbsp;&nbsp;E. Subiyantoro, A. Ashari, and Suprapto, “Cognitive classification based on revised bloom’s taxonomy using learning vector quantization,” in </span><span class="font3" style="font-style:italic;">2020 International Conference on Computer Engineering, Network, and Intelligent Multimedia (CENIM)</span><span class="font3">, 2020, pp. 349–353.</span></p></li>
<li>
<p><span class="font3">[18] &nbsp;&nbsp;&nbsp;I. M. A. S. Widiatmika, I. N. Piarsa, and A. F. Syafiandini, “Recognition of the baby footprint characteristics using wavelet method and k-nearest neighbor (k-NN),” </span><span class="font3" style="font-style:italic;">Lontar Komputer : Jurnal Ilmiah Teknologi Informasi</span><span class="font3">, vol. 12, no. 1, p. 41, mar 2021. [Online]. Available: </span><a href="https://doi.org/10.24843%2Flkjiti.2021.v12.i01.p05"><span class="font3">https://doi.org/10.24843%2Flkjiti.2021.v12.i01.p05</span></a></p></li>
<li>
<p><span class="font3">[19] &nbsp;&nbsp;&nbsp;K. J. Devi, G. B. Moulika, K. Sravanthi, and K. M. Kumar, “Prediction of medicines using lvq methodology,” in </span><span class="font3" style="font-style:italic;">2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS)</span><span class="font3">, 2017, pp. 388–391.</span></p></li>
<li>
<p><span class="font3">[20] &nbsp;&nbsp;&nbsp;E. Haerani, L. Apriyanti, and L. K. Wardhani, “Application of unsupervised k nearest neighbor (UNN) and learning vector quantization (LVQ) methods in predicting rupiah to dollar,” in </span><span class="font3" style="font-style:italic;">2016 4th International Conference on Cyber and IT Service Management</span><span class="font3">. IEEE, apr 2016.</span></p></li>
<li>
<p><span class="font3">[21] &nbsp;&nbsp;&nbsp;O. R. de Lautour and P. Omenzetter, “Nearest neighbor and learning vector quantization classification for damage detection using time series analysis,” </span><span class="font3" style="font-style:italic;">Structural Control and Health Monitoring</span><span class="font3">, 2009.</span></p></li>
<li>
<p><span class="font3">[22] &nbsp;&nbsp;&nbsp;P. Sonar, U. Bhosle, and C. Choudhury, “Mammography classification using modified hybrid svm-knn,” in </span><span class="font3" style="font-style:italic;">2017 International Conference on Signal Processing and Communication (ICSPC)</span><span class="font3">, 2017, pp. 305–311.</span></p></li>
<li>
<p><span class="font3">[23] &nbsp;&nbsp;&nbsp;R. J. A. Kautsar, F. Utaminingrum, and A. S. Budi, “Helmet monitoring system using hough circle and HOG based on KNN,” </span><span class="font3" style="font-style:italic;">Lontar Komputer : Jurnal Ilmiah Teknologi Informasi</span><span class="font3">, vol. 12, no. 1, p. 13, mar 2021.</span></p></li></ul>
<p><span class="font3">150</span></p>