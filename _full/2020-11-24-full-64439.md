---
layout: full_article
title: "Endek Classification Based On GLCM Using Artificial Neural Networks with Adam Optimization"
author: "Putu Wahyu Tirta Guna, Luh Arida Ayu Ayu Rahning Putri"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-64439 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-64439"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-64439"  
comments: true
---

<p><span class="font1">p-ISSN: 2301-5373</span></p>
<p><span class="font1">e-ISSN: 2654-5101</span></p>
<p><span class="font1">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font1">Volume 9, No 2. November 2020</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font2" style="font-weight:bold;"><a name="bookmark1"></a>Endek Classification Based on GLCM Using Artificial Neural Networks with Adam Optimization</span></h1>
<p><span class="font1">Putu Wahyu Tirta Guna<sup>a1</sup></span><span class="font1" style="font-weight:bold;">, </span><span class="font1">Luh Arida Ayu Rahning Putri<sup>a2</sup></span></p>
<p><span class="font1"><sup>a</sup>Informatics Departemen, Faculty of Math and Science, Udayana University Badung, Bali, Indonesia</span></p>
<p><a href="mailto:1wahyutirta123@gmail.com"><span class="font1"><sup>1</sup>wahyutirta123@gmail.com</span></a></p>
<p><a href="mailto:2rahningputri@unud.ac.id"><span class="font1"><sup>2</sup>rahningputri@unud.ac.id</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">Tenun is one of the cloth-making techniques that have been around for centuries. Like other areas in Indonesia, Bali also has a traditional tenun cloth, namely tenun endek. Not many people know that endek itself has 4 known variances. Nowadays, computing and classification algorithm can be implemented to solve classification problem with respect to the features data as input. We can use this computing power to digitalize these endek variances. The features extraction algorithm used in this research is GLCM. Where these features will act as input for the neural network model later. Optimizer algorithm is used to adjust neural network model in back-propagation phase. There are a lot of optimizer algorithms that can be used in this phase. This research used adam as its optimizer in which is one of the newest and most popular optimizer algorithm. To compare its performace we also use SGD which is older but also a popular optimizer algorithm. Later we find that adam algorithm gaves 33% accuracy which is better than what SGD algorithm gave which is 23% accuracy. Shorter epoch also gives better result for overall model accuracy.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">Neural Network, Optimizer, Adam, GLCM, Endek</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font1">Tenun is one of the cloth-making techniques that have been around for centuries. Tenun Culture grows and develops in various places according to human civilization, culture in the local area, colors and decorations or patterns. Therefore, tenun has its own unique characteristics for each region. Like other areas in Indonesia, Bali is one of the regions in Indonesia which has a ton of cultures and expressions. This Culture and expressions have been a connection to public activity in Bali for decades. This cultures and expressions does affect variances of balinese tenun which is known as endek cloth.</span></p>
<p><span class="font1">Not many people know that endek cloth itself has 4 known variances or patterns such as cemplong pattern, sekar pattern, gringsing pattern dan wajik pattern [1]. To solve this problem. We can use computing and several classification to digitalize these patterns. In result, Balinese people will recognize these endek patterns in digital.</span></p>
<p><span class="font1">Image classification can be conducted by using characteristics or features in which are extracted using the extraction algorithm. These features can represent the image itself. Where later these features data will be classified into classes using artificial neural networks.</span></p>
<p><span class="font1">There are two main processs in classification case. First is feature extraction and second is classification process. Feature extraction is an important early process in the classification and image pattern recognition. Several researchs have been conducted to classify traditional cloth like endek. Rahayuda [1] used GLCM as image features exctraction algorithm. Based on [1], this research too, uses GLCM as features extraction algorithm. Where the characteristics or texture features extracted from the endek images data later will become training and testing data for the neural networks.</span></p>
<p><span class="font1">Dewantara [2] used CNN which is one of deep learning algorithm to classifies endek cloth. Deep learning is a learning method based on neural networks. This architecture gave 80% as average accuracy. This implies that any neural networks can be used as classification algorithm for endek cloth. This study uses data from [2], but the classification process in this study</span></p>
<p><span class="font1">uses a basic artificial neural network architecture that utilizes ADAM as an optimizer which is one of the newest optimization algorithms. Hopefully, by utilizing this latest optimizer algorithm, this architecture can outperform the architecture in [2].</span></p>
<p><span class="font1">During the training process, training data consisting of image texture features will act as input of the artificial neural network architecture in which will be formed later. After going through the foward propagation process, the classification layer on the artificial neural network that has been formed will provide feedback in the form of class probability and classification errors. After passing the correction process, testing data will act as input of the artificial neural network to validate the model.</span></p>
<p><span class="font1">The process of correcting the weight and bias of each neuron in the artificial neural network is carried out at the back-propagation step. The optimization process at this step uses the Adam optimization algorithm. Adam optimizer is an optimization algorithm introduced by D. P. Kingma and J. Lei Ba in 2015. Adam is an efficient stochastic optimization that requires only gradient with less memory requirements. This method calculates the individual adaptive learning rate for each parameter from estimation of first and second moments of the gradients [3].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Reseach Methods</span></h2></li></ul>
<p><span class="font1">The system flow that had been built in this research includes preprocessing, GLCM feature extraction and image classification process using artificial neural networks. For more details, see figure 1.</span></p><img src="https://jurnal.harianregional.com/media/64439-1.jpg" alt="" style="width:407pt;height:87pt;">
<p><span class="font1" style="font-weight:bold;">Figure 1. </span><span class="font1">Flowchart System</span></p>
<p><span class="font1">As seen in figure 1, endek images have to be converted into grayscale format so that images will only have one color channel. Later, system can performes feature extraction using GLCM algorithm to these images. These GLCM features will act as input for the artificial neural networks in the training and testing phase. Output classes from testing phase is endek cloth variances which are endek cloth variances which are cemplong pattern, sekar pattern, gringsing pattern dan wajik pattern.</span></p>
<p><span class="font1">This research uses Python 3 programming language with GLCM algorithm from the sklearn library and tensorflow framework version 2.0 as the base for programming the artificial neural network. This framework provides a variety of layer object models and optimizers that can be customized according to user requirements.The artificial neural network model which was formed in this research consists of 16 input neurons.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font1" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Data</span></h2></li></ul>
<p><span class="font1">The data used in this study are secondary data types obtained from [2]. This data contains 4 types(later we will use as class) of endek cloth with different patterns, such as endek cemplong, sringsing, sekar dan wajik(diamond). each class contain approximatey 60 images with a resolution of 40 x 40 pixels.. The data will be divided into training data and test data where the training data is 80% of the total data and test data is 20% of the total data.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font1" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Artificial Neural Network</span></h2></li></ul>
<p><span class="font1">Artificial Neural Network is one of the artificial representations of the human brain which always tries to simulate the learning process in the human brain.</span></p><img src="https://jurnal.harianregional.com/media/64439-2.jpg" alt="" style="width:258pt;height:141pt;">
<p><span class="font1" style="font-weight:bold;">Figure 2. </span><span class="font1">ANN Illustration [2]</span></p>
<p><span class="font1">Explanation each parts in figure 2:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7">1.</span><span class="font1"> &nbsp;&nbsp;&nbsp;A set of synapses, or bridges, each classified by weight or strength.</span></p></li>
<li>
<p><span class="font7">2.</span><span class="font1"> &nbsp;&nbsp;&nbsp;An adder to add up the input signals. Weighted from the synaptic strength of each neuron.</span></p></li>
<li>
<p><span class="font7">3.</span><span class="font1"> &nbsp;&nbsp;&nbsp;An activation function to limit the output amplitude of the neuron.</span></p></li></ul>
<p><span class="font1">The process of forward propagation can be seen in the equations 1, 2 and, 3 [4].</span></p>
<p><a href="#bookmark10"><span class="font7" style="font-style:italic;">Z</span><span class="font6" style="font-style:italic;">t</span><span class="font7" style="font-style:italic;"><sub>n</sub> </span><span class="font6" style="font-style:italic;">j </span><span class="font7" style="font-style:italic;">=</span><span class="font7"> (∑‰</span><span class="font5">1 </span><span class="font7" style="font-style:italic;">X</span><span class="font6" style="font-style:italic;">i </span><span class="font7" style="font-style:italic;">v<sub>l</sub>j)</span><span class="font7"> + </span><span class="font7" style="font-style:italic;">bias</span><span class="font1">(1)</span></a></p>
<p><span class="font1">Inputs of each data will be multiplied by the weight and added by the bias of each neuron.</span></p>
<p><a href="#bookmark11"><span class="font7" style="font-style:italic;">Z</span><span class="font6" style="font-style:italic;">j</span><span class="font7" style="font-style:italic;">=KZ</span><span class="font6" style="font-style:italic;">in j</span><span class="font7" style="font-style:italic;">)</span><span class="font1">(2)</span></a></p>
<p><span class="font1">The output of the previous equation will go through an activation function of the neurons.</span></p>
<p><a href="#bookmark12"><span class="font7" style="font-style:italic;">y<sub>k</sub>=f(Z</span><span class="font6" style="font-style:italic;">in </span><span class="font7" style="font-style:italic;"><sub>k</sub>)</span><span class="font1">(3)</span></a></p>
<p><span class="font1">The output from the hidden layer will go through the activation function of the classification neurons.</span></p>
<p><span class="font1">Table 1 shows details and the activation functions of each layers of ANN models that had been formed in this research. Number of neurons in the hidden layers are based on rules which are provided from previous research [4].</span></p>
<p><span class="font1" style="font-weight:bold;">Table 1. </span><span class="font1">Detail Each Layer</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Layer</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Number Of Neurons</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Activation Function</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Input Layer</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">16</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">-</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">Hidden Layer 1</span></p></td><td style="vertical-align:top;">
<p><span class="font1">29</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-style:italic;">Relu</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Hidden Layer 2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">8</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1" style="font-style:italic;">Relu</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">Classification Layer</span></p></td><td style="vertical-align:top;">
<p><span class="font1">4</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-style:italic;">Softmax</span></p></td></tr>
</table>
<p><span class="font1">The learning process of the neural network model uses the epoch variable as the maximum number of learning cycles that the model passes and the learning rate as the weight correction variable in the optimization algorithm. The epochs itself are 1000, 500 and the learning rate is 0.001.</span></p>
<p><span class="font1">As comparison to Adam's optimization algorithm, this research also used an old optimization algorithm, namely stochastic gradient descent, which is a faster version of gradient descent. Both optimization algorithms use the same epoch values and learning rate. The testing phase is performed twice. First, after model has completed 1 epoch (learning cycle) and secondly testing as final model validation that performed at the end of the training.</span></p>
<h2><a name="bookmark13"></a><span class="font1" style="font-weight:bold;"><a name="bookmark14"></a>Adaptive Moment Estimation(Adam)</span></h2>
<p><span class="font1">Adam an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. Adam's algorithm was first introduced in 2015 [3]. The method is straightforward to implement, is computationally efficient,</span></p>
<p><span class="font1">has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters.</span></p>
<p><span class="font1">The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients [3]. This method is designed to combine the advantages of two recently popular methods: AdaGrad , which works well with scattered gradients, and RMSProp, which works well in setting on-line and non-stationary [3].</span></p>
<p><span class="font1">Adaptive Gradients (AdaGrad) provides us with a simple approach, for changing the learning rate over time. This is important for adapting to the differences in datasets, since we can get small or large updates, according to how the learning rate is defined [3].</span></p>
<p><span class="font1">Root Mean Squared Propagation (RMSprop) is very close to Adagrad, except for it does not provide the sum of the gradients, but instead an exponentially decaying average. This decaying average is realized through combining the Momentum algorithm and Adagrad algorithm, with a new term [3].</span></p>
<p><span class="font1">An important property of RMSprop is that we are not restricted to just the sum of the past gradients, but instead we are more restricted to gradients for the recent time steps. This means that RMSprop changes the learning rate slower than Adagrad, but still reaps the benefits of converging relatively fast [3].</span></p>
<p><span class="font1">Adam’s weight correction algorithm uses equation 4 5 6 and 7 as follows [5]:</span></p>
<p><span class="font7" style="font-style:italic;">v<sub>t</sub> = β2 * V<sub>t-1</sub></span><span class="font7"> + (1 - β2) * </span><span class="font7" style="font-style:italic;">gt</span></p>
<div>
<p><span class="font7" style="font-style:italic;text-decoration:underline;">V</span><span class="font6" style="font-style:italic;text-decoration:underline;">t</span></p>
<p><span class="font7" style="font-style:italic;">1— P</span><span class="font6" style="font-style:italic;">t</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">υ<sub>t</sub></span><span class="font7"> =</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(4)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(5)</span></p>
<p><span class="font1">(6)</span></p>
<p><span class="font1">(7)</span></p>
</div><br clear="all">
<p><span class="font7" style="font-style:italic;">m<sub>t</sub> = β1* v<sub>t-1</sub></span><span class="font7"> + (1 - </span><span class="font7" style="font-style:italic;">β1)</span><span class="font7"> * </span><span class="font7" style="font-style:italic;">gt<sup>2 </sup>__ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;St</span></p>
<p><span class="font7" style="font-style:italic;"><sup>t</sup></span><span class="font7"> 1 — β<sub>1</sub><sup>t</sup></span></p>
<p><span class="font7" style="font-style:italic;">mτ<sub>t</sub></span></p>
<p><span class="font7" style="font-style:italic;">Awt = — η -=---</span><span class="font7">* </span><span class="font7" style="font-style:italic;">gt</span></p>
<p><span class="font7" style="font-style:italic;">√V</span><span class="font7"> + </span><span class="font7" style="font-style:italic;">e</span></p>
<p><span class="font7" style="font-style:italic;">w<sub>t+1</sub></span><span class="font7"> = </span><span class="font7" style="font-style:italic;">w<sub>t</sub> + Awt</span></p>
<p><span class="font1">Where :</span></p>
<p><span class="font7" style="font-style:italic;">η</span><span class="font1"> = initial learning rate</span></p>
<p><span class="font7" style="font-style:italic;">gt</span><span class="font1"> = gradient at time t (step / epoch) along </span><span class="font7">w<sub>t</sub></span><span class="font1">.</span></p>
<p><span class="font7" style="font-style:italic;">v<sub>t</sub></span><span class="font1" style="font-style:italic;">=</span><span class="font1"> the exponential average of the gradient along </span><span class="font7">w<sub>t</sub></span><span class="font1">.</span></p>
<p><span class="font7" style="font-style:italic;">s<sub>t</sub></span><span class="font1"> = the exponential average of the squares of the slope gradient along </span><span class="font7">w<sub>t</sub></span><span class="font1">.</span></p>
<p><span class="font1">Where in Adam itself there are 2 hyperparameters that can be adjusted based on the needs of training model. The hyperparameters are β1 and β2. By default each parameter has the following values:</span></p>
<p><span class="font7">β1 </span><span class="font1">= 0.9</span></p>
<p><span class="font7">β2 </span><span class="font1">= 0.999</span></p>
<p><span class="font1">Parameter </span><span class="font7">e </span><span class="font1">is a very small value to avoid zero division [5]. </span><span class="font7">e </span><span class="font1">= 10<sup>-</sup></span><span class="font0">8</span></p>
<h2><a name="bookmark15"></a><span class="font1" style="font-weight:bold;"><a name="bookmark16"></a>ReLu</span></h2>
<p><span class="font1">ReLU (Rectified Linear Unit) is one type of activation function that can be used in implementing Neural Networks. The basic principle of ReLU operation is that this function will only convert negative input values to 0 values. Equations and illustrations of this function can be seen in equation 8 and figure 3.</span></p>
<p><span class="font7" style="font-style:italic;">ReLU(x)</span><span class="font7"> = (0,x)</span></p>
<div>
<p><span class="font1">(8)</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/64439-3.jpg" alt="" style="width:250pt;height:147pt;">
<p><span class="font1" style="font-weight:bold;">Figure 3. </span><span class="font1">Graphic Illustration of ReLU Activation Function [2]</span></p>
</div><br clear="all">
<p><span class="font1" style="font-weight:bold;">Softmax</span></p>
<p><span class="font1">Softmax is an activation function at the classification layer to convert the output vector values into probability values from each label class. If it is known that </span><span class="font7">% </span><span class="font1">is the weighted input received by neurons in the softmax layer, then the activation </span><span class="font7" style="font-style:italic;">y<sub>j</sub></span><span class="font1"> for neuron j-th can be seen in equation 9.</span></p>
<p><span class="font7" style="font-style:italic;">c<sup>x</sup>^ softmax(y<sub>i</sub>) = =<sub>r77</sub>-----—</span></p>
<div>
<p><span class="font1">(9)</span></p>
</div><br clear="all">
<p><span class="font7" style="font-variant:small-caps;">∑<sup>n</sup></span><span class="font3" style="font-variant:small-caps;">=i &nbsp;&nbsp;</span><span class="font7" style="font-style:italic;">e<sup>χ</sup></span><span class="font4" style="font-style:italic;">k</span></p>
<p><span class="font1">It can be said that in the Softmax layer, the output is a probability distribution for each class. The denominator ensures that the ith output is close to 1. Using softmax we can interpret the network output </span><span class="font7">y</span><span class="font5">&quot; </span><span class="font1">as an estimate </span><span class="font7" style="font-style:italic;">p(x<sup>n</sup>)</span><span class="font1" style="font-weight:bold;font-style:italic;">.</span></p>
<h2><a name="bookmark17"></a><span class="font1" style="font-weight:bold;"><a name="bookmark18"></a>Cross Entropy Error</span></h2>
<div>
<p><span class="font1">The softmax activation function that is applied to the fully connected layer (fc layer) will be paired with cross entropy. This cross entropy will later be used for calculating the amount of loss or error value from the softmax output against the expected output. The cross entropy error can be calculated using equation 10 as follow :</span></p>
<p><span class="font6" style="font-style:italic;">N</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">Cross enthropy(x^)</span><span class="font7"> = -</span></p>
</div><br clear="all">
<div>
<p><span class="font7">∑ </span><span class="font6" style="font-style:italic;">i=ι</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">p(X</span><span class="font6" style="font-style:italic;">i</span><span class="font7" style="font-style:italic;">) * log(q(x</span><span class="font6" style="font-style:italic;">i</span><span class="font7" style="font-style:italic;">))</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(10)</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark19"></a><span class="font1" style="font-weight:bold;"><a name="bookmark20"></a>2.3. &nbsp;&nbsp;&nbsp;GLCM</span></h2></li></ul>
<p><span class="font1">GLCM is one of the feature extraction methods to obtain feature values by calculating the probability value of the co-ocurrence matrix from the adjacency relationship between two pixels at a certain distance and angular orientation [6].</span></p>
<p><span class="font1">The direction and distance of the GLCM can be seen in Figure 4. Figure 5 shows the process of calculating the GLCM co-ocurrence matrix [6].</span></p>
<div><img src="https://jurnal.harianregional.com/media/64439-4.jpg" alt="" style="width:120pt;height:96pt;">
<p><span class="font1" style="font-weight:bold;">Figure 4. </span><span class="font1">GLCM Angle Illustration [6]</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/64439-5.jpg" alt="" style="width:216pt;height:135pt;">
<p><span class="font1" style="font-weight:bold;">Figure 5. </span><span class="font1">Illustration of calculating co-ocurrence matrix [6]</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font1">16 inputs of ANN are derived from the four (4) directions adjacency matrix of cooccurrence matrix of GLCM which later for each direction will have four (4) image features extracted using equations 11, 12, 13, and 14. Foreach image, there are number of directions multiple by number of features that become as input, 16 inputs in this case.</span></p></li></ul>
<p><span class="font1">The statistical features of GLCM extracted from grayscale images in this research are as follows[6] :</span></p>
<div>
<p><span class="font1">1.</span></p>
</div><br clear="all">
<p><span class="font1">Entropy</span></p>
<p><span class="font1">Entropy is used to measure the randomness of the intensity distribution [6]. Entropy</span></p>
<p><span class="font1">Equation [7]:</span></p>
<p><span class="font7" style="font-style:italic;">Ent - -∑</span><span class="font6" style="font-style:italic;">m</span><span class="font7" style="font-style:italic;">∑</span><span class="font6" style="font-style:italic;">∕</span><span class="font7" style="font-style:italic;"><sup>l</sup>p(i,j)log{p(i,j)}</span></p>
<div>
<p><span class="font1">(11)</span></p>
</div><br clear="all">
<div>
<ul style="list-style:none;"><li>
<p><span class="font1">2.</span></p></li></ul>
</div><br clear="all">
<div>
<p><span class="font1">Energy</span></p>
<p><span class="font1">Energy is a feature of GLCM which is used to measure the concentration of intensity pairs on the GLCM matrix [6], and is defined as follows. Energy Equation [7]:</span></p>
</div><br clear="all">
<div>
<ul style="list-style:none;"><li>
<p><span class="font1">3.</span></p></li></ul>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">TT-</span><span class="font1"> Γ mm </span><span class="font7" style="font-style:italic;"><sup>p</sup>(<sup>i,j</sup>) </span><span class="font8" style="font-style:italic;font-variant:small-caps;">&quot;<sup>-</sup>∑<sup>,</sup>∑<sup>i</sup></span><span class="font7" style="font-style:italic;"> ι + (i-j</span><span class="font6" style="font-style:italic;">2</span><span class="font7" style="font-style:italic;">)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(12)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Homogeneity</span></p>
<p><span class="font1">Shows the homogeneity of the intensity variation in the image [6]. The Homogeneity equation is as follows [7]:</span></p>
</div><br clear="all">
<p><span class="font7">fi = </span><span class="font7" style="font-style:italic;">∑</span><span class="font6" style="font-style:italic;">m</span><span class="font7" style="font-style:italic;">∑∕<sup>l</sup>p(i,j)<sup>2</sup></span></p>
<div>
<p><span class="font1">(13)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">4.</span></p>
</div><br clear="all">
<p><span class="font1">Contras</span></p>
<p><span class="font1">Contrast is the calculation of the difference in intensity between one pixel and adjacent pixels throughout the image [6]. Zero contrast for a constant image. Contrast Equation as follows [7]:</span></p>
<div>
<p><span class="font1">(14)</span></p>
</div><br clear="all">
<p><span class="font7" style="font-style:italic;">C2- ∑</span><span class="font6" style="font-style:italic;">m</span><span class="font7" style="font-style:italic;">∑</span><span class="font6" style="font-style:italic;">=o Paj)</span><span class="font7" style="font-style:italic;">(i-J)<sup>2</sup></span></p>
<p><span class="font1">Where [7]:</span></p>
<p><span class="font1">P = normalized GLCM matrix i = row index matrix P j = column index matrix P</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font1" style="font-weight:bold;"><a name="bookmark22"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark23"></a>3.1. &nbsp;&nbsp;&nbsp;Data and Testing Mechanism</span></h2></li></ul>
<p><span class="font1">As previously written in research method. We use 4 types(later we will be written as classes) of endek cloth with different patterns, such as endek cemplong, sringsing, sekar dan wajik(diamond). Each class contain approximatey 60 images with a resolution of 40 x 40 pixels.</span></p>
<p><span class="font1">Before splitting data into 2 categories, we convert these images into grayscale and later on we extract its 4 statistical features from each grayscale version of the image. After we got these features data,The data will be divided into training data and test data where the training data is 80% of the total data and test data is 20% of the total data.</span></p>
<p><span class="font1">This research will evaluate the neural network model twice, which one is performed after each leaning cycle or epoch later we will include graph to illustrate accuracy and loss for each epoch and second is performed after learning step finished to final test the model which has been formed. These two testing use same data which is 20% of the total data.</span></p>
<p><span class="font1">As previously written, each optimizer algorithm will be tested by using several epoch and same learning rate that is for epoch are 1000 and 500 and for learning rate is 0.001. Later we will note and compare accuracy and loss value generated by these variables to find which one has the highest accuracy.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark24"></a><span class="font1" style="font-weight:bold;"><a name="bookmark25"></a>3.2. &nbsp;&nbsp;&nbsp;Adam Optimizer</span></h2></li></ul><img src="https://jurnal.harianregional.com/media/64439-6.jpg" alt="" style="width:368pt;height:360pt;">
<p><span class="font1" style="font-weight:bold;">Figure 6. </span><span class="font1">Epoch 500 and 0.001 learning rate</span></p>
<p><span class="font1">You can see the training and testing accuracy graph in Figure 6. Accuracy in the learning process is in the range of 20% to 37.5%. While at the testing stage, the model accuracy for each epoch is in the range of 15% to 40%. After completing the entire training process and entering the second testing phase for final validation of the model that has been formed. The final model accuracy is 33%.</span></p>
<p><span class="font1">As seen in the loss value graph, both the training and testing graph in Figure 6, there is a tendency for the loss value to fluctuate but not significantly. This indicates that the Adam optimization algorithm does not experience a situation where it stucks in a local minima when finding the lowest point or minimum loss value which is called the global minima of the objective function.</span></p><img src="https://jurnal.harianregional.com/media/64439-7.jpg" alt="" style="width:380pt;height:366pt;">
<p><span class="font1" style="font-weight:bold;">Figure 7. </span><span class="font1">Epoch 1000 and 0.001 learning rate</span></p>
<p><span class="font1">You can see the training and testing accuracy graph in figure 7. Accuracy in the learning process is in the range of 23% to 45%. While at the testing stage, the model accuracy for each epoch is in the range of 10% to 53%. After completing the entire training process and entering the second testing phase for final validation of the model that has been formed. The final model accuracy is 28%.</span></p>
<p><span class="font1">As seen in the loss value graph, both the training and testing graph in Figure 6, there is a tendency for the loss value to fluctuate but not significantly. This indicates that the Adam optimization algorithm does not experience a situation where it stucks in a local minima when finding the lowest point or minimum loss value which is called the global minima of the objective function.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark26"></a><span class="font1" style="font-weight:bold;"><a name="bookmark27"></a>3.3. &nbsp;&nbsp;&nbsp;SGD Or Stochastic Gradient Descent Optimizer</span></h2></li></ul>
<p><span class="font1">It can be seen in the 4 graphs shown in Figure 8. Accuracy in the learning process at the initial epoch is in the range 19% to 26%, then the accuracy looks quite constant around 25.5% to 26%. While at the testing stage, the accuracy of the model for the initial epoch is in the range of 21%, then it looks constant at 23.25%. After completing the entire training process and entering the second testing phase for final validation of the model that has been formed. We got a final model accuracy of 23%.</span></p>
<p><span class="font1">As seen from the loss graph in each epoch both at the training and testing stages. SGD has a tendency that changes in the loss values are insignificant (illustration on the testing loss graph) and even flat (illustration on the training loss graph). Of course this can be caused by two possible circumstances, namely, the model has reached the global minima or the model is stuck</span></p>
<p><span class="font1">in a local minima without, previously reaching the global minima. Look at figure 10 to get an illustration of the local minima and global minima.</span></p><img src="https://jurnal.harianregional.com/media/64439-8.jpg" alt="" style="width:374pt;height:358pt;">
<p><span class="font1" style="font-weight:bold;">Figure 8. </span><span class="font1">Epoch 500 and 0.001 learning rate</span></p>
<p><span class="font1">It can be seen in the 4 graphs shown in Figure 9. Accuracy in the learning process at the initial epoch is in the range 20% to 26%, then the accuracy looks quite constant at 26%. While at the testing stage, the accuracy of the model for the initial epoch is as high as 28%, then it looks dropping down and constant at 21.5%. After completing the entire training process and entering the second testing phase for final validation of the model that has been formed. We got a final model accuracy of 21%.</span></p>
<p><span class="font1">As seen from the loss graph in each epoch both at the training and testing stages. SGD has a tendency that changes in the loss values are insignificant (illustration on the testing loss graph) and even flat (illustration on the training loss graph). Of course this can be caused by two possible circumstances, namely, the model has reached the global minima or the model is stuck in a local minima without, previously reaching the global minima. Look at figure 10 to get an illustration of the local minima and global minima.</span></p><img src="https://jurnal.harianregional.com/media/64439-9.jpg" alt="" style="width:372pt;height:356pt;">
<p><span class="font1" style="font-weight:bold;">Figure 9. </span><span class="font1">Epoch 1000 and 0.001 learning rate</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark28"></a><span class="font1" style="font-weight:bold;"><a name="bookmark29"></a>3.4. &nbsp;&nbsp;&nbsp;Result</span></h2></li></ul><img src="https://jurnal.harianregional.com/media/64439-10.jpg" alt="" style="width:219pt;height:141pt;">
<p><span class="font1" style="font-weight:bold;">Figure 10. </span><span class="font1">Ilustration of Local Minima and Global Minima</span></p>
<p><span class="font1">In Figures 7 and 8, it can be seen that Adam's algorithm has a tendency to change the loss value up and down but not so significant. This is due to the advantages of Adam itself which can move from each local minima without experiencing any stuck or trap conditions. In accordance with the advantages of the Adam algorithm as previously written, this advantage is obtained from the elements of the Adagrad algorithm which are arranged in the Adam algorithm.</span></p>
<p><span class="font1">In Figures 9 and 10, it can be seen that the SGD algorithm, unlike Adam's algorithm, has a tendency to change the loss value which is thin and insignificant. This is due to the possibility</span></p>
<p><span class="font1">that the model is stuck on a local minima with a poor level of accuracy. This situation causes almost constant accuracy at the training and testing stages of the SGD.</span></p>
<p><span class="font1">Final accuracy result of each algorithm with different epochs are given in table 2.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 2. </span><span class="font1">Model Accuracy</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font1">Number of Epoch</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Stochastic Gradient Descent Learning Rate 0.001</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Adam (Adaptive Moment Estimation) Learning Rate 0.001</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">1000</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">21 %</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">28 %</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">500</span></p></td><td style="vertical-align:top;">
<p><span class="font1">23 %</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33 %</span></p></td></tr>
</table>
<p><span class="font1">Shorter epoch also generates better model performance for both optimizer algorithm. Adam algorithm generates higher accuracy than the accuracy that is generated by the algorithm SGD. This can be proven from the training accuracy graph, the first testing phase accuracy graph and the final accuracy of the model. The poor SGD accuracy results from being stuck in a local minima with low accuracy. Meanwhile, Adam's better accuracy is indirectly related to the nature of the algorithm itself which does not experience the problem of being trapped or stuck in cases with scattered gradients.</span></p>
<p><span class="font1">Other thing that get our concern is both SGD and Adam algorithm give terrible accuracy. So that we use another classification algorithm to classify and evaluate our training and testing data. We used KNN classification algorithm to test our data. Unlike artificial neural network, KNN give a little bit better accuracy which is 45%. But still, both classification algorithm give accuracy which is below 50%. This result can be categorized as poor performance. We can conclude that GLCM has difficulty to generates good feature data for classification of endek cloth case.</span></p>
<p><span class="font1">This terrible performance can be occurred because of each variance of endek cloth has almost similar GLCM texture value when compare to the others. Unlike GLCM, deep learning can extract or detect edge inside any images. As proven in previous research CNN which gave 80% average accuracy. This edge cannot be detected using GLCM. Edges is a strong feature when classify images cases based on pattern. As written above, endek variances are differentiated using pattern.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark30"></a><span class="font1" style="font-weight:bold;"><a name="bookmark31"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font1">Based on accuracies of each model which had been formed. Using 500 as number of epoch or training cyclies, generates better model performance for both optimizer algorithm. Adam optimization algorithm performs better than SGD optimization algorithm in this classification case. As given accuracies from graph figures and both final testing phase, SGD algorithm gives 23% and Adam algorithm gives 33%. Secondly, the GLCM feature extraction method used to extract features from the endek images gives poor result of features data when used for classification. This can be seen from the accuracy of the models from two algorithms are below 50%. This poor performance can be occurred because, one, each variance of endek cloth has almost similar GLCM texture value when compared to the others. Secondly, GLCM cannot detect edge feature which is a strong feature in pattern based image classification. Based on these results, we strongly advice for the next research to use extraction algorithm based on edge detection or use one of deep learning architecture and utilize advance optimizer algorithm to achieve better performance.</span></p>
<h2><a name="bookmark32"></a><span class="font1" style="font-weight:bold;"><a name="bookmark33"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;I. G. S. Rahayuda, “Texture Analysis on Image Motif of Endek Bali using K-Nearest Neighbor Classification Method”, </span><span class="font1" style="font-style:italic;">(IJACSA) International Journal of Advanced Computer Science and Applications</span><span class="font1">, vol. 6, no. 9, 205-211, 2015.</span></p></li>
<li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;I. K. R. Dewantara, “Implementasi Deep Learning Menggunakan Convolutional Neural Network (CNN) Untuk Identifikasi Jenis Kain Endek Bali”, </span><span class="font1" style="font-style:italic;">Skripsi Jurusan Ilmu Komputer Fakultas Matematika dan Ilmu Pengetahuan Alam. Bukit Jimbaran</span><span class="font1">, pp., pp., 2019.</span></p></li>
<li>
<p><span class="font1">[3] &nbsp;&nbsp;&nbsp;D. P. Kingma and J. Lei Ba, &quot;ADAM: A METHOD FOR STOCHASTIC</span></p></li></ul>
<p><span class="font1">OPTIMIZATION&quot;, </span><span class="font1" style="font-style:italic;">Published as a conference paper at ICLR 2015</span><span class="font1">, pp, pp. 1-15, 2015.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[4] &nbsp;&nbsp;&nbsp;M. S. Wibawa, “Pengaruh Fungsi Aktivasi, Optimisasi dan Jumlah Epoch Terhadap Performa Jaringan Saraf Tiruan,” </span><span class="font1" style="font-style:italic;">JURNAL SISTEM DAN INFORMATIKA</span><span class="font1"> , Vol. 11, No. 1, 1-8, 2016.</span></p></li>
<li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;S Bock, J. Goppold and M. Wei, . “An improvement of the convergence proof of the ADAM-Optimizer”, </span><span class="font1" style="font-style:italic;">Conference Paper At OTH Cluster Konferenz, pp. pp</span><span class="font1">. 2018.</span></p></li>
<li>
<p><span class="font1">[6] &nbsp;&nbsp;&nbsp;N. Zulpe and V. Pawar, &quot;GLCM Textural Features for Brain Tumor Classification&quot;, </span><span class="font1" style="font-style:italic;">IJCSI International Journal of Computer Science Issues</span><span class="font1">, vol. 9, no. 3, pp. 354-359, 2012.</span></p></li>
<li>
<p><span class="font1">[7] &nbsp;&nbsp;&nbsp;</span><a href="http://www.mathworks.com/help/images/gray-level-co-occurrencematrix-glcm.html"><span class="font1">http://www.mathworks.com/help/images/gray-level-co-occurrencematrix-glcm.html</span></a><span class="font1">. Diunduh 15 Agustus 2020.</span></p></li></ul>
<p><span class="font1">296</span></p>