---
layout: full_article
title: "Pemodelan Topik Teks Berita Menggunakan DistilBERT"
author: "I Made Anditya Mahesastra, I Dewa Made Bayu Atmaja Darmawan"
categories: jnatia
canonical_url: https://jurnal.harianregional.com/jnatia/full-92840 
citation_abstract_html_url: "https://jurnal.harianregional.com/jnatia/id-92840"
citation_pdf_url: "https://jurnal.harianregional.com/jnatia/full-92840"  
comments: true
---

<p><span class="font2">JNATIA Volume 1, Nomor 1, November 2022</span></p>
<p><span class="font2">Jurnal Nasional Teknologi Informasi dan Aplikasinya</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>Pemodelan Topik Teks Berita Menggunakan DistilBERT</span></h1>
<p><span class="font2">I Made Anditya Mahesastra<sup>a1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">I Dewa Made Bayu Atmaja Darmawan<sup>a2</sup></span></p>
<p><span class="font2"><sup>a</sup>Program Studi Informatika, Universitas Udayana</span></p>
<p><span class="font2">Jimbaran, Badung, Bali, Indonesia </span><a href="mailto:1andimahesastra@email.com"><span class="font2"><sup>1</sup>andimahesastra@email.com</span></a><span class="font2"> </span><a href="mailto:2dewabayu@unud.ac.id"><span class="font2"><sup>2</sup>dewabayu@unud.ac.id</span></a></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font2" style="font-style:italic;">Online newspapers are content that can be a source of information or entertainment for the audience. There are so many online newspapers on the internet and also from various publishers. This condition causes the available news to be very varied and with different structures. If in certain cases we want to group these online newspapers efficiently, then a technique will be needed that will be able to group these online newspapers efficiently into several groups so that the available online newspapers can be more structured to be enjoyed according to the needs and tastes of the readers. The technique that can be applied is topic modeling. In the case of modeling the topic of Indonesian online newspapers, currently LDA is one of the most widely applied algorithms. So, this study aims to determine whether the performance of using the DistilBERT model will be better or not when compared to commonly used algorithms such as LDA for topic modeling tasks in Indonesian online newspapers.</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font2" style="font-style:italic;">Natural Language Processing, Topic Modeling, DistilBERT, HDBSCAN, UMAP</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font2">Berita yang dapat dibaca secara daring menjadi salah satu konten yang menjadi sumber informasi dan hiburan bagi para penikmatnya, dengan saking banyaknya berita yang ada in internet dan dari penerbit yang beragam, tentunya akan diperlukan sebuah teknik yang akan dapat mengelompokkan berita tersebut secara efisien agar penyuguhan berita dapat menjadi lebih terstruktur untuk dapat dinikmati oleh pembaca sesuai dengan kebutuhan dan selera pembaca. Hal tersebut masuk ke dalam ranah pemrosesan bahasa alami, dalam pemrosesan bahasa alami ada yang disebut dengan pemodelan topik. Sampai saat ini, algoritma LDA masih menjadi algoritma yang paling sering digunakan dalam pemodelan topik [1], khususnya dalam kasus pemodelan topik berita berbahasa Indonesia. Saat ini pula, </span><span class="font2" style="font-style:italic;">Bidirectional Encoder Representation from Transformer</span><span class="font2"> (BERT) menjadi </span><span class="font2" style="font-style:italic;">state-of-the-art language models</span><span class="font2"> dikarenakan performanya yang terbukti unggul dalam berbagai kasus pemrosesan Bahasa alami [2]. Penelitian mengenai penggunaan model BERT dalam melakukan tugas pemodelan topik pada berita berbahasa Indonesia akan sangat berguna untuk mengetahui performa yang di dapat apakah dapat bersaing dengan algoritma yang lebih umum digunakan seperti LDA. Oleh karena itu, penelitian ini bertujuan untuk mencari fakta terkait hal tersebut. Hasil penelitian ini diharapkan dapat menjadi salah satu acuan pemilihan metode dalam kasus pemrosesan bahasa alami lainnya.</span></p>
<p><span class="font2">Ada pula beberapa penelitian lain yang meneliti kasus yang serupa dengan penelitian ini, contohnya [3], yaitu pemodelan topik pada teks berita berbahasa Indonesia yang datanya dikumpulkan mulai dari tanggal 10 sampai 23 April 2020 sebanyak 59.279 data, penelitian ini menggunakan metode LDA dengan nilai </span><span class="font2" style="font-style:italic;">topic coherence</span><span class="font2"> sekitar 0.2. Penelitian serupa lainnya adalah pemodelan teks berita berbahasa Indonesia yang dikumpulkan mulai dari tanggal 1 Oktober 2019 hingga 31 Maret 2020 sebanyak 68.537 data, penelitian ini menggunakan metode LDA dengan nilai </span><span class="font2" style="font-style:italic;">topic coherence</span><span class="font2"> sebesar 0.67 [4]. Penelitian serupa selanjutnya adalah analisis sentimen pada judul teks berita berbahasa Indonesia, penelitian ini menggunakan metode LDA dan LSTM dengan nilai akurasi sebesar 71.13% [5].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font2" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Reseach Methods</span></h2></li></ul>
<p><span class="font2">Penelitian ini diawali dengan tahap penumpulan dan pembersihan data, data dikumpulkan dengan menggunakan </span><span class="font2" style="font-style:italic;">web scraper</span><span class="font2"> berbasis pustaka Selenium. Tahap kedua adalah membuat </span><span class="font2" style="font-style:italic;">document embedding</span><span class="font2"> dari data yang telah tersedia untuk mendapatkan representasi dokumen dalam ruang vektor yang dilakukan menggunakan </span><span class="font2" style="font-style:italic;">pre-trained embedding model</span><span class="font2"> yaitu DistilBERT. Tahap ketiga adalah melakukan </span><span class="font2" style="font-style:italic;">dimensionality reduction</span><span class="font2"> yang dilakukan menggunakan UMAP untuk meningkatkan kinerja algoritma pengelompokan. Tahap keempat adalah melakukan </span><span class="font2" style="font-style:italic;">document clustering</span><span class="font2"> untuk mengelompokkan dokumen yang serupa secara semantik yang dilakukan menggunakan HDBSCAN. Tahap kelima adalah </span><span class="font2" style="font-style:italic;">keyword extraction </span><span class="font2">yang dilakukan menggunakan c-TF-IDF untuk mencari representasi setiap kelompok/topik yang dihasilkan algoritma pengelompokan. Terakhir adalah tahap evaluasi metode menggunakan metrik </span><span class="font2" style="font-style:italic;">topic coherence</span><span class="font2"> untuk menilai seberapa baik suatu topik didukung oleh korpus referensi. Gambaran alur penelitian ditunjukan pada Gambar 1.</span></p><img src="https://jurnal.harianregional.com/media/92840-1.jpg" alt="" style="width:426pt;height:31pt;">
<p><span class="font2" style="font-weight:bold;">Gambar 1. </span><span class="font2">Alur Penelitian</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font2" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Data Gathering</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Dataset</span><span class="font2"> pada penelitian ini bersifat primer. </span><span class="font2" style="font-style:italic;">Dataset</span><span class="font2"> yang digunakan adalah kumpulan teks berita berbahasa Indonesia. </span><span class="font2" style="font-style:italic;">Dataset</span><span class="font2"> pada penelitian ini diambil dari 26 portal berita berbahasa Indonesia, di antaranya adalah Tribunnews.com, Detik.com, Kompas.com, Liputan6.com, Merdeka.com, Kapanlagi.com, Okezone.com, Tempo.co, Viva.co.id, Suara.com, JPNN.com, Sindonews.com, CNN Indonesia, CNBC Indonesia, Republika, IDN Times, TvOne News, ANTARA News, Portal Pekalongan, Tirto.ID, BBC Indonesia, Brilio.net, iNews.id, Kompasiana.com, dream.co.id, WowKeren.com. Data ini diambil dengan menggunakan </span><span class="font2" style="font-style:italic;">web scraper</span><span class="font2"> berbasis pustaka Selenium. Proses pengambilan data dilakukan mulai dari tanggal 7 Mei 2022 sampai tanggal 18 Agustus 2022. Selama periode tersebut data yang telah terkumpul mencapai 6598 data.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font2" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Document Embedding</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Document embedding</span><span class="font2"> diperlukan sebagai langkah untuk mendapatkan representasi dokumen dalam ruang vektor agar dokumen satu dengan dokumen lainnya dapat dibandingkan secara semantik [6], dengan asumsi dokumen yang memiliki topik yang sama juga akan serupa secara semantik [7]. Dalam melakukan </span><span class="font2" style="font-style:italic;">document embedding</span><span class="font2">, penelitian ini menggunakan model DistilBERT [8]. DistilBERT adalah model </span><span class="font2" style="font-style:italic;">embedding</span><span class="font2"> yang telah dilatih sebelumnya, model tersebut digunakan untuk memberikan representasi dokumen dalam ruang vektor. Performa sangat baik di tunjukan oleh model DistilBert pada saat diuji dalam beberapa tugas seperti menjawab pertanyaan menggunakan </span><span class="font2" style="font-style:italic;">Stanford Question Answering Dataset</span><span class="font2"> (SQuAD), dan analisis sentimen menggunakan </span><span class="font2" style="font-style:italic;">Internet Movie Database</span><span class="font2"> (IMDb) [8]. </span><span class="font2" style="font-style:italic;">Document embedding </span><span class="font2">adalah langkah yang membedakan proses pemodelan topik lainnya yang menggunakan </span><span class="font2" style="font-style:italic;">Bag of Words</span><span class="font2"> (BOW) seperti LDA atau LSA [9]. </span><span class="font2" style="font-style:italic;">Document embedding</span><span class="font2"> kemudian digunakan untuk mengelompokan dokumen yang serupa secara semantik. Teknik </span><span class="font2" style="font-style:italic;">embedding</span><span class="font2"> lainnya juga dapat diterapkan pada tahap ini.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font2" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Dimensionality Reduction</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Dimensionality reduction</span><span class="font2"> diperlukan sebagai langkah untuk mengurangi dimensi dari </span><span class="font2" style="font-style:italic;">document embedding</span><span class="font2">. Dalam melakukan </span><span class="font2" style="font-style:italic;">dimensionality reduction</span><span class="font2">, penelitian ini menggunakan UMAP [10]. Penerapan UMAP dalam melakukan dimensionality reduction terbukti dapat meningkatkan kinerja algoritma pengelompokan seperti k-Means dan HDBSCAN baik dari segi akurasi maupun waktu pemrosesan [11]. UMAP dapat digunakan pada seluruh model </span><span class="font2" style="font-style:italic;">embedding</span><span class="font2"> dengan ruang dimensi yang berbeda dikarenakan tidak memiliki batasan komputasi pada dimensi </span><span class="font2" style="font-style:italic;">embedding </span><span class="font2">[10].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font2" style="font-weight:bold;"><a name="bookmark13"></a>2.4. &nbsp;&nbsp;&nbsp;Document Clustering</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Document clustering</span><span class="font2"> dilakukan untuk mengelompokkan dokumen yang serupa secara semantik yang selanjutnya kumpulan dokumen yang di kelompokan akan di proses lebih lanjut untuk mencari kumpulan kata kunci yang berperan penting pada setiap kelompok dokumen. Dalam melakukan </span><span class="font2" style="font-style:italic;">document clustering</span><span class="font2">, penelitian ini menggunakan </span><span class="font2" style="font-style:italic;">Hierarchical Density Based Clustering</span><span class="font2"> (HDBSCAN) [12]. HDBSCAN memodelkan kelompok menggunakan pendekatan </span><span class="font2" style="font-style:italic;">soft-clustering</span><span class="font2"> yang memungkinkan </span><span class="font2" style="font-style:italic;">noise</span><span class="font2"> dimodelkan sebagai </span><span class="font2" style="font-style:italic;">outlier</span><span class="font2"> untuk mencegah dokumen yang tidak terkait dikelompokan kedalam kelompok manapun sehingga akan dapat meningkatkan representasi topik [13]. Selain itu, penerapan HDBSCAN dalam melakukan pengelompokan setelah penerapan UMAP dalam mengurangi dimensi </span><span class="font2" style="font-style:italic;">embedding</span><span class="font2"> pada data terbukti dapat meningkatkan akurasi [11].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font2" style="font-weight:bold;"><a name="bookmark15"></a>2.5. &nbsp;&nbsp;&nbsp;Keyword Extraction</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Keyword extraction</span><span class="font2"> dilakukan untuk mencari representasi setiap kelompok/topik yang dihasilkan algoritma pengelompokan. Representasi dari suatu kelompok dimodelkan berdasarkan kumpulan dokumen penyusun kelompok tersebut. Setiap kelompok harus memiliki alasan mengapa kumpulan dokumen dapat di kelompokan ke dalam kelompok yang satu dan bukan kelompok lainnya. Dalam penelitian ini setiap kelompok akan di representasikan sebagai kumpulan kata kunci, kumpulan kata kunci tersebut di dapat dengan menggunakan algoritma </span><span class="font2" style="font-style:italic;">class-based</span><span class="font2"> TF-IDF untuk menentukan seberapa relevan kumpulan kata tersebut dengan kelompok dokumen tertentu. Prosedur TF-IDF klasik menggabungkan dua statistik, yaitu </span><span class="font2" style="font-style:italic;">term frequency</span><span class="font2"> (TF) dan </span><span class="font2" style="font-style:italic;">inverse document frequency</span><span class="font2"> (IDF), dimana </span><span class="font2" style="font-style:italic;">term frequency</span><span class="font2"> mengukur frekuensi </span><span class="font2" style="font-style:italic;">term</span><span class="font2"> t dalam dokumen d, dan </span><span class="font2" style="font-style:italic;">Inverse document frequency</span><span class="font2"> mengukur seberapa banyak informasi yang diberikan suatu </span><span class="font2" style="font-style:italic;">term</span><span class="font2"> ke dokumen [14]. Dalam penelitian ini c-TF-IDF digunakan dengan maksud memperlakukan satu kelompok dokumen sebagai satu buah dokumen, di mana algoritma TF-IDF diterapkan untuk mendapatkan kumpulan kata kunci yang relevan terhadap satu kelompok dokumen.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font2" style="font-weight:bold;"><a name="bookmark17"></a>2.6. &nbsp;&nbsp;&nbsp;Topic Coherence Measurement</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Topic Coherence</span><span class="font2"> adalah metrik untuk menilai seberapa baik suatu topik (dalam hal ini adalah kumpulan kata kunci) didukung oleh kumpulan teks (korpus referensi). Perhitungan koherensi topik menggunakan statistik dan probabilitas untuk memberikan skor koherensi pada suatu topik dengan kisaran nilai 0 sampai 1, di mana nilai 0 berarti topik tidak relevan, sedangkan nilai 1 berarti sebaliknya. Nilai koherensi topik tidak hanya tergantung pada topik itu sendiri tetapi juga pada kumpulan data yang digunakan sebagai referensi (korpus referensi). </span><span class="font2" style="font-style:italic;">Topic coherence </span><span class="font2">adalah suatu </span><span class="font2" style="font-style:italic;">pipeline</span><span class="font2"> yang menerima topik dan korpus referensi sebagai </span><span class="font2" style="font-style:italic;">input</span><span class="font2"> kemudian mengeluarkan nilai koherensi dari topik tersebut sebagai </span><span class="font2" style="font-style:italic;">output</span><span class="font2">. Rincian dari </span><span class="font2" style="font-style:italic;">pipeline</span><span class="font2"> tersebut di antaranya </span><span class="font2" style="font-style:italic;">segmentation</span><span class="font2">, </span><span class="font2" style="font-style:italic;">probability calculation</span><span class="font2">, </span><span class="font2" style="font-style:italic;">confirmation measure</span><span class="font2">, dan </span><span class="font2" style="font-style:italic;">aggregation</span><span class="font2">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a.</span><span class="font2" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Segmentation</span><span class="font2"> berperan dalam membuat pasangan kata yang akan digunakan untuk menilai koherensi topik, berikut adalah rumus untuk mencari </span><span class="font2" style="font-style:italic;">segmentation</span></p></li></ul>
<p><span class="font7">S(W) = </span><span class="font7" style="font-style:italic;">{(W</span><span class="font4" style="font-style:italic;">'</span><span class="font7" style="font-style:italic;">, W</span><span class="font4" style="font-style:italic;">*</span><span class="font7" style="font-style:italic;">), W</span><span class="font6">'</span><span class="font7">, </span><span class="font7" style="font-style:italic;">W</span><span class="font4" style="font-style:italic;">* </span><span class="font5" style="font-style:italic;">∈</span><span class="font7" style="font-style:italic;">W}</span><span class="font2"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">b.</span><span class="font2" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Probability calculation</span><span class="font2"> berperan dalam menghitung probabilitas dari tiap pasangan kata, penelitian ini menggunakan </span><span class="font7" style="font-style:italic;">Pbd</span><span class="font2" style="font-style:italic;">. </span><span class="font7" style="font-style:italic;">Pbd</span><span class="font2"> menghitung </span><span class="font7" style="font-style:italic;">P(W</span><span class="font4" style="font-style:italic;">'</span><span class="font7" style="font-style:italic;">)</span><span class="font2"> sebagai jumlah dokumen yang berisi kata </span><span class="font7" style="font-style:italic;">W</span><span class="font4" style="font-style:italic;">'</span><span class="font2"> dibagi dengan jumlah dokumen yang ada, dan </span><span class="font7" style="font-style:italic;">P(W</span><span class="font6">'</span><span class="font7">W</span><span class="font6">*</span><span class="font7">) </span><span class="font2">sebagai jumlah dokumen yang berisi kata </span><span class="font7" style="font-style:italic;">W</span><span class="font4" style="font-style:italic;">'</span><span class="font2"> dan </span><span class="font7" style="font-style:italic;">W</span><span class="font4" style="font-style:italic;">*</span><span class="font2"> dibagi dengan jumlah dokumen yang ada</span></p></li>
<li>
<p><span class="font2">c.</span><span class="font2" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Confirmation measure</span><span class="font2"> berperan dalam menghitung nilai konfirmasi dengan menggunakan </span><span class="font7" style="font-style:italic;">P(W</span><span class="font4" style="font-style:italic;">'</span><span class="font7" style="font-style:italic;">)</span><span class="font2"> dan </span><span class="font7" style="font-style:italic;">P(W</span><span class="font4" style="font-style:italic;">'</span><span class="font7" style="font-style:italic;">W</span><span class="font4" style="font-style:italic;">*</span><span class="font7" style="font-style:italic;">)</span><span class="font2">. Terdapat beberapa rumus dalam menghitung nilai konfirmasi. Penelitian ini menggunakan rumus berikut:</span></p></li></ul>
<p><span class="font7" style="font-style:italic;">m<sub>c</sub>(S<sub>i</sub>)</span><span class="font7"> = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font2">(2)</span></p>
<p><span class="font2"><sup>cv </sup></span><span class="font4" style="font-style:italic;"><sup>lj</sup> P(W )</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">d.</span><span class="font2" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Aggregation</span><span class="font2"> adalah nilai rata-rata dari seluruh nilai yang didapat pada tiap </span><span class="font2" style="font-style:italic;">segment </span><span class="font2">Keterangan:</span></p></li></ul>
<p><span class="font7">S(W) </span><span class="font2">: Himpunan pasangan kata dari himpunan kata</span></p>
<p><span class="font7">W </span><span class="font2">: Himpunan Kata</span></p>
<p><span class="font7">W</span><span class="font6">' </span><span class="font2">: kata 1</span></p>
<p><span class="font7" style="font-style:italic;">W</span><span class="font4" style="font-style:italic;">*</span><span class="font2"> : kata 2</span></p>
<p><span class="font7" style="font-style:italic;">m<sub>c</sub>(S<sub>i</sub></span><span class="font7">) </span><span class="font2">: confirmation measure dari </span><span class="font7" style="font-style:italic;">(W </span><span class="font4" style="font-style:italic;">'</span><span class="font7" style="font-style:italic;">,W</span><span class="font4" style="font-style:italic;">*</span><span class="font7" style="font-style:italic;">)</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font2" style="font-weight:bold;"><a name="bookmark19"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span><br><br><span class="font2" style="font-weight:bold;"><a name="bookmark20"></a>3.1. &nbsp;&nbsp;&nbsp;Data Gathering</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Dataset</span><span class="font2"> pada penelitian ini diambil dari 26 portal berita berbahasa Indonesia, di antaranya adalah Tribunnews.com, Detik.com, Kompas.com, Liputan6.com, Merdeka.com, Kapanlagi.com, Okezone.com, Tempo.co, Viva.co.id, Suara.com, JPNN.com, Sindonews.com, CNN Indonesia, CNBC Indonesia, Republika, IDN Times, TvOne News, ANTARA News, Portal Pekalongan, Tirto.ID, BBC Indonesia, Brilio.net, iNews.id, Kompasiana.com, dream.co.id, WowKeren.com dengan rincian yang ditunjukkan pada Tabel 1.</span></p>
<p><span class="font2" style="font-weight:bold;">Tabel 1. </span><span class="font2">Rincian Data</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Penerbit</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Jumlah Berita</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">CNN Indonesia</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">513</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Sindonews.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">498</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Detik.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">483</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">TvOne News</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">455</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">ANTARA News</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">430</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Kompas.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">388</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">CNBC Indonesia</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">378</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Republika</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">347</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Tribunnews.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">301</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Tempo.co</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">289</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Okezone.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">243</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Merdeka.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">243</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">JPNN.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">233</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Suara.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">231</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">dream.co.id</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">182</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">BBC Indonesia</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">165</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Liputan6.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">149</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">iNews.id</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">149</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Tirto.ID</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">148</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Portal Pekalongan</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">140</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">WowKeren.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">137</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Viva.co.id</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">133</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Kompasiana.com</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">131</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Kapanlagi.com</span></p></td><td style="vertical-align:top;">
<p><span class="font2">106</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">IDN Times</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Brilio.net</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">27</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Total</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">6598</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font2" style="font-weight:bold;"><a name="bookmark22"></a>3.2. &nbsp;&nbsp;&nbsp;Document Embedding</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Document embedding</span><span class="font2"> dilakukan dengan tujuan mendapatkan representasi dokumen dalam ruang vektor. Dalam melakukan </span><span class="font2" style="font-style:italic;">document embedding</span><span class="font2">, penelitian ini menggunakan model DistilBERT. Contoh penggunaan DistilBERT ditunjukan pada Tabel 2.</span></p>
<p><span class="font2" style="font-weight:bold;">Tabel 2. </span><span class="font2">Contoh Hasil dari Penggunaan DistilBERT dalam Melakukan </span><span class="font2" style="font-style:italic;">Documment Embedding</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Teks</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-style:italic;">Embedding</span><span class="font2"> dalam bentuk vektor sepanjang 768</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">[-2.77933143e-02 -7.33952671e-02 -</span></p>
<p><span class="font2">9.79798436e-02 1.03335127e-01</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Serangan Rusia ke Ukraina terus berlanjut dan kini Kota Mariupol mendapat gempuran bertubi-tubi.</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2.59766459e-01 1.61224723e-01 -</span></p>
<p><span class="font2">6.59341663e-02 1.98788896e-01</span></p>
<p><span class="font2">-4.11727317e-02 2.53799617e-01</span></p>
<p><span class="font2">1.94205835e-01 3.57450619e-02</span></p>
<p><span class="font2">-8.34005997e-02</span></p></td></tr>
</table>
<p><span class="font2">… ]</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark23"></a><span class="font2" style="font-weight:bold;"><a name="bookmark24"></a>3.3. &nbsp;&nbsp;&nbsp;Dimensionality Reduction</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Dimensionality reduction</span><span class="font2"> diperlukan sebagai langkah untuk mengurangi dimensi dari </span><span class="font2" style="font-style:italic;">document embedding</span><span class="font2">. Dalam melakukan </span><span class="font2" style="font-style:italic;">dimensionality reduction</span><span class="font2">, penelitian ini menggunakan UMAP. Contoh penggunaan UMAP ditunjukan pada Tabel 3.</span></p>
<p><span class="font2" style="font-weight:bold;">Tabel 3. </span><span class="font2">Contoh Hasil dari Penggunaan UMAP dalam Melakukan </span><span class="font2" style="font-style:italic;">Dimensionality Reduction</span></p>
<div>
<p><span class="font2" style="font-style:italic;">Embedding</span><span class="font2"> dalam bentuk vektor yang telah direduksi hingga sepanjang 2</span></p>
</div><br clear="all">
<div>
<p><span class="font2">[15.094062 , 6.592128 ]</span></p>
</div><br clear="all">
<p><span class="font2" style="font-style:italic;">Embedding</span><span class="font2"> dalam bentuk vektor sepanjang 768</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[-2.77933143e-02 -7.33952671e-02 -</span></p></li></ul>
<p><span class="font2">9.79798436e-02 1.03335127e-01</span></p>
<p><span class="font2">2.59766459e-01 1.61224723e-01 -</span></p>
<p><span class="font2">6.59341663e-02 1.98788896e-01</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">-4.11727317e-02 2.53799617e-01</span></p></li></ul>
<p><span class="font2">1.94205835e-01 3.57450619e-02</span></p>
<p><span class="font2">-8.34005997e-02</span></p>
<p><span class="font2">… ]</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark25"></a><span class="font2" style="font-weight:bold;"><a name="bookmark26"></a>3.4. &nbsp;&nbsp;&nbsp;Document Clustering</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Document clustering</span><span class="font2"> dilakukan untuk mengelompokan dokumen yang serupa secara semantik. Dalam melakukan </span><span class="font2" style="font-style:italic;">document clustering</span><span class="font2">, penelitian ini menggunakan </span><span class="font2" style="font-style:italic;">Hierarchical Density Based Clustering</span><span class="font2"> (HDBSCAN). Hasil dari proses </span><span class="font2" style="font-style:italic;">document clustering</span><span class="font2"> ditunjukan pada Gambar 2. Proses pengelompokan dokumen menghasilkan 23 Kelompok.</span></p>
<div><img src="https://jurnal.harianregional.com/media/92840-2.jpg" alt="" style="width:421pt;height:226pt;">
<p><span class="font0" style="font-weight:bold;">0.0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7.5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10.0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12 5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15.0</span></p>
<p><span class="font2" style="font-weight:bold;">Gambar 2. </span><span class="font2">Visualisasi Hasil </span><span class="font2" style="font-style:italic;">Document Clustering</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark27"></a><span class="font2" style="font-weight:bold;"><a name="bookmark28"></a>3.5. &nbsp;&nbsp;&nbsp;Keyword Extraction</span></h2></li></ul>
<p><span class="font2" style="font-style:italic;">Keyword extraction</span><span class="font2"> dilakukan untuk mencari representasi setiap klaster/topik yang dihasilkan algoritma pengelompokan. Dalam penelitian ini c-TF-IDF digunakan dengan maksud memperlakukan satu kelompok dokumen sebagai satu buah dokumen, dimana algoritma TF-IDF</span></p>
<p><span class="font2">diterapkan untuk mendapatkan kumpulan kata kunci yang relevan terhadap satu kelompok dokumen. Kumpulan kata yang dihasilkan tiap klaster/topik ditunjukan pada Gambar 3.</span></p><img src="https://jurnal.harianregional.com/media/92840-3.jpg" alt="" style="width:426pt;height:490pt;">
<p><span class="font1">Custer 15</span></p>
<p><span class="font1">Custer 16</span></p>
<p><span class="font1">Cluster 17</span></p>
<p><span class="font1" style="font-weight:bold;">polymerase molekuler</span></p>
<p><span class="font2" style="font-weight:bold;">Gambar 3. </span><span class="font2">Representasi Tiap Klaster yang Dihasilkan Dari Penggunaan Metode DistilBERT</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark29"></a><span class="font2" style="font-weight:bold;"><a name="bookmark30"></a>3.6. &nbsp;&nbsp;&nbsp;Topic Coherence Measurement</span></h2></li></ul>
<p><span class="font2">Hasil dari perhitungan </span><span class="font2" style="font-style:italic;">Topic Coherence</span><span class="font2"> menggunakan korpus referensi data teks berita yang sama menunjukan bahwa penggunaan model </span><span class="font2" style="font-style:italic;">embedding</span><span class="font2"> DistilBERT dapat memberikan hasil yang lebih unggul apabila dibandingkan dengan penggunaan metode yang lebih umum yaitu LDA.</span></p>
<p><span class="font2">Hasil perhitungan </span><span class="font2" style="font-style:italic;">topic coherence</span><span class="font2"> menggunakan DistilBERT dintunjukan pada Gambar 4.</span></p><img src="https://jurnal.harianregional.com/media/92840-4.jpg" alt="" style="width:323pt;height:293pt;">
<p><span class="font2" style="font-weight:bold;">Gambar 4. </span><span class="font2">Hasil Perhitungan </span><span class="font2" style="font-style:italic;">Topic Coherence</span><span class="font2"> pada Topik yang Dihasilkan Dari Penggunaan</span></p>
<p><span class="font2">Metode DistilBERT</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark31"></a><span class="font2" style="font-weight:bold;"><a name="bookmark32"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font2">Jumlah klaster paling optimal yang didapat untuk data teks berita berbahasa Indonesia yang digunakan pada penelitian ini yaitu sejumlah 23 klaster. Hasil dari perhitungan </span><span class="font2" style="font-style:italic;">Topic Coherence </span><span class="font2">untuk 23 klaster yang dihasilkan dari penggunaan metode DistilBERT menggunakan korpus referensi data teks berita yang sama didapat nilai rata-rata sebesar 0.8402955343126479. Sedangkan hasil dari perhitungan </span><span class="font2" style="font-style:italic;">Topic Coherence</span><span class="font2"> untuk klaster yang dihasilkan dari penggunaan metode LDA menggunakan korpus referensi data teks berita yang sama didapat nilai rata-rata sebesar 0.8389966817692823. Hasil tersebut menunjukan bahwa penggunaan model </span><span class="font2" style="font-style:italic;">embedding</span><span class="font2"> DistilBERT dapat memberikan hasil yang lebih unggul apabila dibandingkan dengan penggunaan metode yang lebih umum yaitu LDA.</span></p>
<h2><a name="bookmark33"></a><span class="font2" style="font-weight:bold;"><a name="bookmark34"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;C. E. Moody, “Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec,” Mei 2016, [Daring]. Available: </span><a href="http://arxiv.org/abs/1605.02019"><span class="font2">http://arxiv.org/abs/1605.02019</span></a></p></li>
<li>
<p><span class="font2">[2] &nbsp;&nbsp;&nbsp;N. Reimers dan I. Gurevych, “Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation,” Apr 2020, [Daring]. Available: </span><a href="http://arxiv.org/abs/2004.09813"><span class="font2">http://arxiv.org/abs/2004.09813</span></a></p></li>
<li>
<p><span class="font2">[3] &nbsp;&nbsp;&nbsp;Wahyudin, “APLIKASI TOPIC MODELING PADA PEMBERITAAN PORTAL BERITA ONLINE SELAMA MASA PSBB PERTAMA.”</span></p></li>
<li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;“Pemodelan Topik Berita pada Portal Berita Online Berbahasa Indonesia Menggunakan Latent Dirichlet Allocation (LDA),” </span><span class="font2" style="font-style:italic;">Jurnal Ilmiah Komputasi</span><span class="font2">, vol. 20, no. 2, Jun 2021, doi: 10.32409/jikstik.20.2.2719.</span></p></li>
<li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;C. Naury, D. H. Fudholi, dan A. F. Hidayatullah, “Topic Modelling pada Sentimen Terhadap Headline Berita Online Berbahasa Indonesia Menggunakan LDA dan LSTM,” </span><span class="font2" style="font-style:italic;">JURNAL MEDIA INFORMATIKA BUDIDARMA</span><span class="font2">, vol. 5, no. 1, hlm. 24, Jan 2021, doi: 10.30865/mib.v5i1.2556.</span></p></li>
<li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;D. Jiang, Z. Chen, R. Lian, S. Bao, dan C. Li, “Familia: An Open-Source Toolkit for Industrial Topic Modeling,” Jul 2017, [Daring]. Available: </span><a href="http://arxiv.org/abs/1707.09823"><span class="font2">http://arxiv.org/abs/1707.09823</span></a></p></li>
<li>
<p><span class="font2">[7] &nbsp;&nbsp;&nbsp;A. B. Dieng, F. J. R. Ruiz, dan D. M. Blei, “Topic Modeling in Embedding Spaces,” Jul 2019, [Daring]. Available: </span><a href="http://arxiv.org/abs/1907.04907"><span class="font2">http://arxiv.org/abs/1907.04907</span></a></p></li>
<li>
<p><span class="font2">[8] &nbsp;&nbsp;&nbsp;V. Sanh, L. Debut, J. Chaumond, dan T. Wolf, “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,” Okt 2019, [Daring]. Available: </span><a href="http://arxiv.org/abs/1910.01108"><span class="font2">http://arxiv.org/abs/1910.01108</span></a></p></li>
<li>
<p><span class="font2">[9] &nbsp;&nbsp;&nbsp;R. Hakim </span><span class="font2" style="font-style:italic;">dkk.</span><span class="font2">, “Topic Modeling Pada Abstrak Skripsi Menggunakan Metode Latent Semantic Analysis,” 2022. [Daring]. Available: </span><a href="http://digilib.uinsby.ac.id"><span class="font2">http://digilib.uinsby.ac.id</span></a><span class="font2">.</span></p></li>
<li>
<p><span class="font2">[10] &nbsp;&nbsp;&nbsp;L. McInnes, J. Healy, dan J. Melville, “UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,” Feb 2018, [Daring]. Available: </span><a href="http://arxiv.org/abs/1802.03426"><span class="font2">http://arxiv.org/abs/1802.03426</span></a></p></li>
<li>
<p><span class="font2">[11] &nbsp;&nbsp;&nbsp;M. Allaoui, M. L. Kherfi, dan A. Cheriet, “Considerably improving clustering algorithms using umap dimensionality reduction technique: A comparative study,” dalam </span><span class="font2" style="font-style:italic;">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</span><span class="font2">, 2020, vol. 12119 LNCS, hlm. 317–325. doi: 10.1007/978-3-030-51935-3_34.</span></p></li>
<li>
<p><span class="font2">[12] &nbsp;&nbsp;&nbsp;L. McInnes, J. Healy, dan S. Astels, “hdbscan: Hierarchical density based clustering,” </span><span class="font2" style="font-style:italic;">The Journal of Open Source Software</span><span class="font2">, vol. 2, no. 11, hlm. 205, Mar 2017, doi: 10.21105/joss.00205.</span></p></li>
<li>
<p><span class="font2">[13] &nbsp;&nbsp;&nbsp;M. Grootendorst, “BERTopic: Neural topic modeling with a class-based TF-IDF procedure,” Mar 2022, [Daring]. Available: </span><a href="http://arxiv.org/abs/2203.05794"><span class="font2">http://arxiv.org/abs/2203.05794</span></a></p></li>
<li>
<p><span class="font2">[14] &nbsp;&nbsp;&nbsp;T. Joachims, “A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization.”</span></p></li></ul>
<p><span class="font2">306</span></p>