---
layout: full_article
title: "Implementation of Data Backup and Synchronization Based on Identity Column Real Time Data Warehouse"
author: "I Gede Adnyana, I Made Dwi Jendra Sulastra"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-55592 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-55592"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-55592"  
comments: true
---

<p><span class="font2" style="font-weight:bold;">LONTAR KOMPUTER VOL. 11, NO. 1 APRIL 2020</span></p>
<p><span class="font2" style="font-weight:bold;">DOI : 10.24843/LKJITI.2020.v11.i01.p02</span></p>
<p><span class="font2" style="font-weight:bold;">Accredited B by RISTEKDIKTI Decree No. 51/E/KPT/2017</span></p>
<p><span class="font2" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font2" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>Implementation of Data Backup and Synchronization Based on Identity Column Real-Time Data Warehouse</span></h1>
<p><span class="font2">I Gede Adnyana<sup>a1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">I Made Dwi Jendra Sulastra<sup>b2</sup></span></p>
<p><span class="font2"><sup>a</sup>Computer System Department, STMIK STIKOM Indonesia Denpasar, Indonesia </span><a href="mailto:1adnyana.nakkuta@gmail.com"><span class="font2"><sup>1</sup>adnyana.nakkuta@gmail.com</span></a></p>
<p><span class="font2"><sup>b</sup>Accounting Department, Bali State Polytechnic</span></p>
<p><span class="font2">Denpasar, Indonesia</span></p>
<p><a href="mailto:2dwijendrasulastra@gmail.com"><span class="font2"><sup>2</sup>dwijendrasulastra@gmail.com</span></a></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font2" style="font-style:italic;">Failure in the process of loading data from the Online Transactional Processing(OLTP) system to the Normalized Data Store (NDS) database can occur. This caused by a disruption in the network so that the OLTP system is unable to save data to the OLTP and NDS databases. Backup and synchronization data scenarios are needed to maintain data consistency and data availability. In this research, the process of data backup and synchronization is done by providing an identity column for the table in the OLTP database. An identity column is used to give status data, value '0' if the inserting process fails, and value '1' if successful. Data backup is done by storing temporary data into a CSV file format, then the CSV file is read, and an insert process is carried out into the OLTP database. After the insertion process into the OLTP database is successful, it continues with the synchronization process between the OLTP database and the NDS. Data synchronization between OLTP and NDS databases is done by checking the value of the Identity Column in each table in the OLTP database.</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font2" style="font-style:italic;">Normalized Data Store, Real-Time Data Warehouse, Backup, Synchronization, Identity Column</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font2">Data processing using information systems provides benefits, including speed up processing time due to automation. Information systems can also process data accurately, thereby reducing the risk of human error. Problems arise when information systems cannot process data with very large volumes, and data is scattered in different systems with diverse database structures [1][2][3][4][5][6]. An organization will experience difficulty in decision making when it encounters conflicting reports due to the lack of consistency of data from the various data sources used. Therefore we need an integrated data processing model that can process heterogeneous transactional data called data warehouse (DWH) [1][2][3][4][5][6][7][8][9][10][11].</span></p>
<p><span class="font2">The current trend, an organization needs the latest information in decision making. The realtime data warehouse has different characteristics from the classic data warehouse [1][2][5][7][11]. To achieve real-time data warehouse is very dependent on the process known as ETL (Extract, Transform, Loading) [1][2][5][7][10][11][12][13][14][15][16][17]. There are several ETL approaches to realizing real-time data warehouse between processing data that only undergoes updating or known as the Change Data Capture (CDC) concept [1][2][5][10][11][16][18]. Another approach is to accelerate the frequency of data extraction [2][11][12][15][19]. Both approaches aim to reduce the data processing time lag so that real-time data warehouse is realized.</span></p>
<p><span class="font2">In recent years Real-Time Data Warehouse (RTDW) has become a trend worldwide. ETL Data Warehouse that used to be run once a day is now running every hour, even every 5 minutes</span></p>
<p><span class="font2">(mini-batch). This can be done by using two approaches, namely push approach and pull approach. By using a push approach, the source system pushes data into the Data Warehouse. The data warehouse will be updated as soon as the data in the source system changes. Changes to the source system will be detected using database triggers. The pull approach method uses time intervals to update data in the data warehouse. Changes to the source system are detected using the timestamp or identity column method. An identity column is a column (field) in a table whose value will be used as a benchmark when data will be pulled from the source system and then stored in a data warehouse [2].</span></p>
<p><span class="font2">Some research has been carried out of RTDW, such as research on RTDW modeling using the CDC [5][10][11][16][18]. There is also research on making RTDW using the CDC Event-Driven Programming method [11]. Then there is research on the architecture of RTDW and how the process of making RTDW from the Traditional model [7]. The process of making RTDW is very much determined by the ETL process. To optimize the process of making RTDW, several studies have been conducted on the ETL process [11][12][13][14][15][16][17][18]. From these, several studies all focus on making RTDW without discussing the possibility of failure in the process of transferring data from the source system to the RTDW database.</span></p>
<p><span class="font2">To realize RTDW, data from the source system is loaded into a staging area or Normalized Data Store (NDS) which is used as a temporary storage place for data from various sources and ETL process sites before it is loaded in the Dimensional Data Store [1][2][5][11][12][13][14][15][16][17]. In one study on RTDW that implemented the use of CDC based on Event-Driven Programming [11], data was parallel stored in the OLTP database and the NDS database. In the process of loading data from the source system into the NDS, it is possible for the failure to occur due to the failure of the OLTP system to save data to the OLTP database and the NDS database. Failures can be caused by disruptions on the network, causing connection failures between the OLTP system and the OLTP database and the NDS database. To maintain data consistency between the source database and the NDS database and data availability, backup and synchronization data scenarios are needed.</span></p>
<p><span class="font2">In this research, the data backup process is carried out by storing temporary data in the form of a file with the format of Comma Separated Values (CSV), which is then followed by the process of synchronizing data. The data synchronization process is carried out by giving identity columns to each table in the source database. Furthermore, checking is done by checking the value of the identity column.</span></p>
<p><span class="font2">This research is expected to be able to handle the failure of the data transfer process from the OLTP system to the Real-Time Data Warehouse.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font2" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h2></li></ul>
<p><span class="font2">The stages in this research are generally shown in Figure 1. In Figure 1, the research stage starts with defining the problem where the problem that arises is the possibility of failure data transfer from the source system to the formation of RTDW. Then the next stage is the study of literature to obtain supporting literature for solving the problem. The backup scenario design is done by designing a backup scenario model of the problem if there is a failure of data transfer from the source system to the data warehouse. The next step is to design a data synchronization scenario between the source database and the NDS database using the identity column method. Data backup and synchronization models are then developed and implemented. Then do the system functionality test and analysis of test results. Then from the analysis of the test results, conclusions are made.</span></p><img src="https://jurnal.harianregional.com/media/55592-1.jpg" alt="" style="width:384pt;height:343pt;">
<p><span class="font2" style="font-weight:bold;">Figure 1. </span><span class="font2">General Stages of Research</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font2" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Real-Time Data Warehouse</span></h2></li></ul>
<p><span class="font2">The data warehouse is a system that retrieves and consolidates data periodically from source systems into dimensional or normalized data stores [2]. The data warehouse is a collection of data that has a subject-oriented, integrated, time-variant, and non-volatile nature of data collection in support of the management decision-making process [2]. The data warehouse is a system that extracts, cleans, adapts, and sends data sources into a dimensional data storage and then supports the implementation of queries and analysis for decision-making purposes. Information in the data warehouse is always presented in the form of dimensions and facts [1][2].</span></p>
<p><span class="font2">Classic data warehouse usually updates the data every day or every week. In accordance with business requests that require up to date or real-time data processing, the concept of the realtime data warehouse was created. In a real-time data warehouse, the process of updating data is carried out dynamically continuously with a break time that is almost close to zero [2]. To create a real-time data warehouse highly dependent on the Extract, Transform, Load (ETL) process. Another approach to reducing the lag time is to only process data that has been updated or known as the Change Data Capture concept [1][2].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font2" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Change Data Capture (CDC)</span></h2></li></ul>
<p><span class="font2">Change Data Capture (CDC) is an innovative approach to data integration, based on identifying, capturing, and sending changes made by data sources. By processing only the changes, the CDC makes the data integration process more efficient and reduces costs by reducing latency [1][2].</span></p>
<p><span class="font2">CDC is designed to maximize the efficiency of the ETL process. Without CDC, all data in the Online Transaction Processing (OLTP) database will be moved to the data warehouse whenever needed, while with CDC, only data changes that occur in the OLTP database will be moved. Therefore, the CDC can minimize the restore that is used to move data changes and minimize the latency of sending information, so this will save costs. There are two CDC scenario models integrated with ETL tools: [2].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. &nbsp;&nbsp;&nbsp;Batch-Oriented CDC (Pull CDC) Scenario: It is processing a set of data that only experiences periodic changes in high or low frequency.</span></p></li>
<li>
<p><span class="font2">b. &nbsp;&nbsp;&nbsp;Live CDC Scenarios (Push CDC): Are sending data changes to the ETL tool after the changes occur. It can be done with an event-delivery mechanism or messaging middleware.</span></p></li>
<li>
<h2><a name="bookmark10"></a><span class="font2" style="font-weight:bold;"><a name="bookmark11"></a>2. 2.1. CDC Based on Event-Driven Programming.</span></h2></li></ul>
<p><span class="font2">Event-driven programming is a programming technique where all program execution flows are determined by an event. When the program starts, it will wait for user input events. For each event that appears, the program will run the syntax to respond. The flow of program execution is determined by the order in which events occur [11].</span></p>
<p><span class="font2">In CDC based event-driven programming, when a user runs an event by clicking the button on the GUI, the data that has been filled in the GUI will be stored in the OLTP database, and the data will also be sent to the Normalized Data Store (NDS) database for further process. Data that has been inputted in a parallel GUI will be stored in two databases, namely the OLTP database and the NDS database [11].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font2" style="font-weight:bold;"><a name="bookmark13"></a>2.3. &nbsp;&nbsp;&nbsp;Extract, Transform, Loading (ETL)</span></h2></li></ul>
<p><span class="font2">Extract, Transform, Loading (ETL) is a very important process in the data warehouse, with this ETL data from the operating system can be entered into the data warehouse. The purpose of ETL is to collect, filter, process, and combine data from various sources to be stored in a data warehouse [1][2].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font2" style="font-weight:bold;"><a name="bookmark15"></a>2.3.1. &nbsp;&nbsp;&nbsp;Extract</span></h2></li></ul>
<p><span class="font2">Most of the data in the source system are very complex, so determining the relevant data is very difficult. Efforts to design and create extraction processes are very consuming time [1][2]. Raw data originating from the source system can usually be directly stored in the staging area with minimal restructuring to maintain the authenticity of the data. There are three methods for extraction that are commonly used, namely [2]:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. &nbsp;&nbsp;&nbsp;Whole table every time, this method extracts all rows in the table (full extraction). This method is suitable if the table size is small and consists of only a few rows.</span></p></li>
<li>
<p><span class="font2">b. &nbsp;&nbsp;&nbsp;Incremental extract, this method extracts only the changed rows, not extracts the entire table. In getting a changed row, you can use the timestamp column, identity column, transaction date, triggers, or combinations.</span></p></li>
<li>
<p><span class="font2">c. &nbsp;&nbsp;&nbsp;Fixed range, this method extracts several records or extracts with a certain time period; for example, the last six months of data.</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font2" style="font-weight:bold;"><a name="bookmark17"></a>2.3.2. &nbsp;&nbsp;&nbsp;Transform</span></h2></li></ul>
<p><span class="font2">The transformation phase applies a set of rules or functions to data taken from the source to get data to be sent to the final target. Some data sources will require very little or no data manipulation [1][2].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font2" style="font-weight:bold;"><a name="bookmark19"></a>2.3.3. &nbsp;&nbsp;&nbsp;Loading</span></h2></li></ul>
<p><span class="font2">The process of loading or also known as the process of delivering, is a process in which the transformed data is ready to be entered into a data warehouse, where the design of the table structure of data to be loaded (load) is made in the dimensional design process. The data from the loading process is ready to be queried and presented by the data warehouse. Therefore the</span></p>
<p><span class="font2">dimensional data warehouse design will determine the speed of the query process performed [1][2].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark20"></a><span class="font2" style="font-weight:bold;"><a name="bookmark21"></a>2.4. &nbsp;&nbsp;&nbsp;Data Backup and Synchronization Based on Identity Column</span></h2></li></ul>
<p><span class="font2">Data backup is needed to maintain the availability of data if there are problems with the system; for example, there are database connection problems in the system. In this research, data backup is done by storing temporary data in the form of files in the format of Comma Separated Values (CSV). If the connection problem has been resolved, then the CSV file reading process is then performed then the insert process is carried out in the OLTP database. If the insert process in the OLTP database is successfully continued with the process of synchronizing the data between the OLTP database and the NDS database</span></p>
<p><span class="font2">Data synchronization is needed to ensure data consistency is maintained. Data flow in the data warehouse, data from the source system is loaded into a staging area storage or Normalized Data Store (NDS). It is possible that the data failed to be loaded into the NDS, therefore synchronization of the data between the OLTP database as the source and the NDS database as the destination. The technique used to check records that fail to load on an NDS basis is the Identity Column, which is a column with a certain status value such as '0' or '1'. If the record fails to load in the NDS database, then the Identity Column record will have a value of ‘0’. Then the record with the Identity Column value of ‘0’ is carried out by inserting into the NDS database. If the insert process is successful, then the process of updating the Identity Column value will be made to a value of ‘1’.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark22"></a><span class="font2" style="font-weight:bold;"><a name="bookmark23"></a>3. &nbsp;&nbsp;&nbsp;Results and Discussion</span></h2></li></ul>
<p><span class="font2">The test is carried out using a Customer Relationship Management System (CRM) Online Transaction Processing (OLTP) simulation application that is useful for recording any customer complaints telecommunications services.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark24"></a><span class="font2" style="font-weight:bold;"><a name="bookmark25"></a>3.1. &nbsp;&nbsp;&nbsp;CDC Event Programming</span></h2></li></ul>
<p><span class="font2">When users enter new data in the CRM system, the data will be sent in parallel to the OLTP database and NDS database. CDC event programming is triggered by events that occur in the CRM system. The data input process on the CRM system can be seen in Figure 2.</span></p><img src="https://jurnal.harianregional.com/media/55592-2.jpg" alt="" style="width:426pt;height:246pt;">
<p><span class="font2" style="font-weight:bold;">Figure 2. </span><span class="font2">CDC Event Programming on CRM Systems</span></p>
<p><span class="font2">Figure 3 shows the data has been successfully stored in the OLTP database in the TComplain table.</span></p><img src="https://jurnal.harianregional.com/media/55592-3.jpg" alt="" style="width:427pt;height:40pt;">
<ul style="list-style:none;"><li>
<p><span class="font2" style="font-weight:bold;">Figure 3.</span><span class="font2"> &nbsp;&nbsp;&nbsp;Results of CDC Event Programming in OLTP Database</span></p></li></ul>
<p><span class="font2">Figure 4 shows the data has been successfully stored in the NDS database in the TNDSComplain</span></p>
<p><span class="font1" style="font-weight:bold;">Kuncikomplain nocase MSISDN JnsKompIain TgIKompIain DispatchKe TgIDispatch Active</span></p>
<p><span class="font1" style="font-weight:bold;">1 Ml..............................] CS180919094423 €2817600777 WbisaSMS 20184)3-19 00:00:00.000 FautVAS 2018-89-19 00:00:00.000 1</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2" style="font-weight:bold;">Figure 4.</span><span class="font2"> &nbsp;&nbsp;&nbsp;Results of CDC Event Programming in the NDS Database</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark26"></a><span class="font2" style="font-weight:bold;"><a name="bookmark27"></a>3.2. &nbsp;&nbsp;&nbsp;Data Backup</span></h2></li></ul>
<p><span class="font2">Data backups are needed to ensure that every transaction remains recorded when a database connection problem occurs. Transactions will be saved in a file format with Comma Separated Values (CSV) format and will be processed into the OLTP database when the database connection is back to normal. The process of inputting data on an OLTP system while offline can be seen in Figure 5.</span></p><img src="https://jurnal.harianregional.com/media/55592-4.jpg" alt="" style="width:425pt;height:223pt;">
<p><span class="font2" style="font-weight:bold;">Figure 5. </span><span class="font2">Create Case Form Input</span></p>
<p><span class="font2">If there is a database connection problem, the save and update buttons will be enabled on the offline action feature. In this research, the connection failure is simulated with a scenario of giving a false (closed) value to the connection variable in the OLTP system so that the connection with the OLTP database and the NDS database fails. Since the connection cannot be made, all transaction data will be saved in the CSV file shown in Figure 6.</span></p>
<p><span class="font5">I 'J FiIeCFeateCase.C'sv</span></p>
<p><span class="font6">1 CΞ181111020340,62819800710,Cannot call,11/11/2018 12:00:00 AM,,,EMP900001,Dispatch,Fault MSC,11/11/2018 12:OC 2 CS181111020445,62819800715,Cannot access UMB,11/11/2018 12:00:00 AM,,,EMP900001,Dispatch,Fault VAS,11/11/201£ 3 CS181111020607,62817600963,Cannot SMS,11/11/2018 12:00:00 AM,,,EMP900001,Dispatch,Fault VAS,11/11/2018 12:00: 4 CS181111020916,62817600965,Cannot MBanking,11/11/2018 12:00:00 AM,,,EMP900001,Dispatch,Fault VAS,11/11/2018 1 5 CS181111021246,62818100270,Cannot SMS,11/11/2018 12:00:00 AM,,,EMP900001,Dispatch,Fault VAS,11/11/2OlB 12:00: 6 CS181111021331,62818100325,Cannot &amp;££££ data,11/11/2018 12:00:00 AM,,,EMP900001,Dispatch,Fault GPRS/IN,11/11/</span></p>
<p><span class="font2" style="font-weight:bold;">Figure 6. </span><span class="font2">Transaction Data in CSV Format</span></p>
<p><span class="font2">When the database connection has returned to normal, the data file in CSV format will first be read into a tabular form, and then the process is inserted into the TComplain table in the OLTP database. The process of reading a CSV file and inserting data into an OLTP database is shown in Figure 7.</span></p><img src="https://jurnal.harianregional.com/media/55592-5.jpg" alt="" style="width:426pt;height:203pt;">
<p><span class="font2" style="font-weight:bold;">Figure 7. </span><span class="font2">CSV File Format Reading</span></p>
<p><span class="font2">If the OLTP insert key is pressed, the data stored in the CSV file will be inserted into the TComplain table in the OLTP CRM database. The results of adding data to the TComplain table in the OLTP CRM database are shown in Figure 8.</span></p><img src="https://jurnal.harianregional.com/media/55592-6.jpg" alt="" style="width:426pt;height:234pt;">
<p><span class="font2" style="font-weight:bold;">Figure 8. </span><span class="font2">CRM Synchronization Menu</span></p>
<p><span class="font2">Adding data from files in CSV format is done in the OLTP CRM database, which then requires the process of synchronizing data between the OLTP database and the NDS database shown in Figure 9.</span></p><img src="https://jurnal.harianregional.com/media/55592-7.jpg" alt="" style="width:426pt;height:176pt;">
<p><span class="font2" style="font-weight:bold;">Figure 9. </span><span class="font2">Synchronization between TComplain Table and TNDSComplain Table</span></p>
<p><span class="font2">Then every 30 seconds interval, the scheduler runs the data extraction process in the NDS database. Data in the NDS database is sent to the database warehouse using the Incremental Extraction based Timestamp method. This method can process only the latest data contained in the NDS database. The results of the process of inserting into a data warehouse database with the Incremental Extraction method based on Timestamp are shown in Figure 10.</span></p>
<p><span class="font0" style="font-weight:bold;">∏ Results Jj Messages</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font6">CompIainKey</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Case Number</span></p></td><td style="vertical-align:top;">
<p><span class="font6">MSISDN_A</span></p></td><td style="vertical-align:top;">
<p><span class="font6">MSISDN B</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Complain</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Date_of_Complain</span></p></td><td style="vertical-align:top;">
<p><span class="font6">DispatchJo</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Effective date</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">1 &nbsp;&nbsp;&nbsp;</span><span class="font4">∣</span><span class="font6"> 40</span></p></td><td style="vertical-align:top;">
<p><span class="font6">■ CS181111020340</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62819800710</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62817214231</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Cannot call</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018 11 11 00:00:00.000</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Fault MSC</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018 11 11 03:01:19.570</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">2 &nbsp;&nbsp;&nbsp;41</span></p></td><td style="vertical-align:top;">
<p><span class="font6">CS181111020445</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62819800715</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62878123467</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Cannot access UMB</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018-11-11 00 00 00 OOO</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Fault VAS</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018-11-11 03 01 19 570</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">3 &nbsp;&nbsp;&nbsp;42</span></p></td><td style="vertical-align:top;">
<p><span class="font6">CS181111020607</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62817600963</span></p></td><td style="vertical-align:top;">
<p><span class="font6">628193231987</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Cannot SMS</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018-11-11 00.00:00.000</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Fault VAS</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018-11-11 03:01:19.570</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">4 &nbsp;&nbsp;&nbsp;43</span></p></td><td style="vertical-align:top;">
<p><span class="font6">CS181111020916</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62817600965</span></p></td><td style="vertical-align:top;">
<p><span class="font6">6281281211114</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Cannot MBanking</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018-11-11 00.00.00.000</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Fault VAS</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018-11-11 03.01.19.570</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font6">5 &nbsp;&nbsp;&nbsp;44</span></p></td><td style="vertical-align:top;">
<p><span class="font6">CS181111021246</span></p></td><td style="vertical-align:top;">
<p><span class="font6">62818100270</span></p></td><td style="vertical-align:top;">
<p><span class="font6">628189988165</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Cannot SMS</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018 11 11 00:00:00.000</span></p></td><td style="vertical-align:top;">
<p><span class="font6">Fault VAS</span></p></td><td style="vertical-align:top;">
<p><span class="font6">2018 11 11 03:01:19.570</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font6">6 &nbsp;&nbsp;&nbsp;45</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">CS181111021331</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">62818100325</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">628561113562</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">Cannot acces data</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">2018-11-11 00 00 00 000</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">Fault GPRS/IN</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6">2018-11-11 03 01 19 570</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark28"></a><span class="font2" style="font-weight:bold;"><a name="bookmark29"></a>3.3. &nbsp;&nbsp;&nbsp;Data Synchronization</span></h2></li></ul>
<p><span class="font2">In the CDC based event-driven programming method, data inputted on the parallel CRM system GUI is stored in the OLTP database and the NDS database. Data synchronization is needed to ensure the data in the OLTP database is the same as the NDS database. Checking and synchronizing data in the OLTP database can be seen in Figure 11.</span></p><img src="https://jurnal.harianregional.com/media/55592-8.jpg" alt="" style="width:426pt;height:193pt;">
<p><span class="font2" style="font-weight:bold;">Figure 11. </span><span class="font2">CRM Synchronization Menu</span></p>
<p><span class="font2">Figure 11 shows the results of checking synchronization in the OLTP CRM database. Two data are not synchronized. Both of these data failed to be stored in the NDS database, and the data was stored in the OLTP CRM database. Asynchronous data is found by checking the status of the Identity Column in the TComplain table, which functions as an Identity Column, the NDS_Status column, which, if not synchronized, has a value of '0'.</span></p>
<p><span class="font2">If data that has not been synchronized found, then the data synchronization process is carried out by pressing the Synchronize button. When the Synchronize button is pressed, insert the asynchronous data into the TNDSComplain table in the NDS database. If the data is successfully inserted into the NDS database, the NDS_status column value is updated to '1'. The process of synchronizing data between the TComplain table and the TNDSComplain table is shown in Figure 12.</span></p><img src="https://jurnal.harianregional.com/media/55592-9.jpg" alt="" style="width:426pt;height:186pt;">
<p><span class="font2" style="font-weight:bold;">Figure 12. </span><span class="font2">Synchronizing data between TComplain table and TNDSComplain table</span></p>
<p><span class="font2">Then every 30 seconds interval, the scheduler runs the data extraction process in the NDS database. Data in the NDS database is sent to the database warehouse using the Incremental Extraction based Timestamp method. This method can process only the latest data contained in the NDS database. The results of the process of inserting into a data warehouse database with the Incremental Extraction method based on Timestamp are shown in Figure 13.</span></p><img src="https://jurnal.harianregional.com/media/55592-10.jpg" alt="" style="width:425pt;height:55pt;">
<p><span class="font2" style="font-weight:bold;">Figure 13. </span><span class="font2">Results in the TFact_Complain table</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark30"></a><span class="font2" style="font-weight:bold;"><a name="bookmark31"></a>4. &nbsp;&nbsp;&nbsp;Conclusions</span></h2></li></ul>
<p><span class="font2">Data backup is done by storing temporary data in a file in the format of Comma Separated Values (CSV), then the CSV file is read, and the process is inserted into the OLTP database.</span></p>
<p><span class="font2">After the insertion process into the OLTP database is successful, it continues with the</span></p>
<p><span class="font2">synchronization process between the OLTP database and the NDS database. Data synchronization between the OLTP database and the NDS database is done by checking the</span></p>
<p><span class="font2">value of the Identity Column in each table in the OLTP database. If the value of the Identity</span></p>
<p><span class="font2">Column is ‘0’, then the process of inserting the data into the NDS database is carried out. If the data is successful, it is loaded into the NDS database, then the value of the Identity Column is updated to be ‘1’. Data in the NDS database is loaded into the Dimensional Data Store with the Incremental Extraction based Timestamp method to create the Real-Time Data Warehouse.</span></p>
<h2><a name="bookmark32"></a><span class="font2" style="font-weight:bold;"><a name="bookmark33"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;R. Kimball and M. Ross, </span><span class="font2" style="font-style:italic;">The Data Warehouse Toolkit, The Definitive Guide to Dimensional Modeling</span><span class="font2">. 2013.</span></p></li>
<li>
<p><span class="font2">[2] &nbsp;&nbsp;V. Rainardi, </span><span class="font2" style="font-style:italic;">Building a data warehouse: With examples in SQL server</span><span class="font2">. 2008.</span></p></li>
<li>
<p><span class="font2">[3] &nbsp;&nbsp;A. Khalaf Hamoud, A. Salah Hashim, and W. Akeel Awadh, “CLINICAL DATA</span></p></li></ul>
<p><span class="font2">WAREHOUSE A REVIEW,” </span><span class="font2" style="font-style:italic;">Iraqi Journal Computing Informatics</span><span class="font2">, &nbsp;2018, doi:</span></p>
<p><span class="font2">10.25195/2017/4424.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;M. P. Ambara, M. Sudarma, and I. N. S. Kumara, “Desain Sistem Semantic Data Warehouse dengan Metode Ontology dan Rule Based untuk Mengolah Data Akademik Universitas XYZ di Bali,” </span><span class="font2" style="font-style:italic;">Majalah Ilmiah Teknologi Elektro</span><span class="font2"> 2016, doi: 10.24843/mite.2016.v15i01p02.</span></p></li>
<li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;I. M. D. J. Sulastra, M. Sudarma, and I. N. S. Kumara, “PEMODELAN INTEGRASI NEARLY REAL TIME DATA WAREHOUSE DENGAN SERVICE ORIENTED ARCHITECTURE UNTUK MENUNJANG SISTEM INFORMASI RETAIL,” </span><span class="font2" style="font-style:italic;">Majalah Ilmiah Teknologi Elektro</span><span class="font2"> 2015, doi: 10.24843/mite.2015.v14i02p03.</span></p></li>
<li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;M. R. Pasha, “Data Warehousing and the Unstructured Data,” </span><span class="font2" style="font-style:italic;">Bahria Univercity Islam. Campus Gradute Resuslt</span><span class="font2">, vol. DOI:10.1, 2016.</span></p></li>
<li>
<p><span class="font2">[7] &nbsp;&nbsp;&nbsp;S. Bouaziz, A. Nabli, and F. Gargouri, “From traditional data warehouse to real time data warehouse,” in </span><span class="font2" style="font-style:italic;">Advances in Intelligent Systems and Computing</span><span class="font2">, 2017, doi: 10.1007/978-3-319-53480-0_46.</span></p></li>
<li>
<p><span class="font2">[8] &nbsp;&nbsp;&nbsp;S. Bouaziz, A. Nabli, and F. Gargouri, “Design a data warehouse schema from document-oriented &nbsp;&nbsp;&nbsp;database,” &nbsp;&nbsp;&nbsp;in &nbsp;&nbsp;&nbsp;</span><span class="font2" style="font-style:italic;">Procedia &nbsp;&nbsp;&nbsp;Computer &nbsp;&nbsp;&nbsp;Science</span><span class="font2">, &nbsp;&nbsp;&nbsp;2019, &nbsp;&nbsp;&nbsp;doi:</span></p></li></ul>
<p><span class="font2">10.1016/j.procs.2019.09.177.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[9] &nbsp;&nbsp;&nbsp;F. Z. Al Faris, Suharjito, Diana, and A. Nugroho, “Development of Data Warehouse to Improve Services in IT Services Company,” in </span><span class="font2" style="font-style:italic;">Proceedings of 2018 International Conference on Information Management and Technology, ICIMTech 2018</span><span class="font2">, 2018, doi: 10.1109/ICIMTech.2018.8528146.</span></p></li>
<li>
<p><span class="font2">[10] &nbsp;&nbsp;&nbsp;H. Chandra, “Analysis of Change Data Capture Method in Heterogeneous Data Sources to Support RTDW,” in </span><span class="font2" style="font-style:italic;">2018 4th International Conference on Computer and Information</span></p></li></ul>
<p><span class="font2" style="font-style:italic;">Sciences: Revolutionising Digital Landscape for Sustainable Smart Society, ICCOINS 2018 - Proceedings</span><span class="font2">, 2018, doi: 10.1109/ICCOINS.2018.8510574.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[11] &nbsp;&nbsp;&nbsp;I. G. Adnyana, M. Sudarma, and W. G. Ariastina, “Middleware ETL with CDC based on Event Driven Programming,” </span><span class="font2" style="font-style:italic;">International Journal Of Engineering And Emerging Technology</span><span class="font2">, vol. Vol. 3, No, 2018.</span></p></li>
<li>
<p><span class="font2">[12] &nbsp;&nbsp;&nbsp;A. Wibowo, “Problems and available solutions on the stage of Extract, Transform, and Loading in near real-time data warehousing (a literature study),” in </span><span class="font2" style="font-style:italic;">2015 International Seminar on Intelligent Technology and Its Applications, ISITIA 2015 - Proceeding</span><span class="font2">, 2015, doi: 10.1109/ISITIA.2015.7220004.</span></p></li>
<li>
<p><span class="font2">[13] &nbsp;&nbsp;&nbsp;A. Sabtu </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, “The challenges of Extract, Transform and Loading (ETL) system implementation for near real-time environment,” in </span><span class="font2" style="font-style:italic;">International Conference on Research and Innovation in Information Systems, ICRIIS</span><span class="font2">, 2017, doi: 10.1109/ICRIIS.2017.8002467.</span></p></li>
<li>
<p><span class="font2">[14] &nbsp;&nbsp;&nbsp;R. P. Deb Nath, K. Hose, T. B. Pedersen, and O. Romero, “SETL: A programmable semantic extract-transform-load framework for semantic data warehouses,” </span><span class="font2" style="font-style:italic;">Information Systems</span><span class="font2">, 2017, doi: 10.1016/j.is.2017.01.005.</span></p></li>
<li>
<p><span class="font2">[15] &nbsp;&nbsp;&nbsp;N. Biswas, A. Sarkar, and K. C. Mondal, “Efficient incremental loading in ETL processing for real-time data integration,” </span><span class="font2" style="font-style:italic;">Innovation in System Software Engineering</span><span class="font2">, 2019, doi: 10.1007/s11334-019-00344-4.</span></p></li>
<li>
<p><span class="font2">[16] &nbsp;S. Thulasiram and N. Ramaiah, “Real Time Data Warehouse Updates Through</span></p></li></ul>
<p><span class="font2">Extraction-Transformation-Loading Process Using Change Data Capture Method,” 2020.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[17] &nbsp;B. Pan, G. Zhang, and X. Qin, “Design and realization of an ETL method in business</span></p></li></ul>
<p><span class="font2">intelligence project,” in </span><span class="font2" style="font-style:italic;">2018 3rd IEEE International Conference on Cloud Computing and Big Data Analysis, ICCCBDA 2018</span><span class="font2">, 2018, doi: 10.1109/ICCCBDA.2018.8386526.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[18] &nbsp;&nbsp;&nbsp;Denny, I. P. M. Atmaja, A. Saptawijaya, and S. Aminah, “Implementation of change data capture in ETL process for data warehouse using HDFS and apache spark,” in </span><span class="font2" style="font-style:italic;">Proceedings - WBIS 2017: 2017 International Workshop on Big Data and Information Security</span><span class="font2">, 2018, doi: 10.1109/IWBIS.2017.8275102.</span></p></li>
<li>
<p><span class="font2">[19] &nbsp;&nbsp;&nbsp;I. Mekterović and L. Brkić, “Delta view generation for incremental loading of large dimensions in a data warehouse,” in </span><span class="font2" style="font-style:italic;">2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2015 -</span></p></li></ul>
<p><span class="font2" style="font-style:italic;">Proceedings</span><span class="font2">, 2015, doi: 10.1109/MIPRO.2015.7160496.</span></p>
<p><span class="font2">19</span></p>