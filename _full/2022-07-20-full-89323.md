---
layout: full_article
title: "Implementation of Natural Feature Tracking in Eclipse Applications Using Augmented Reality"
author: "Diky Rizky Awan, Agus Muliantara, I Gede Arta Wibawa, Cokorda Rai Adi Pramartha, Ida Bagus Gede Dwidasmara, I Putu Gede Hendra Suputra"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-89323 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-89323"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-89323"  
comments: true
---

<p><span class="font1">p-ISSN: 2301-5373</span></p>
<p><span class="font1">e-ISSN: 2654-5101</span></p>
<p><span class="font1">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font1">Volume 11, No 3. February 2023</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>Solar Eclipse Augmented Reality Application Using Natural Feature Tracking Method</span></h1>
<p><span class="font1">Diky Rizky Awan<sup>a1</sup></span><span class="font1" style="font-weight:bold;">, </span><span class="font1">Agus Muliantara<sup>a2</sup></span><span class="font1" style="font-weight:bold;">, </span><span class="font1">I Gede Arta Wibawa<sup>a3</sup>, Cokorda Rai Adi Pramartha<sup>a4</sup>, Ida Bagus Gede Dwidasmara<sup>a5</sup>, I Putu Gede Hendra Suputra<sup>a6</sup></span></p>
<p><span class="font1"><sup>a</sup>Informatics Department, Faculty of Mathematics and Natural Sciences, Udayana University South Kuta, Badung, Bali, Indonesia </span><a href="mailto:1dikyrizkyawan@gmail.com"><span class="font1"><sup>1</sup>dikyrizkyawan@gmail.com</span></a></p>
<p><a href="mailto:2muliantara@unud.ac.id"><span class="font1"><sup>2</sup>muliantara@unud.ac.id</span></a><span class="font1"> </span><a href="mailto:3gede.arta@unud.ac.id"><span class="font1"><sup>3</sup>gede.arta@unud.ac.id</span></a><span class="font1"> </span><a href="mailto:4cokorda@unud.ac.id"><span class="font1"><sup>4</sup>cokorda@unud.ac.id</span></a><span class="font1"> </span><a href="mailto:5dwidasmara@unud.ac.id"><span class="font1"><sup>5</sup>dwidasmara@unud.ac.id</span></a><span class="font1"> </span><a href="mailto:6hendra.suputra@unud.ac.id"><span class="font1"><sup>6</sup>hendra.suputra@unud.ac.id</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">An eclipse is a phenomenon that occurs when a celestial body moves into the shadow of another celestial body, be it a solar eclipse or a lunar eclipse. In the process of understanding this phenomenon, people do not understand what is meant by the eclipse phenomenon. Because of this, we need a teaching aid to help us understand the material.</span></p>
<p><span class="font1" style="font-style:italic;">The application will apply Augmented Reality technology using the Natural Feature Tracking (NFT) method. The application in this research will be made in the form of a native website. With python 3.7.0 64-bit programming language using the IDE (integrated development engineer), namely Visual Studio Code. This application will take advantage of the flask frameworks. This website will be native. Application implementation using native website templates with HTML (Hypertext Markup Language), CSS (Cascading Style Sheets), and Javascript. This application will be intended for a platform website resolution of 16:9 with a breakdown of 1920 x 1080 pixels. However, this ratio will be able to adjust because it applies the bootstrap 4 framework. From some tests conducted, the percentage of accuracy of the percentage of 3D object appearance is 52% on objects performing in the correct position, 34% on objects performing in the wrong position, and 10.67% on objects not appearing. In terms of functions, it has been running and meets the requirements with black box testing with 100% accuracy which is carried out based on 9 functions in the use case diagram.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">Solar Eclipse, Augmented Reality, Natural Feature Tracking</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h4></li></ul>
<p><span class="font1">An eclipse is a phenomenon that occurs when a space object moves into the shadow of another space object, including a solar eclipse or lunar eclipse [1]. A solar eclipse is a phenomenon when the moon is right between the sun and the earth so that the moon blocks the sun's rays from reaching the earth causing the sun not visible from the earth because it is covered by the moon's shadow [2].</span></p>
<p><span class="font1">In the process of understanding this phenomenon, people do not understand what is meant by the eclipse phenomenon because the eclipse is a fairly rare phenomenon because the frequency of eclipses with identical configurations is quite long repeatedly using the Saros cycle period which will occur every 18 years and 10 days [3]. And in his explanation in the book, only contains theories and images that can only be seen in two dimensions. Because of this, people tend to feel bored quickly and less interested so they have difficulty in understanding and reduce interest in learning about the eclipse phenomenon. Because of this, we need a prop to help in understanding the material.</span></p>
<p><span class="font1">Augmented Reality (AR) is the combination of real and virtual objects or two-dimensional or threedimensional virtual in a real environment that runs interactively in real-time, where the virtual objects are integrated into the real world [4]. Natural Feature Tracking (NFT) is one of the image tracking methods to detect and track features naturally in images from angles, lines, or blobs [5].</span></p>
<p><span class="font1">NFT itself has many algorithms in its application such as the SIFT algorithm, SURF, ORB, BRISK, and others. This study uses the ORB algorithm because the ORB algorithm is a binary descriptor algorithm that is fast and has high resistance in feature detection so it is suitable for applying to AR applications [6].</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Method</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark6"></a>2.1 &nbsp;Augmented Reality</span></h4></li></ul>
<p><span class="font1">The merging of real and virtual objects is possible with the appropriate display technology, interactivity is possible through certain input devices, and good integration requires effective tracking. AR allows users to see the real world equipped with virtual objects that are combined with the real world. AR plays a role in complementing reality rather than completely replacing it. This helps users bring up the desired objects as if the virtual and real objects coexist in the same space [4].</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark7"></a><span class="font1" style="font-weight:bold;"><a name="bookmark8"></a>2.2 &nbsp;&nbsp;&nbsp;Natural Feature Tracking</span></h4></li></ul>
<p><span class="font1">The initial process of NFTs is to rely on a Point of Interest (POI), where objects visible in the environment can be applied directly to detect the feature [7]. After going through the detection process, a feature description will be carried out which gets results in the form of floating-point or binary-based vectors, then continued by matching features that match the image in the database which aims to extract every six degrees of freedom (6DoF) pose which is used as a reference in the freedom of movement of an object. Then, a feature matching process is carried out where the two descriptors on the image will be matched. The matching of each camera frame must be calculated by the descriptor. If a sufficient connection is found between the features in the database and the camera frame, then the pose of the 6DoF camera can be calculated as well as determine the position of the visual object in the real environment.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark9"></a><span class="font1" style="font-weight:bold;"><a name="bookmark10"></a>2.3 &nbsp;&nbsp;&nbsp;Feature Detection</span></h4></li></ul>
<p><span class="font1">The detection feature is used to search for and identify objects in the image [8]. Feature detection must meet several requirements including fast computational time, stability of changing lighting conditions, and image defocusing, stability to observations from different points of view and invariant scale [7]. The ORB algorithm for feature detection uses an application based on the FAST corner detection algorithm with the initial image that has been converted to grayscale. Here is the flow of the feature detection process in the FAST algorithm. It first determines the p point on the image with the starting position (x,y) and the threshold value [9]. Then determine the radius of 3 pixels from point p so that 16 pixel points are obtained. Next, determine the location of 4 points of 16 pixels. The first point on the coordinates (xp, yp+3), the second point on the coordinates (xp+3, yp), the third point on the coordinates (xp, yp-3), and the fourth point on the coordinates (xp-3, yp). It then compares the intensity of the center point p with the reference of the previous four points for up to 16-pixels. The center point p is the angular point when there are at least 3 points without normal intensity. These categories include as in equation (1).</span></p>
<p><span class="font4" style="font-style:italic;">!</span><span class="font9" style="font-style:italic;">Dark, lp → x ≤ lp — t</span></p>
<h3><a name="bookmark11"></a><span class="font9" style="font-style:italic;"><a name="bookmark12"></a>Normal, lp — t &lt;&nbsp;lp → x &lt;lp + t</span><span class="font2">........................................................................(1)</span></h3>
<h3><a name="bookmark13"></a><span class="font9" style="font-style:italic;"><a name="bookmark14"></a>Light, lp + t ≤ lp → x</span></h3>
<p><span class="font1">Where :</span></p>
<p><span class="font8" style="font-style:italic;">Sp</span><span class="font1" style="font-style:italic;">→</span><span class="font1"> : Intensity of the center point (p point)</span></p>
<p><span class="font8" style="font-style:italic;">Ip</span><span class="font1" style="font-style:italic;">→</span><span class="font1"> : Pixel intensity x (neighbor intensity point)</span></p>
<p><span class="font1">t : threshold</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark15"></a><span class="font1" style="font-weight:bold;"><a name="bookmark16"></a>2.4 &nbsp;&nbsp;&nbsp;Feature Descriptor</span></h4></li></ul>
<p><span class="font1">The descriptor aims to describe a region around a key point to generate a description vector because the descriptor describes each detected feature as a vector of a fixed length [6]. The ORB algorithm uses the basis of the improved BRIEF algorithm to calculate the descriptor for each point and the descriptor vector consists of the numbers 0 and 1 because the algorithm is of binary type. To increase the resistance to digital noise, gaussian filtering was first carried out on the image. The binary test can be formulated as in equation (2).</span></p>
<p><span class="font9"><sub>=</sub> { <sup>1,</sup>p(<sup>χ</sup>)&lt;p(y) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font0" style="font-style:italic;font-variant:small-caps;">(<sub>7</sub>λ</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9" style="font-style:italic;"><sup>τ</sup></span><span class="font9"> (;,) lθ,p(χ)≥p(y)</span><span class="font9" style="font-style:italic;">(2)</span></p></li></ul>
<p><span class="font1">Where (x) is the gray value in the x plane around the image feature point, and (y) is the gray value in the y plane around the image feature point. Then, determine a patch with a size of S × S and randomly select N which usually amounts to 256. Then, the brightness value of each pair of points is compared according to the equation and binary assignment. Obtained an N-dimensional vector consisting of a binary string N as in equation (3).</span></p>
<p><span class="font9" style="font-style:italic;">f </span><span class="font7" style="font-style:italic;">N</span><span class="font9" style="font-style:italic;">(P)</span><span class="font9"> = ∑</span><span class="font7">ι≤i≤N </span><span class="font9">2</span><span class="font7">i<sup>-</sup>1 </span><span class="font9" style="font-style:italic;">τ (PW</span><span class="font7" style="font-style:italic;">i)</span><span class="font9">.....................................................................................................................................(3)</span></p>
<p><span class="font1">Where fN (p) is an N-dimensional vector that stores binary values from patches that are 2i-1 in size. The ORB algorithm uses the Steer BRIEF algorithm to calculate the main direction of each feature point because the BRIEF descriptor has no invariants to the rotation, causing easy data loss when the image is rotated.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark17"></a><span class="font1" style="font-weight:bold;"><a name="bookmark18"></a>2.5 &nbsp;&nbsp;&nbsp;Feature Matching</span></h4></li></ul>
<p><span class="font1">The feature point-based matching method is the main method for image matching because of its simple, fast, high-accuracy calculations, and has invariants to grayscale scales, lighting, and graphic distortions [10]. Feature matching is performed to determine which characteristics are represented in the descriptor of the two images according to the criteria that can be matched using a brute-force matcher.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark19"></a><span class="font1" style="font-weight:bold;"><a name="bookmark20"></a>2.6 &nbsp;&nbsp;&nbsp;Data</span></h4></li></ul>
<p><span class="font1">The data used in this study is divided into two types, namely image data and virtual object data. There are two kinds of reference imagery data, namely those obtained from camera photos and camera frames in real-time. Virtual object data is component data from an eclipse that is used as a reference in making 3D modeling objects. This data is obtained through books and library sources. That is solar eclipse data which is when the eclipse has a sun-moon-earth configuration on one straight line and a total solar eclipse is when the solar eclipse and the entire sun are covered by the moon.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark21"></a><span class="font1" style="font-weight:bold;"><a name="bookmark22"></a>2.7 &nbsp;&nbsp;&nbsp;Flowchart</span></h4></li></ul>
<p><span class="font1">First, the image on the camera and the reference image is inputted, followed by the feature detection process for both images. Then after the image is detected the feature is continued with the feature description process. Then, the two features are matched and when the two features match, it is continued by estimating the image pose. Then proceed with the process of displaying virtual objects in a real environment. If the two features do not match then the process will be repeated on the camera image. The process flow can be seen in figure 1.</span></p>
<div><img src="https://jurnal.harianregional.com/media/89323-1.jpg" alt="" style="width:179pt;height:278pt;">
<p><span class="font1" style="font-weight:bold;">Figure 1 </span><span class="font1">Flowchart</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h4><a name="bookmark23"></a><span class="font1" style="font-weight:bold;"><a name="bookmark24"></a>2.8 &nbsp;&nbsp;&nbsp;System Design</span></h4></li></ul>
<p><span class="font1">This interface design was created using Adobe Illustrator CC 2017 as a design maker or application design framework and implemented in the form of a native website. This design implementation uses a native website template with HTML (Hypertext Markup Language), CSS (Cascading Style Sheets), and Javascript. This design is intended for website platforms with a resolution of 16:9 with a resolution of 1920 x 1080 pixels. This interface design consists of 5 pages, namely the home page that can be seen in figure 2, how to use that can be seen in figure 3, about that can be seen in figure 4, new marker that can be seen in figure 5, and AR that can be seen in figure 6.</span></p>
<div><img src="https://jurnal.harianregional.com/media/89323-2.jpg" alt="" style="width:201pt;height:103pt;">
<p><span class="font1" style="font-weight:bold;font-style:italic;">Figure 3 </span><span class="font1" style="font-style:italic;">Home Page</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/89323-3.jpg" alt="" style="width:200pt;height:105pt;">
<p><span class="font1" style="font-weight:bold;font-style:italic;">Figure 2 </span><span class="font1" style="font-style:italic;">How to Use</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/89323-4.jpg" alt="" style="width:199pt;height:115pt;">
<p><span class="font1" style="font-weight:bold;font-style:italic;">Figure 5 </span><span class="font1" style="font-style:italic;">About</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/89323-5.jpg" alt="" style="width:196pt;height:126pt;">
<p><span class="font1" style="font-weight:bold;font-style:italic;">Figure 4 </span><span class="font1" style="font-style:italic;">New Marker</span></p>
</div><br clear="all"><img src="https://jurnal.harianregional.com/media/89323-6.jpg" alt="" style="width:443pt;height:226pt;">
<p><span class="font1" style="font-weight:bold;font-style:italic;">Figure 6 </span><span class="font1" style="font-style:italic;">Augmented Reality</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark25"></a><span class="font1" style="font-weight:bold;"><a name="bookmark26"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark27"></a>3.1. &nbsp;Virtual Object Testing</span></h4></li></ul>
<p><span class="font1">This test, it was carried out at a distance of 10 cm, 12 cm, 14 cm, and 16 cm. Tests are performed with a range of 30 frames on each test which in each frame will be counted as points. Then the points of the number of successful tests will be totaled and divided by the total number of tests and the result will be multiplied by 100%.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 1 </span><span class="font1">Distance Aspect Virtual Object Testing Results</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">No</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">Result</span></p></td><td colspan="5" style="vertical-align:top;">
<p><span class="font1">Distance</span></p></td><td rowspan="2" style="vertical-align:top;">
<p><span class="font1">Total</span></p>
<p><span class="font1">Point</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">Percentage</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">8 cm</span></p></td><td style="vertical-align:top;">
<p><span class="font1">10 cm</span></p></td><td style="vertical-align:top;">
<p><span class="font1">12 cm</span></p></td><td style="vertical-align:top;">
<p><span class="font1">14 cm</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16 cm</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">The object appears correctly positioned</span></p></td><td style="vertical-align:top;">
<p><span class="font1">30</span></p></td><td style="vertical-align:top;">
<p><span class="font1">29</span></p></td><td style="vertical-align:top;">
<p><span class="font1">15</span></p></td><td style="vertical-align:top;">
<p><span class="font1">4</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0</span></p></td><td style="vertical-align:top;">
<p><span class="font1">78</span></p></td><td style="vertical-align:top;">
<p><span class="font1">52%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">The object appears incorrectly positioned</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0</span></p></td><td style="vertical-align:top;">
<p><span class="font1">1</span></p></td><td style="vertical-align:top;">
<p><span class="font1">15</span></p></td><td style="vertical-align:top;">
<p><span class="font1">21</span></p></td><td style="vertical-align:top;">
<p><span class="font1">14</span></p></td><td style="vertical-align:top;">
<p><span class="font1">51</span></p></td><td style="vertical-align:top;">
<p><span class="font1">34%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Object not displaying</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16</span></p></td><td style="vertical-align:top;">
<p><span class="font1">10.67%</span></p></td></tr>
</table>
<p><span class="font1">In table 1, it can be seen that the farther the distance, the more likely the accuracy of the object decreases. Testing the objects of this system produced quite good results, judging from the 3D objects that continue to appear at close range, although at long distances they get quite poor results. All points are summed up and divided by the overall test amount and multiplied by 100%. Testing for objects to perform correctly positioned earned a percentage of 52%. Meanwhile, testing for objects to appear incorrectly positioned got a percentage of 34%. The last test for non-performing objects got a percentage of 10.67%.</span></p>
<p><span class="font1" style="font-weight:bold;">Figure 7 </span><span class="font1">Virtual Object Testing Graph</span></p>
<h2><a name="bookmark28"></a><span class="font6" style="font-weight:bold;"><a name="bookmark29"></a>VIRTUAL OBJECT TESTING</span></h2>
<div>
<p><span class="font5">40</span></p>
<p><span class="font5">30</span></p>
<p><span class="font5">20</span></p>
<p><span class="font5">10</span></p>
<p><span class="font5">0</span></p>
</div><br clear="all">
<p><span class="font5">30 29</span></p>
<p><span class="font5">15 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;15 <sup>21</sup> 14 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;16</span></p>
<p><span class="font5"><sup>4</sup>0 &nbsp;&nbsp;01 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0000</span></p>
<div>
<p><span class="font5">The object appears correctly positioned</span></p>
</div><br clear="all">
<div>
<p><span class="font5">Object appears &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Object not displaying</span></p>
</div><br clear="all">
<div>
<p><span class="font5">incorrectly positioned</span></p>
</div><br clear="all">
<p><span class="font0">■</span><span class="font5"> 8 cm </span><span class="font0">■</span><span class="font5"> 10 cm 12 cm </span><span class="font0">■</span><span class="font5"> 14 cm </span><span class="font0">■</span><span class="font5"> 16 cm</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark30"></a><span class="font1" style="font-weight:bold;"><a name="bookmark31"></a>3.2. &nbsp;&nbsp;&nbsp;Black Box Testing</span></h4></li></ul>
<p><span class="font1">This test will be carried out by testing the 9 functions already described in the use case diagram. This test will be performed on several people and each test will be recorded Test Results obtained from each function tried, whether it has been successful or not. Black box testing of this system can be seen in table 2.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 2 </span><span class="font1">Black Box Testing</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font1">No</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Use Case Function</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Description</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Result</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">1</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Running NFTs with the ORB algorithm</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">It is the main process of the system which is to apply the ORB method In the NFT process series. This process is used to apply Augmented Reality technology</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">2</span></p></td><td style="vertical-align:top;">
<p><span class="font1">View the Home page</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">It is a process of displaying the home that will appear when the application first runs which has 3 main buttons, namely How to Use, About, Start</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Display the How To use page</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">It is a process of displaying a How to use page that contains the steps to use the application so that users do not have difficulties.</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">4</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Display the About page</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">It is a process of displaying an about page containing an application description and author biodata</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Displaying 3D objects</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Is the process of displaying 3D objects above a predetermined target as an application of Augmented Reality</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">6</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Displays marker turnover</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">It is the process of displaying the marker substitution page that was used when the first start process appeared</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font1">7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">View an AR page</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">It is the process of displaying a page that is used to turn on the camera and display 3D objects</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">8</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Turning on the Camera</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">It is the process of turning on the camera so that it can be used to see the target and make 3D objects appear in Real-Time</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">9</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Retrieving and storing markers</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Is a process to retrieve marker data entered by the user. Then the marker is saved as a new marker by the system</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">True</span></p></td></tr>
</table>
<p><span class="font1">The results of the black box testing of this system are very satisfactory. This can be seen from each function that the 9 functions in the use case diagram can fit all. It can be concluded that this black box test was successful with 100% accuracy.</span></p>
<ul style="list-style:none;"><li>
<h4><a name="bookmark32"></a><span class="font1" style="font-weight:bold;"><a name="bookmark33"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h4></li></ul>
<p><span class="font1">From the research that has been carried out, several conclusions can be drawn as follows:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">1. &nbsp;&nbsp;&nbsp;From virtual object testing, it can be seen that the farther the distance, the more likely the accuracy of the object decreases. Testing the objects of this system produced quite good results, judging from the 3D objects that continue to appear at close range, although at long distances they get quite poor results. Testing for objects to perform correctly positioned earned a percentage of 52%. Meanwhile, testing for objects to appear incorrectly positioned got a percentage of 34%. The last test for non-performing objects got a percentage of 10.67%.</span></p></li>
<li>
<p><span class="font1">2. &nbsp;&nbsp;&nbsp;From the black box test, it can be concluded that it gets 100% accuracy because the overall functional features of the application tested through the application can run as expected. Features tested according to the features described in the 9-feature Use Case diagram.</span></p></li></ul>
<h4><a name="bookmark34"></a><span class="font1" style="font-weight:bold;"><a name="bookmark35"></a>References</span></h4>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;Sulaiman, R. (2017). Gerhana dan Keharusan Kosmologis Manusia: Tinjauan Filsafat Wujud. Jurnal Kependidikan Dan Sosial Keagamaan.</span></p></li>
<li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;Khazin, M. (2004). Ilmu Falak Dalam Teori dan Praktek. Yogyakarta: Buana Pustaka.</span></p></li>
<li>
<p><span class="font1">[3] &nbsp;&nbsp;&nbsp;Siregar, S. (2017). Fisika Tata Surya.</span></p></li>
<li>
<p><span class="font1">[4] &nbsp;&nbsp;&nbsp;Azuma, R. T. (1997). A Survey of Augmented Reality. Malibu: Hughes Research Laboratories.</span></p></li>
<li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;Ćuković, S., Gattullo, M., Pankratz, F., &amp;&nbsp;Devedžić, G. (2015). Marker Based vs. Natural Feature Tracking Augmented Reality Visualization of the 3D Foot Phantom.</span></p></li>
<li>
<p><span class="font1">[6] &nbsp;&nbsp;&nbsp;Hamidia, M., Zenati-Henda, N., Belghit, H., &amp;&nbsp;Bellarbi, A. (2015). Object Recognition Based on ORB Descriptor for Markerless Augmented Reality. 9ème Conférence sur le Génie Electrique.</span></p></li>
<li>
<p><span class="font1">[7] &nbsp;&nbsp;&nbsp;Carmigniani, J., &amp;&nbsp;Furht, B. (2011). Handbook Of Augmented Reality. Florida: Springer-Verlag New York.</span></p></li>
<li>
<p><span class="font1">[8] &nbsp;&nbsp;&nbsp;Jakubovic, A., &amp;&nbsp;Velagic, J. (2018). Image Feature Matching and Object Detection Using BruteForce Matchers. Conference: 2018 International Symposium ELMAR. Zadar: Institute of Electrical and Electronics Engineers ( IEEE ).</span></p></li>
<li>
<p><span class="font1">[9] &nbsp;&nbsp;&nbsp;Wahyudi, N., Harianto, R. A., &amp;&nbsp;Endang Setyati, D. I. (2019). Augmented Reality Marker Based Tracking Visualisasi Drawing 2D ke dalam Bentuk 3D dengan Metode FAST Corner Detection. Journal of Intelligent Systems and Computation.</span></p></li>
<li>
<p><span class="font1">[10] &nbsp;&nbsp;&nbsp;Luo, C., Yang, W., Huang, P., &amp;&nbsp;Zhou, J. (2019). Overview of Image Matching Based on ORB Algorithm. ICSP 2019. Weihai: IOP Publishing</span></p></li></ul>
<p><span class="font1" style="font-style:italic;">This page is intentionally left blank.</span></p>
<p><span class="font1">652</span></p>