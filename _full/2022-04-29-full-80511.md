---
layout: full_article
title: "XGBOOST DENGAN RANDOM SEARCH HYPER-PARAMETER TUNING UNTUK KLASIFIKASI SITUS PHISING"
author: "Muhammad Ryan Afrizal"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-80511 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-80511"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-80511"  
comments: true
---

<p><span class="font0">Jurnal Ilmu Komputer VOL. 15. Nomor 1</span></p>
<p><span class="font0">p-ISSN: 1979-5661</span></p>
<p><span class="font0">e-ISSN: 2622-321X</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font1" style="font-weight:bold;font-style:italic;"><a name="bookmark1"></a>XGBOOST</span><span class="font1" style="font-weight:bold;"> DENGAN </span><span class="font1" style="font-weight:bold;font-style:italic;">RANDOM SEARCH HYPERPARAMETER TUNING</span><span class="font1" style="font-weight:bold;"> UNTUK KLASIFIKASI SITUS </span><span class="font1" style="font-weight:bold;font-style:italic;">PHISING</span></h1>
<p><span class="font0">Muhammad Ryan Afrizal<sup>a1</sup></span><span class="font0" style="font-weight:bold;">, </span><span class="font0">Muliadi<sup>a2</sup></span><span class="font0" style="font-weight:bold;">, </span><span class="font0">Radityo Adi Nugroho<sup>a3</sup>, Dwi Kartini<sup>a4</sup>, Rudy Herteno<sup>a5</sup></span></p>
<p><span class="font0"><sup>a</sup>Ilmu Komputer Fakultas Matematika dan Ilmu Alam Universitas Lambung Mangkurat Jalan Ahmad Yani Km. 36, Banjarbaru, Kalimantan Selatan, Indonesia </span><a href="mailto:11711016210020@mhs.ulm.ac.id"><span class="font0"><sup>1</sup>1711016210020@mhs.ulm.ac.id</span></a><span class="font0"> </span><a href="mailto:2muliadi@ulm.ac.id"><span class="font0"><sup>2</sup>muliadi@ulm.ac.id</span></a><span class="font0"> </span><a href="mailto:3radityo.adi@ulm.ac.id"><span class="font0"><sup>3</sup>radityo.adi@ulm.ac.id</span></a><span class="font0"> </span><a href="mailto:4dwikartini@ulm.ac.id"><span class="font0"><sup>4</sup>dwikartini@ulm.ac.id</span></a></p>
<p><a href="mailto:5rudy.herteno@ulm.ac.id"><span class="font0"><sup>5</sup>rudy.herteno@ulm.ac.id</span></a></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font0" style="font-style:italic;">Phishing is a form of cyber crime that harms other people and includes acts that are against the law. There are several approaches to combating phishing crimes, one of which is by classifying phishing websites using machine learning methods. The dataset used is a phishing websites dataset from the UCI Repository with 11055 data and 30 categorical features. The classifier method used is XGBoost. XGBoost is good for classifying data with categorical features, but the performance of this algorithm can still be improved. To overcome these problems, researchers used a hyper-parameter tuning solution. XGBoost has several hyper-parameters that can be configured to improve the performance of the model. The problem of identifying good values for hyper-parameters is called hyper-parameter tuning. The hyper-parameter tuning method used is Random Search which is then validated using 5-Fold Cross Validation for 30 iterations. The configured XGBoost hyper-parameters include n_estimators, max_depth, subsample and learning_rate. Testing on XGBoost without hyperparameter tuning obtained an accuracy of 95.34%. Testing on XGBoost with hyperparameter tuning obtained an accuracy of 97.69%. Hyper-parameter tuning with Random Search on XGBoost for phishing websites classification provides improved model performance at an accuracy of about 2.35%.</span></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font0" style="font-style:italic;">Phishing, XGBoost, Hyper-parameter Tuning, Random Search, Categorical Feature.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font0" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Pendahuluan</span></h2></li></ul>
<p><span class="font0" style="font-style:italic;">Phising</span><span class="font0"> adalah salah satu bentuk </span><span class="font0" style="font-style:italic;">cyber crime yang</span><span class="font0"> menggunakan website palsu dimana tampilannya mirip dengan website aslinya. Website palsu ini digunakan untuk memancing korban memasukkan informasi pribadi rahasia sehingga </span><span class="font0" style="font-style:italic;">phiser</span><span class="font0"> dapat mencuri informasi rahasia tersebut. </span><span class="font0" style="font-style:italic;">Phising</span><span class="font0"> merupakan tindak pidana yang sangat merugikan dan termasuk perbuatan yang melawan hukum. Kurangnya pengetahuan pengguna terhadap ciri-ciri website </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> sehingga terjebak pada website palsu merupakan faktor penyebab terjadinya </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> [1]. Terdapat beberapa pendekatan untuk memberantas kejahatan </span><span class="font0" style="font-style:italic;">phising</span><span class="font0">, salah satunya dengan melakukan klasifikasi terhadap situs </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> menggunakan metode </span><span class="font0" style="font-style:italic;">machine learning</span><span class="font0">. Dataset yang umum digunakan yaitu dataset situs </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> dari </span><span class="font0" style="font-style:italic;">UCI Repository</span><span class="font0"> dengan 11055 data dan 30 fitur kategorial [2].</span></p>
<p><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> merupakan algoritma klasifikasi berbasis </span><span class="font0" style="font-style:italic;">decision tree</span><span class="font0"> yang menerapkan teknik </span><span class="font0" style="font-style:italic;">ensemble boosting. XGBoost</span><span class="font0"> memiliki kinerja yang baik pada data dengan fitur kategorikal dan</span></p>
<p><span class="font0">tidak terlalu berpengaruh terhadap data dengan kelas tidak seimbang [3]. </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> memiliki keunggulan dalam hal kecepatan dan penggunaan memori. Pemanfaatan cache prosesor yang lebih baik, pemrosesan multicore dan komputasi paralel terdistribusi membuat sistem dapat berjalan lebih cepat daripada algoritma populer yang umum digunakan [4]. Kinerja dari </span><span class="font0" style="font-style:italic;">XGBoost </span><span class="font0">masih dapat ditingkatkan salah satunya dengan cara menyetel nilai </span><span class="font0" style="font-style:italic;">Hyper-parameter</span><span class="font0">. </span><span class="font0" style="font-style:italic;">XGBoost </span><span class="font0">merupakan salah satu algoritma yang memiliki banyak </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> dan konfigurasi </span><span class="font0" style="font-style:italic;">hyperparameter</span><span class="font0"> yang tepat dapat meningkatkan kinerja dari </span><span class="font0" style="font-style:italic;">XGBoost.</span><span class="font0"> Masalah mengidentifikasi nilai yang baik untuk </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> disebut optimasi </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> atau </span><span class="font0" style="font-style:italic;">hyper-parameter tuning.</span></p>
<p><span class="font0" style="font-style:italic;">Random search</span><span class="font0"> merupakan teknik </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> yang memilih konfigurasi berdasarkan ruang </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> secara acak. Teknik ini sepenuhnya acak dan tidak menggunakan kecerdasan dalam memilih titik percobaan [5]. Kinerja model menggunakan </span><span class="font0" style="font-style:italic;">Random search </span><span class="font0">setara dengan kinerja yang diperoleh menggunakan teknik </span><span class="font0" style="font-style:italic;">hyper-parameter tuning meta heuristik </span><span class="font0">seperti </span><span class="font0" style="font-style:italic;">Genetic Algorithm, PSO</span><span class="font0"> dan </span><span class="font0" style="font-style:italic;">EDA</span><span class="font0">, tetapi dengan komputasi yang lebih cepat dan efisien pada ruang </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> berdimensi tinggi, sehingga </span><span class="font0" style="font-style:italic;">random search</span><span class="font0"> cocok diterapkan pada </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> yang memiliki banyak </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> [6].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font0" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Metodologi Penelitian</span></h2></li></ul>
<p><span class="font0">Alur dari penelitian dari klasifikasi situs </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> menggunakan </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> secara sistematis terdiri dari pengumpulan data, modeling dan evaluasi yang di presentasikan pada gambar 1.</span></p><img src="https://jurnal.harianregional.com/media/80511-1.jpg" alt="" style="width:230pt;height:290pt;">
<p><span class="font0" style="font-weight:bold;">Gambar 1. </span><span class="font0">Desain alur penelitian</span><span class="font0" style="font-style:italic;">.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font0" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Pengumpulan Data</span></h2></li></ul>
<p><span class="font0">Dataset pada penelitian ini menggunakan </span><span class="font0" style="font-style:italic;">Phising</span><span class="font0"> Website Data Set dari </span><span class="font0" style="font-style:italic;">UCI Machine Learning Repository.</span><span class="font0"> Dataset ini memiliki 11.055 instance dengan 30 fitur kategorikal dengan nilai -1, 0 dan 1 dan memiliki dua kelas yang terdiri dari 6.157 kelas </span><span class="font0" style="font-style:italic;">non-phising</span><span class="font0"> yang dilambangkan dengan angka 1 dan 4.898 kelas </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> yang dilambangkan dengan angka -1. Perbandingan persentase</span></p>
<p><span class="font0">kelas pada dataset ini yaitu kelas </span><span class="font0" style="font-style:italic;">non-phising</span><span class="font0"> sebesar 55,7% dan kelas </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> sebesar 44,3%, sehingga termasuk data yang seimbang. Dataset ini sudah bersih sehingga tidak dilakukan proses </span><span class="font0" style="font-style:italic;">preprocessing</span><span class="font0">.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font0" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Modeling</span></h2></li></ul>
<p><span class="font0">Tahapan awal dari modeling yaitu membagi data. 80% dari total data digunakan sebagai data training, dan 20% dari total data digunakan sebagai data testing. Pembagian data dilakukan secara stratifikasi, sehingga proporsi perbandingan kelas pada </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> dan </span><span class="font0" style="font-style:italic;">data testing </span><span class="font0">sama. </span><span class="font0" style="font-style:italic;">Data training</span><span class="font0"> berfungsi sebagai data untuk melatih model dan </span><span class="font0" style="font-style:italic;">data testing</span><span class="font0"> merupakan representasi dari data masa depan yang akan diprediksi kelasnya. </span><span class="font0" style="font-style:italic;">Data testing</span><span class="font0"> merupakan </span><span class="font0" style="font-style:italic;">unseen data</span><span class="font0"> yang digunakan untuk mengetahui kinerja dari model yang telah dilatih.</span></p>
<p><span class="font0" style="font-weight:bold;">Tabel 1. </span><span class="font0">Pembagian Data</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font0">Data</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Kelas </span><span class="font0" style="font-style:italic;">Phising</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Kelas Non-</span><span class="font0" style="font-style:italic;">Phising</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Total Instance</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0" style="font-style:italic;">Data training</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">4.926</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">3.918</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">8.844</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0" style="font-style:italic;">Data testing</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">1.231</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">980</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">2.211</span></p></td></tr>
</table>
<p><span class="font0">Modeling pada penelitian ini terbagi menjadi dua percobaan yaitu pertama melakukan klasifikasi menggunakan </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0">, dan yang ke dua melakukan klasifikasi menggunakan </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> menggunakan </span><span class="font0" style="font-style:italic;">Random search</span><span class="font0">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">a. &nbsp;&nbsp;&nbsp;Klasifikasi </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">Hyper-parameter Tuning</span></p></li></ul>
<p><span class="font0">Setelah dataset dibagi, selanjutnya sebanyak 8.844 </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> digunakan untuk melakukan klasifikasi dengan </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0">. Pada tabel 2 diuraikan penjelasan </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> yang terdapat pada </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0">.</span></p>
<p><span class="font0" style="font-weight:bold;">Tabel 2</span><span class="font0">. </span><span class="font0" style="font-style:italic;">Hyper-parameter Default XGBoost</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font0">Nama</span></p></td><td style="vertical-align:top;">
<p><span class="font0">Deskripsi</span></p></td><td style="vertical-align:top;">
<p><span class="font0">Nilai</span></p>
<p><span class="font0">Default</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">n_estimator</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Jumlah pohon individu (</span><span class="font0" style="font-style:italic;">tree boosting)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">100</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">max_depth</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Kedalaman maksimum dari pohon individu</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">3</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">subsample</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Rasio </span><span class="font0" style="font-style:italic;">instance</span><span class="font0"> dari </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> yang digunakan untuk membuat pohon individu</span></p></td><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">learning_rate</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Penyusutan langkah yang digunakan dalam pembaruan model</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.1</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">gamma</span></p></td><td style="vertical-align:top;">
<p><span class="font0" style="font-style:italic;">Loss reduction</span><span class="font0"> minimum yang diperlukan untuk membuat partisi pada simpul daun dari pohon individu.</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">colsample_bytree</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Rasio fitur dari </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> yang digunakan untuk membuat pohon individu</span></p></td><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">min_child_weight</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Bobot minimum yang diperlukan untuk membuat sebuah daun pada pohon individu</span></p></td><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td></tr>
</table>
<p><span class="font0">Klasifikasi dilakukan dengan melatih model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> menggunakan seluruh </span><span class="font0" style="font-style:italic;">data training</span><span class="font0">, yaitu sebanyak 8.844 </span><span class="font0" style="font-style:italic;">instance</span><span class="font0">. Model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> kemudian dievaluasi untuk mengetahui kinerja dari model.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">b. &nbsp;&nbsp;&nbsp;Klasifikasi </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">Hyper-parameter Tuning</span></p></li></ul>
<p><span class="font0">Setelah dataset dibagi, selanjutnya selanjutnya sebanyak 8.844 </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> untuk melakukan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">random search</span><span class="font0">. </span><span class="font0" style="font-style:italic;">Random search</span><span class="font0"> merupakan teknik </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> yang memilih konfigurasi berdasarkan ruang </span><span class="font0" style="font-style:italic;">hyperparameter</span><span class="font0"> secara acak. Teknik ini sepenuhnya acak dan tidak menggunakan kecerdasan</span></p>
<p><span class="font0">dalam memilih titik percobaan. Pada penelitian ini </span><span class="font0" style="font-style:italic;">Random search</span><span class="font0"> dilakukan sebanyak 30 iterasi sehingga akan menghasilkan 30 kandidat </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0">, kemudian setiap kandidat </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> akan divalidasi dengan 5-</span><span class="font0" style="font-style:italic;">Fold Cross Validation</span><span class="font0">. Konfigurasi </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> optimal dipilih berdasarkan nilai akurasi </span><span class="font0" style="font-style:italic;">cross validation</span><span class="font0"> tertinggi dari semua kandidat yang telah dihasilkan. Pada tabel 3 dijelaskan </span><span class="font0" style="font-style:italic;">hyper-parameter XGBoost </span><span class="font0">yang dikonfigurasi menggunakan </span><span class="font0" style="font-style:italic;">random search</span><span class="font0">.</span></p>
<p><span class="font0" style="font-weight:bold;">Tabel 3. </span><span class="font0">Domain Pencarian </span><span class="font0" style="font-style:italic;">Hyper-parameter</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font0">Nama</span></p></td><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font0">Deskripsi</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font0">Domain</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">Min</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Maks</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">n_estimator</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Jumlah pohon individu (</span><span class="font0" style="font-style:italic;">tree boosting)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">200</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">max_depth</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Kedalaman maksimum dari pohon individu</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">10</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">subsample</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Rasio </span><span class="font0" style="font-style:italic;">instance</span><span class="font0"> dari </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> yang digunakan untuk membuat pohon individu</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.25</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.75</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">learning_rate</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Penyusutan langkah yang digunakan dalam pembaruan model</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.01</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.5</span></p></td></tr>
</table>
<p><span class="font0">Konfigurasi </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> optimal kemudian digunakan untuk melatih ulang model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> menggunakan keseluruhan dari </span><span class="font0" style="font-style:italic;">data training</span><span class="font0">. Model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyperparameter tuning</span><span class="font0"> kemudian dievaluasi untuk mengetahui kinerja dari model.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font0" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Evaluasi</span></h2></li></ul>
<p><span class="font0">Evaluasi merupakan tahapan untuk menilai kinerja dari model yang dihasilkan. Untuk mengetahui kinerja dari model, pada penelitian ini model dievaluasi menggunakan akurasi, recall dan presisi.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font0" style="font-weight:bold;"><a name="bookmark13"></a>3. &nbsp;&nbsp;&nbsp;Hasil Dan Pembahasan</span><br><br><span class="font0" style="font-weight:bold;"><a name="bookmark14"></a>3.1. &nbsp;&nbsp;&nbsp;Hasil</span></h2></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font0">a. &nbsp;&nbsp;&nbsp;Klasifikasi </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">Hyper-parameter Tuning</span></p></li></ul>
<p><span class="font0">Model </span><span class="font0" style="font-style:italic;">XGBoost tanpa hyper-parameter tuning</span><span class="font0"> dilakukan training menggunakan </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> kemudian model dievaluasi untuk mengetahui kinerja dari model. Adapun untuk hasil kinerja yang dihasilkan pada model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning </span><span class="font0">disajikan pada gambar 2.</span></p><img src="https://jurnal.harianregional.com/media/80511-2.jpg" alt="" style="width:308pt;height:174pt;">
<p><span class="font0" style="font-weight:bold;">Gambar 2</span><span class="font0">. Kinerja model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">b. &nbsp;&nbsp;&nbsp;Klasifikasi XGBoost dengan Hyper-parameter Tuning</span></p></li></ul>
<p><span class="font0">Hyper-parameter tuning dengan random search dilakukan sebanyak 30 iterasi sehingga menghasilkan 30 kandidat hyper-parameter secara acak. Pada tabel 4 disajikan kandidat hyper-parameter yang dihasilkan dari random search dengan 30 iterasi</span></p>
<p><span class="font0" style="font-weight:bold;">Tabel 4. </span><span class="font0">Kandidat </span><span class="font0" style="font-style:italic;">hyper-parameter</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font0">Iterasi</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Learning_rate</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Max_depth</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">N_estimators</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Subsample</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">0</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.193524658</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">8</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">189</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.548425079</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.228458049</span></p></td><td style="vertical-align:top;">
<p><span class="font0">7</span></p></td><td style="vertical-align:top;">
<p><span class="font0">75</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.479624446</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">2</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.173517219</span></p></td><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">152</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.575444236</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.037641674</span></p></td><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">158</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.356169555</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">4</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.099094234</span></p></td><td style="vertical-align:top;">
<p><span class="font0">5</span></p></td><td style="vertical-align:top;">
<p><span class="font0">161</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.402121121</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">5</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.267130651</span></p></td><td style="vertical-align:top;">
<p><span class="font0">9</span></p></td><td style="vertical-align:top;">
<p><span class="font0">49</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.51238733</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">6</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.205931876</span></p></td><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">108</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.507117219</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">7</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.300283139</span></p></td><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">51</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.590153769</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.230744633</span></p></td><td style="vertical-align:top;">
<p><span class="font0">2</span></p></td><td style="vertical-align:top;">
<p><span class="font0">132</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.721100878</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">9</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.286011227</span></p></td><td style="vertical-align:top;">
<p><span class="font0">10</span></p></td><td style="vertical-align:top;">
<p><span class="font0">9</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.257983126</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">10</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.123137975</span></p></td><td style="vertical-align:top;">
<p><span class="font0">7</span></p></td><td style="vertical-align:top;">
<p><span class="font0">172</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.497588455</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">11</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.026850375</span></p></td><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td><td style="vertical-align:top;">
<p><span class="font0">164</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.341118044</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">12</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.380127091</span></p></td><td style="vertical-align:top;">
<p><span class="font0">6</span></p></td><td style="vertical-align:top;">
<p><span class="font0">54</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.52335514</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">13</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.100578683</span></p></td><td style="vertical-align:top;">
<p><span class="font0">2</span></p></td><td style="vertical-align:top;">
<p><span class="font0">44</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.719749471</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">14</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.448465402</span></p></td><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">190</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.412665165</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">15</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.200451872</span></p></td><td style="vertical-align:top;">
<p><span class="font0">2</span></p></td><td style="vertical-align:top;">
<p><span class="font0">111</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.664368755</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">16</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.18480913</span></p></td><td style="vertical-align:top;">
<p><span class="font0">9</span></p></td><td style="vertical-align:top;">
<p><span class="font0">157</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.65109849</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">17</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.046529815</span></p></td><td style="vertical-align:top;">
<p><span class="font0">7</span></p></td><td style="vertical-align:top;">
<p><span class="font0">9</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.636122385</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">18</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.107370684</span></p></td><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">63</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.657730714</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">19</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.356360098</span></p></td><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">163</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.635635173</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">20</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.046281879</span></p></td><td style="vertical-align:top;">
<p><span class="font0">7</span></p></td><td style="vertical-align:top;">
<p><span class="font0">41</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.707479838</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">21</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.426518903</span></p></td><td style="vertical-align:top;">
<p><span class="font0">2</span></p></td><td style="vertical-align:top;">
<p><span class="font0">33</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.281779175</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">22</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.162381338</span></p></td><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">37</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.614803089</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">23</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.322403161</span></p></td><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">193</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.441463437</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">24</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.486138927</span></p></td><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.610864761</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">25</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.125632611</span></p></td><td style="vertical-align:top;">
<p><span class="font0">7</span></p></td><td style="vertical-align:top;">
<p><span class="font0">27</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.511366415</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">26</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.219495099</span></p></td><td style="vertical-align:top;">
<p><span class="font0">10</span></p></td><td style="vertical-align:top;">
<p><span class="font0">124</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.303945713</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">27</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.025400301</span></p></td><td style="vertical-align:top;">
<p><span class="font0">7</span></p></td><td style="vertical-align:top;">
<p><span class="font0">52</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.531637786</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">28</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.350802882</span></p></td><td style="vertical-align:top;">
<p><span class="font0">5</span></p></td><td style="vertical-align:top;">
<p><span class="font0">151</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.55220869</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">29</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.274522135</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">13</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.288489955</span></p></td></tr>
</table>
<p><span class="font0">Untuk menentukan konfigurasi </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> mana yang paling optimal, setiap konfigurasi dilakukan validasi secara 5-Fold Cros Validation menggunakan seluruh </span><span class="font0" style="font-style:italic;">data training</span><span class="font0">. Adapun hasil dari 5-Fold Cross Validation dari setiap kandidat disajikan dalam bentuk diagram batang pada gamberGambar </span><span class="font0" style="font-weight:bold;">3</span><span class="font0">.</span></p><img src="https://jurnal.harianregional.com/media/80511-3.jpg" alt="" style="width:368pt;height:150pt;">
<p><span class="font0">Pada grafik diatas, garis vertikal mewakili rata-rata akurasi dari kandidat </span><span class="font0" style="font-style:italic;">hyper-parameter </span><span class="font0">atau validation score, sedangkan garis horizontal mewakili iterasi atau percobaan ke </span><span class="font0" style="font-style:italic;">n </span><span class="font0">dari </span><span class="font0" style="font-style:italic;">random search</span><span class="font0">. Dari hasil </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> menggunakan </span><span class="font0" style="font-style:italic;">random search </span><span class="font0">didapat akurasi 5-</span><span class="font0" style="font-style:italic;">Fold Cross Validation</span><span class="font0"> tertinggi pada iterasi ke 16 yaitu sebesar 97.14%, dengan nilai learning rate=0.18480913, max depth=9, n_estimator=157, subsample=0.65109849.</span></p>
<p><span class="font0">Setelah didapat nilai </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> optimal menggunakan </span><span class="font0" style="font-style:italic;">random search.</span><span class="font0"> Model </span><span class="font0" style="font-style:italic;">XGBoost dengan hyper-parameter tuning</span><span class="font0"> kemudian dilakukan final training menggunakan </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> beserta dengan konfigurasi </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> optimal, kemudian model dievaluasi untuk mengetahui kinerja dari model. Adapun untuk hasil kinerja yang dihasilkan pada model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> disajikan pada gambar 4.</span></p><img src="https://jurnal.harianregional.com/media/80511-4.jpg" alt="" style="width:294pt;height:174pt;">
<p><span class="font0" style="font-weight:bold;">Gambar 4. </span><span class="font0">Kinerja model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark15"></a><span class="font0" style="font-weight:bold;"><a name="bookmark16"></a>3.2. &nbsp;&nbsp;&nbsp;Pembahasan</span></h2></li></ul>
<p><span class="font0">Pada percobaan klasifikasi </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0">, model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyperparameter tuning</span><span class="font0"> dilatih menggunakan 8.844 </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> dan dievaluasi menggunakan 2.211 </span><span class="font0" style="font-style:italic;">data testing</span><span class="font0">. Hasil kinerja dari klasifikasi </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> yaitu diperoleh akurasi sebesar 95,34%, recall sebesar 93,78% dan presisi sebesar 95,63%.</span></p>
<p><span class="font0">Pada percobaan klasifikasi </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0">, 8.844 </span><span class="font0" style="font-style:italic;">data training </span><span class="font0">digunakan untuk melakukan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">random search</span><span class="font0">. </span><span class="font0" style="font-style:italic;">Random search </span><span class="font0">dilakukan sebanyak 30 iterasi sehingga akan menghasilkan 30 kandidat </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0">, kemudian setiap kandidat </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> divalidasi secara 5-Fold Cross Validation. Nilai </span><span class="font0" style="font-style:italic;">hyper-</span></p>
<p><span class="font0" style="font-style:italic;">parameter</span><span class="font0"> optimal dipilih berdasarkan kandidat dengan nilai validasi tertinggi. </span><span class="font0" style="font-style:italic;">Random search </span><span class="font0">dilakukan sebanyak 30 iterasi karena tidak mendapatkan hasil yang lebih baik ketika digunakan iterasi yang lebih dari 30. Pada tabel 5 disajikan perbedaan konfigurasi </span><span class="font0" style="font-style:italic;">hyper-parameter</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> dan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning.</span></p>
<p><span class="font0" style="font-weight:bold;">Tabel 5. </span><span class="font0">Perbedaan Konfigurasi </span><span class="font0" style="font-style:italic;">Hyper-parameter</span></p>
<p><span class="font0">Model &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Learning_rate &nbsp;Max_depth &nbsp;N_estimators &nbsp;Subsample</span></p>
<p><span class="font0">Tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;100 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1</span></p>
<p><span class="font0">Dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> &nbsp;0.18480913 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;157 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.65109849</span></p>
<p><span class="font0">Model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> kemudian dilakukan final training, yaitu model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> dilatih menggunakan 8.844 </span><span class="font0" style="font-style:italic;">data training</span><span class="font0"> dan dievaluasi menggunakan 2.211 </span><span class="font0" style="font-style:italic;">data testing</span><span class="font0">. Hasil kinerja dari klasifikasi </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> yaitu diperoleh akurasi sebesar 97,69%, recall sebesar 96,33% dan presisi sebesar 98,44%.</span></p>
<p><span class="font0">Untuk mengetahui pergaruh </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> pada kinerja </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dalam melakukan klasifikasi pada dataset situs </span><span class="font0" style="font-style:italic;">phising</span><span class="font0">, maka dilakukan perbandingan antara kinerja model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> dengan kinerja model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyperparameter tuning.</span><span class="font0"> Pada Gambar 5 dapat dilihat grafik perbandingan anatara kinerja model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> dengan kinerja model </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">hyper-parameter tuning.</span></p><img src="https://jurnal.harianregional.com/media/80511-5.jpg" alt="" style="width:400pt;height:174pt;">
<p><span class="font0" style="font-weight:bold;">Gambar 5. </span><span class="font0">Perbandingan kinerja model</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark17"></a><span class="font0" style="font-weight:bold;"><a name="bookmark18"></a>4. &nbsp;&nbsp;&nbsp;Kesimpulan</span></h2></li></ul>
<p><span class="font0">Klasifikasi situs </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> menggunakan </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> mendapatkan akurasi sebesar 95,34%, recall sebesar 93,78% dan presisi sebesar 95,63%. Klasifikasi situs </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> menggunakan </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> tanpa </span><span class="font0" style="font-style:italic;">hyper-parameter tuning</span><span class="font0"> mendapatkan akurasi sebesar 97,69%, recall sebesar 96,33% dan presisi sebesar 98,44%. </span><span class="font0" style="font-style:italic;">Hyper-parameter tuning</span><span class="font0"> dengan </span><span class="font0" style="font-style:italic;">random search</span><span class="font0"> pada </span><span class="font0" style="font-style:italic;">XGBoost</span><span class="font0"> untuk klasifikasi situs </span><span class="font0" style="font-style:italic;">phising</span><span class="font0"> memberikan peningkatan kinerja pada akurasi sekitar 2,35%, pada recall mengalami peningkatan sekitar 2,55% dan pada presisi mengalami peningkatan sekitar 2,81%.</span></p>
<h2><a name="bookmark19"></a><span class="font0" style="font-weight:bold;"><a name="bookmark20"></a>Daftar Pustaka</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font0">[1] . A. S. Gulo and K. Nawawi, “Cyber Crime dalam Bentuk </span><span class="font0" style="font-style:italic;">Phising</span><span class="font0"> Berdasarkan Undang-Undang Informasi dan Transaksi Elektronik” </span><span class="font0" style="font-style:italic;">PAMPAS: Journal of Criminal,</span><span class="font0"> vol. 1, no. 2, p. 68-81, 2020.</span></p></li>
<li>
<p><span class="font0">[2] . R. M. Mohammad, F. Thabtah and L. McCluskey, “Intelligent rule-based phishing websites classification</span><span class="font0" style="font-style:italic;">”</span><span class="font0"> IET Information Security, vol. 8, p.153-160, 2013</span></p></li>
<li>
<p><span class="font0">[3] . R. Pavan, M. Nara, S. Gopianth and N. Patil, “Bayesian Optimization and Gradient Boosting to Detect Phishing Websites” in </span><span class="font0" style="font-style:italic;">2021 55th Annual Conference on Information Sciences and Systems (CISS)</span><span class="font0">, Baltimore, MD, USA.</span></p></li>
<li>
<p><span class="font0">[4] . T. Chen and C. Guestrin, “XGBoost: A Scalable Tree Boosting System” in </span><span class="font0" style="font-style:italic;">KDD '16: The 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</span><span class="font0"> San Francisco California, USA, 2016, pp. 785-794.</span></p></li>
<li>
<p><span class="font0">[5] . J. Bergstra and Y. Bengio, “Random Search for Hyper-Parameter Optimization” </span><span class="font0" style="font-style:italic;">Journal of Machine Learning Research,</span><span class="font0"> vol. 13, no. 10, p. 281-305, 2012.</span></p></li>
<li>
<p><span class="font0">[6] . R. G. Mantovani, A. L. D. Rossi, J. Vanschoren, B. Bischl and A. C. P. L. F. Carvalho, “Effectiveness of Random Search in SVM Hyper-parameter Tuning” in </span><span class="font0" style="font-style:italic;">2015 International Joint Conference on Neural Networks (IJCNN),</span><span class="font0"> Killarney, Ireland, 2015</span><span class="font0" style="font-style:italic;">.</span></p></li></ul>
<p><span class="font0">47</span></p>