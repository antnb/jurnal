---
layout: full_article
title: "Implementation Equal-Width Interval Discretization in Naive Bayes Method  for Increasing Accuracy of Students' Majors Prediction"
author: "Alfa Saleh, Fina Nasari"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-38405 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-38405"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-38405"  
comments: true
---

<p><span class="font1" style="font-weight:bold;">LONTAR KOMPUTER VOL. 9, NO. 2, AUGUST 2018</span></p>
<p><span class="font1" style="font-weight:bold;">DOI : 10.24843/LKJITI.2018.v09.i02.p05</span></p>
<p><span class="font1" style="font-weight:bold;">Accredited B by RISTEKDIKTI Decree No. 51/E/KPT/2017</span></p>
<p><span class="font1" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font1" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font2" style="font-weight:bold;"><a name="bookmark1"></a>Implementation of Equal-Width Interval Discretization in Naive Bayes Method for Increasing Accuracy of Students' Majors Prediction</span></h1>
<p><span class="font1">Alfa Saleh<sup>a1</sup></span><span class="font1" style="font-weight:bold;">, </span><span class="font1">Fina Nasari<sup>a2</sup></span></p>
<p><span class="font1"><sup>a</sup>Faculty of Computer Science and Engineering, Potensi Utama University Jl. K.L Yos Sudarso KM 6.5 Tanjung Mulia Medan, Indonesia </span><a href="mailto:1alfa@potensi-utama.ac.id"><span class="font1"><sup>1</sup>alfa@potensi-utama.ac.id</span></a><span class="font1"> </span><a href="mailto:2fina@potensi-utama.ac.id"><span class="font1"><sup>2</sup>fina@potensi-utama.ac.id</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">The Selection of majors for students is a positive step that is done to focus students in accordance with their potential, it is considered important because with the majors, students are expected to develop academic ability according to the field of interest. In previous research, Naive Bayes method has been tested to classify the student’s department based on the criteria that support the case study on Private Madrasah Aliyah PAB 6 Helvetia students and the accuracy of the test from 100 student data is 90%. in this study, the researcher developed a previously used method by applying an equal-width interval discretization that would transform numerical or continuous criteria into a categorical criteria with a predetermined k value, different k values would be tested to find the best accuracy value. from the 120-student data that have been tested, it is proved that the result of the classification of the application of equal-width interval discretization on the Naive Bayes method with the value of k = 8 is better and increased the accuracy value 91.7% to 93.3%.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">Data Mining, Naive Bayes, Equal-Width Interval Discretization, Students’ Majors</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;INTRODUCTION</span></h2></li></ul>
<p><span class="font1">The role of education is very important in supporting the development of technology that almost has penetrated into all areas. It also affects the determination of majors for high school / equivalent students, where the determination of the student's department is a process to focus students in a particular area of the interested field, this is done so that each student can learn more in the subjects that are in accordance with the concentration which has been specified for the student. The problem is the ongoing system of private school Madrasah Aliyah PAB 2 Helvetia Medan, the place where researchers conduct research is not entirely effective because students are given a questionnaire to determine which majors they are interested in regardless of other criteria that may have a stake in determining eligibility students in terms of choosing majors. Through the process of determining the majors for students is an important step in preparing students to concentrate on the field that students are interested in when it should continue to the next education level. In the previous research, researchers also have done the process of mining to dig information about the determination of student majors using Naive Bayes method, the results of the research were tested 100 student data based on several criteria include the average score of natural science subjects, the average value of science social, classroom teacher recommendation and the questionnaire value filled by the students concerned. from the 100 data tested using the Naive Bayes method, it is obtained the accuracy value of determining student majors by 90% with an error of 10% [1] . The Naive Bayes method was chosen because it was widely implemented in various fields of science, as in the Xingxing Zhou research (2016), the Naive Bayes method was used to classify images to improve the accuracy of brain diagnosis using NMR imagery, where 94.5% sensitivity classification was obtained, 91.70% and the overall accuracy of 92.60 [2]. Naive Bayes is one of the top ten (10) data mining algorithms for simplicity and efficiency, as evidenced by the performance of Naive Bayes in classifying text [3], [4]. In addition, Naive Bayes is widely recognized as a simple and effective probabilistic classification method [5]–[7], and its performance is proportional to or higher than the decision tree [8] and artificial neural networks [9].</span></p>
<p><span class="font1">However, researchers wanted to expand their previous research by applying Unsupervised Discretization [10] to improve the performance of the Naive Bayes method so that the percentage of predicted accuracy results could increase compared to the previous one. Where Unsupervised Discretization techniques in transforming numerical criteria / attributes are excellent [11].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1" style="font-weight:bold;">2. &nbsp;&nbsp;&nbsp;Research Methods</span></p>
<ul style="list-style:none;">
<li>
<p><span class="font1" style="font-weight:bold;">2.1. &nbsp;&nbsp;&nbsp;Naïve Bayes</span></p></li></ul></li></ul>
<p><span class="font1">Naive Bayes is a model-based classification method and offers competitive classification performance compared with other data-driven classification methods [12]–[15], such as neural network, support vector machine (SVM), logistic regression, and k-nearest neighbors. The naive Bayes applies the Bayes’ theorem with the “naive” assumption that any pair of features is independent for a given class. The classification decision is made based upon the maximum-a-posteriori (MAP) rule. Usually, three distribution models, including Bernoulli model, multinomial model and Poisson model, have commonly been incorporated into the Bayesian framework and have resulted in classifiers of Bernoulli naive Bayes (BNB), multinomial naive Bayes (MNB) and Poisson naive Bayes (PNB), respectively[4]. The formula of Bayes's theorem is [16]:</span></p>
<p><span class="font6" style="font-style:italic;">P</span><span class="font5">(</span><span class="font6" style="font-style:italic;">H</span><span class="font5">|</span><span class="font6" style="font-style:italic;">X</span><span class="font5">)= </span><span class="font3"><sup>P</sup>- </span><span class="font3" style="text-decoration:underline;"><sup>(</sup></span><span class="font3"><sup> X</sup>- </span><span class="font3" style="text-decoration:underline;">| ). (</span><span class="font3"> </span><span class="font6" style="font-style:italic;">1</span><span class="font6"> </span><span class="font6" style="text-decoration:underline;"><sup>)</sup></span></p>
<div>
<p><span class="font5">(1)</span></p>
</div><br clear="all">
<p><span class="font4" style="font-style:italic;">p</span><span class="font3">(</span><span class="font4" style="font-style:italic;">X</span><span class="font3">)</span></p>
<p><span class="font1">Where variable X represents Data with unknown class, H represents The data hypothesis is a specific class, P (H|X) represents The probability of hypothesis H is based on condition X (posterior probability), P (H) represents Hypothesis probability H (prior probability), while P (X|H) represents The probability of X is based on the conditions in hypothesis H and P (X) represents Probability X. Therefore, the method of Naive Bayes above is adjusted as follows:</span></p>
<div>
<p><span class="font6" style="font-style:italic;">P</span><span class="font5">(</span><span class="font6" style="font-style:italic;">C</span><span class="font5">|</span><span class="font6" style="font-style:italic;">F</span><span class="font5">1…</span><span class="font6" style="font-style:italic;">Fn</span><span class="font5">)</span></p>
</div><br clear="all">
<div>
<p><span class="font6" style="font-style:italic;">P</span><span class="font5" style="text-decoration:underline;">(</span><span class="font6" style="font-style:italic;">C</span><span class="font5" style="text-decoration:underline;">)</span><span class="font6" style="font-style:italic;">P</span><span class="font5" style="text-decoration:underline;">(</span><span class="font6" style="font-style:italic;">F</span><span class="font5" style="text-decoration:underline;">1…</span><span class="font6" style="font-style:italic;">Fn</span><span class="font5"> </span><span class="font5" style="text-decoration:underline;">|</span><span class="font6" style="font-style:italic;">C</span><span class="font5" style="text-decoration:underline;">) </span><span class="font6" style="font-style:italic;">P</span><span class="font5">(</span><span class="font6" style="font-style:italic;">F</span><span class="font5">1…</span><span class="font6" style="font-style:italic;">Fn</span><span class="font5">)</span></p>
</div><br clear="all">
<div>
<p><span class="font5">(2)</span></p>
</div><br clear="all">
<p><span class="font1">Where Variable C represents the class, while the F1 ... Fn represents the characteristics of the user for the classification process. Therefore, the above formula can also be written simply as follows:</span></p>
<p><span class="font6" style="font-style:italic;">prior x Iikeli</span><span class="font5">ℎ</span><span class="font6" style="font-style:italic;">ood</span></p>
<p><span class="font6" style="font-style:italic;">Posterior</span><span class="font5"> = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)</span></p>
<p><span class="font6" style="font-style:italic;">evidence</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1" style="font-weight:bold;">2.2. &nbsp;&nbsp;&nbsp;Unsupervised Discretization</span></p></li></ul>
<p><span class="font1">Discretization is the process of converting a continuous attribute value into a limited number of intervals and associated with each interval with a discrete numerical value. Discretization process is carried out before the learning process [17]</span><span class="font5">. </span><span class="font1">Among the methods of Unsupervised Discretization, there are several simple methods. (Equal-width Interval Discretization and equalfrequency Interval Discretization) and more sophisticated, based on clustering analysis, such as k-means discretization. The Continuous range is divided into subranges by user-specified width or Frequency[18]. But in this study, researchers used Equal-width interval Discretization technique, which is the simplest discretization method that divides the observed range of values in each feature / attribute. The process involves sorting the observed values of the continuous feature / attribute and finding the minimum (Vmin) and maximum (Vmax) values. The interval can be calculated by dividing the observed range of values for the variables into k of the same size using the following formula [18].</span></p>
<p><span class="font6" style="font-style:italic;">V,</span><span class="font5"> </span><span class="font5" style="text-decoration:underline;">-</span></p>
<p><a href="#bookmark4"><span class="font4" style="font-style:italic;"><sub>τ</sub> , &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sub>1</sub> <sup>v</sup>max <sup>v</sup>, ..</span></a></p>
<p><a href="#bookmark5"><span class="font6" style="font-style:italic;">Interval</span><span class="font5"> =(4)</span></a></p>
<p><a href="#bookmark6"><span class="font6" style="font-style:italic;">Boundaries</span><span class="font5"> = &nbsp;&nbsp;&nbsp;&nbsp;+ ( </span><span class="font6" style="font-style:italic;">i x Interval</span><span class="font5">)(5)</span></a></p>
<p><span class="font1">Then the limits can be constructed for i = 1 ... k-1 using the above equation. This type of discretization does not depend on multi-relational data structures. However, this discretization method is sensitive to outliers that can drastically reduce the range. The limitations of this method are given by the uneven distribution of data points: some intervals may contain more data points than others.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark7"></a><span class="font1" style="font-weight:bold;"><a name="bookmark8"></a>2.3. &nbsp;&nbsp;&nbsp;Research Stages</span></h2></li></ul>
<p><span class="font1">In the Naïve Bayes method, the constant (categorical) String data is distinguished from continuous numerical data, this difference will be seen when determining the probability value of each criterion whether it is a criterion with a string data value or a criterion with a numeric data value. The stages of applying the method of Naive Bayes in this study can be seen in Figure 1 below.</span></p><img src="https://jurnal.harianregional.com/media/38405-1.jpg" alt="" style="width:325pt;height:263pt;">
<p><span class="font1" style="font-weight:bold;">Figure 1. </span><span class="font1">Research Stages of Equal-Width Interval Discretization on Naive Bayes</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark9"></a><span class="font1" style="font-weight:bold;"><a name="bookmark10"></a>2.3.1. &nbsp;&nbsp;&nbsp;Data Collection</span></h2></li></ul>
<p><span class="font1">The data that will be used as training data is the academic data of the students as respondents, where the sample of student data is taken as much as 120 data, they consist of The students’ academic data such as the score of Mathematics, Physics, Chemistry, Biology, Economics, Geography, History and Sociology ,the questionnaire that is filled by students and recommendation from the homeroom.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark11"></a><span class="font1" style="font-weight:bold;"><a name="bookmark12"></a>2.3.2. &nbsp;&nbsp;&nbsp;Data Cleaning</span></h2></li></ul>
<p><span class="font1">In the process of data cleaning, the data that eventually used in this research is the exact value of subjects, non-exact subjects, a recommendation from the homeroom, and questionnaires filled by students.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark13"></a><span class="font1" style="font-weight:bold;"><a name="bookmark14"></a>2.3.3. &nbsp;&nbsp;&nbsp;Determining the Criteria</span></h2></li></ul>
<p><span class="font1">The criteria that used based on data that has been collected is as in table 1 below:</span></p>
<p><span class="font1" style="font-weight:bold;">Table 1. </span><span class="font1">Criteria</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font1">NO</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Criterion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Type of Criterion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>Value</sup></span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">The average score of exact</span></p>
<p><span class="font1">Numerical/Continuous &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 - 100</span></p>
<p><span class="font1">subjects</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">The average score of non-exact</span></p>
<p><span class="font1">Numerical/Continuous &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 - 100</span></p>
<p><span class="font1">subjects</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Recommendation &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Categorical &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>Science,</sup></span></p>
<p><span class="font1">Social Studies</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">4</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Questionnaire &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Science,</span></p>
<p><span class="font1">Categorical &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>,</sup></span></p>
<p><span class="font1">Social Studies</span></p></td></tr>
</table>
<p><span class="font1">There are four (4) criteria used in this research, namely the average score of exact subjects, the average value of non-exact subjects, recommendation and lift. Two (2) of them are numerical / continuous criteria and two (2) categorical criteria. To improve the accuracy of the Naive Bayes method, discretization is performed using unsupervised discretization techniques on numerical / continuous criteria, the goal is to transform numerical/continuous criteria into categorical criteria using formulas 4 and 5. The following table 2 discriminates numerical criteria / continuous.</span></p>
<p><span class="font1" style="font-weight:bold;text-decoration:underline;">Table 2. </span><span class="font1" style="text-decoration:underline;">The results of Discretization with k=8</span></p>
<p><span class="font1">Numerical/Continuous Criteria</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font1">The average score of exact subjects</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">The average score of non-exact subjects</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">&lt;71.9125</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">&lt;71.875</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">71.9125 – 73.825</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">71.875 – 73.75</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">73.825 – 75.7375</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">73.75 – 75.625</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">75.7375 – 77.65</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">75.625 – 77.5</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">77.65 – 79.5625</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">77.5 – 79.375</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">79.5625 – 81.475</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">79.375 – 81.25</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">81.475 – 83.3875</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">81.25 – 83.125</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">83.3875&gt;</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">83.125&gt;</span></p></td></tr>
</table>
<p><span class="font1">In table 2 above, you can see the results of the discretization process using the Unsupervised Discretization technique. Where the criteria / attributes of The average values of exact and nonexact subjects with numerical or continuous type are transformed into categorical criteria with 8 categories. The first category is the average value of exact sciences that are below 71.9125, the second category is the average value of exact subjects which are between 71.9125-73.825, the third category is the average value of exact subjects which are between 73.825- 75.7375, the fourth category is the average value of exact subjects that are between 75.7375-77.65, the fifth category is the average value of exact subjects that are between 77.65-79.5625, the sixth category is the average value of exact subjects that are between 79.5625-81.475, the seventh category is the average value of exact subjects which are between 81.475-83.3875, and the eighth category is the average value of exact sciences that are above 83.3875.</span></p>
<p><span class="font1">Furthermore, the results of the discretization of the criteria for the average value of non-exact subjects are also divided into 8 categories, where the first category is the average value of nonexact subjects under 71,875, the second category is the average value of non-exact subjects -acts that are between 71,875-73,75, the third category is the average value of non-exact subjects that are between 73.75-75.625, the fourth category is the average value of non-exact subjects that are between 75.625-77.5, the fifth category is the average value of non-exact subjects that are between 77.5-79.375, the sixth category is the average value of non-exact subjects that are between 79.375-81.25, the seventh category is the average value of non-exact subjects between 81.25-83.125, and the eighth category are the average values of non-exact subjects above 83.125.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark15"></a><span class="font1" style="font-weight:bold;"><a name="bookmark16"></a>2.3.4. &nbsp;&nbsp;&nbsp;The Probability of Each Criterion</span></h2></li></ul>
<p><span class="font1">Several criteria have been set as a reference in classifying students' majors using Unsupervised Discretization techniques on the Naive Bayes method. The next step, determining the probability value of each criterion, for example, the probability value of the average scores of the exact scores of subjects to be shown is the probability value with the value k = 8.</span></p>
<p><span class="font1">Here the value of probability criteria of the average value of the exact sciences can be seen in table 3.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 3. </span><span class="font1">The Probability of The average score of exact subjects with k=8</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">The Average Score of Exact Subject</span></p></td><td colspan="2" style="vertical-align:middle;">
<p><span class="font1">Probability</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Science</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Social Studies</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">&lt;71.9125</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.067</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.283</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">71.9125 – 73.825</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.05</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.133</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">73.825 – 75.7375</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.2</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.2</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">75.7375 – 77.65</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.017</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.05</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">77.65 – 79.5625</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.033</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.033</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">79.5625 – 81.475</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.217</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.15</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">81.475 – 83.3875</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.133</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.1</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">83.3875&gt;</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.283</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.050</span></p></td></tr>
</table>
<p><span class="font1">from table 3 above, there were 60 students placed in the science studies major and 60 students were placed in the social studies major . Based on these data, there were 4 students with the average value of exact subjects below 71.9125 placed in the science studies major and the probability value of 0.067, 3 student with an average value of exact subjects between 71.912573.825 placed in the science studies major and the probability value of 0.05 , 12 students with the average value of exact subjects between 73.825-75.7375 are placed in the science studies major and the probability value is 0.2, 1 student with an average value of exact subjects between 75.7375-77.65 is placed in the science studies major and the probability value is 0.017, 2 students with the average value of exact subjects between 77.65-79.5625 are placed in the science studies major and the probability value is 0.033, 13 students with the the average value of exact subjects between 79.5625-81.475 are placed in the science studies major and the probability value is 0.217, 8 students with the average value of exact subjects between 81,475-83.3875 is placed in the science studies major and the probability value is 0.133, 17 students with the average value of exact subjects above 83.3875 are placed in the science studies major and the probability value is 0.283. Meanwhile, there were 17 students with the average value of exact subjects below 71.9125 placed at the social studies major and the probability value was 0.283, 8 students with the average value of exact subjects between 71.9125-73.825 were placed in the social studies major and the probability value was 0.133, 12 students with the average value of exact subjects between 73.825-75.7375 were placed in the social studies major and the probability value was 0.2, 3 students with the average value of exact subjects between 75.7375-77.65 were placed in the social studies major and the probability value was 0.05, 2 students the average value of exact subjects between 77.6579.5625 are placed in the social studies major and the probability value is 0.033, 9 students with the average value of exact subjects between 79.5625-81.475 are placed in the social studies major and the probability value is 0.15, 6 students with the average value of exact subjects is between 81,475-8 3.3875 is placed at the social studies major and the probability value is 0.1, 3 students with an average value of exact subjects above 83.3875 are placed at the social studies major and the probability value is 0.05.</span></p>
<p><span class="font1">The probability value of the average score of non-exact subjects with a value of k = 8, be shown in table 4 as follows.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 4. </span><span class="font1">The Probability of The average score of non-exact subjects with k=8</span></p>
<p><span class="font1">The average score &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sub>Probability</sub></span></p>
<p><span class="font1">of non-exact</span></p>
<p><span class="font1">subjects &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Science &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Social Studies</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font1">&lt;71.875</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.3</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.05</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">71.875 – 73.75</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.167</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.1</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">73.75 – 75.625</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.15</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.25</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">75.625 – 77.5</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.033</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.017</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">77.5 – 79.375</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.067</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">79.375 – 81.25</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.167</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.183</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">81.25 – 83.125</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.133</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.167</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">83.125&gt;</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.05</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.167</span></p></td></tr>
</table>
<p><span class="font1">from table 4 above, there were 60 students placed in the science studies major and 60 students were placed in the social studies major. Based on these data, there were 18 students with the average value of non-exact subjects below 71.9125 placed in the science studies major and the probability value of 0.3, 10 student with an average value of non-exact subjects between</span></p>
<p><span class="font1">71.9125-73.825 placed in the science studies major and the probability value of 0.167, 9 students with the average value of non-exact subjects between 73.825-75.7375 are placed in the science studies major and the probability value is 0.15, 2 student with an average value of non-exact subjects between 75.7375-77.65 is placed in the science studies major and the probability value is 0.033, there is no student with the average value of non-exact subjects between 77.65-79.5625 are placed in the science studies major and the probability value is 0, 10 students with the the average value of non-exact subjects between 79.5625-81.475 are placed in the science studies major and the probability value is 0.167, 8 students with the average value of non-exact subjects between 81,475-83.3875 is placed in the science studies major and the probability value is 0.133, 3 students with the average value of non-exact subjects above 83.3875 are placed in the science studies major and the probability value is 0.05. Meanwhile, there were 3 students with the average value of non-exact subjects below 71.9125 placed at the social studies major and the probability value was 0.05, 6 students with the average value of non-exact subjects between 71.9125-73.825 were placed in the social studies major and the probability value was 0.1 , 15 students with the average value of nonexact subjects between 73.825-75.7375 were placed in the social studies major and the probability value was 0.25, 1 students with the average value of non-exact subjects between 75.7375-77.65 were placed in the social studies major and the probability value was 0.033, 4 students the average value of non-exact subjects between 77.65-79.5625 are placed in the social studies major and the probability value is 0.067, 11 students with the average value of non-exact subjects between 79.5625-81.475 are placed in the social studies major and the probability value is 0.183, 10 students with the average value of non-exact subjects is between 81,475-8 3.3875 is placed at the social studies major and the probability value is 0.167, 10 students with an average value of non-exact subjects above 83.3875 are placed at the social studies major and the probability value is 0.167.</span></p>
<p><span class="font1">The probability value for the recommendation criteria can be seen in table 5.</span></p>
<p><span class="font1" style="font-weight:bold;text-decoration:underline;">Table 5. </span><span class="font1" style="text-decoration:underline;">The Probability of the recommendation criteria with k=8</span></p>
<p><span class="font1">Probability Recommendation</span></p>
<p><span class="font1">Science &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Social Studies</span></p>
<p><span class="font1"><sup>Science</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.967 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.15</span></p>
<p><span class="font1">Social Studies &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.033 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.85</span></p>
<p><span class="font1">The number of students used was 120 students who had been recommended by the previous homeroom teacher, there were 60 students were placed in the science studies major and 60 students were placed in the social studies major. Based on these data there were 59 students who were recommended to enter the science studies major and placed in the science studies major, while there was 1 student who was recommended to enter the social studies major but was placed in the science studies major. Furthermore, there were 9 students who were recommended to enter the science studies major but were placed at the social studies major while there were 51 students who were recommended to enter the social studies major and placed at the social studies major. Thus, the probability of students who are recommended to enter the science studies major and be placed in the science studies major is 0.967 while the probability of students who are recommended to enter the social studies major but is placed at the science studies major is 0.033. While the probability of students who were recommended to enter the science studies major but placed in the social studies major was 0.15. then, the probability of students being recommended to enter the social studies major and placed in the social studies major was 0.85. The probability value for the questionnaire criteria can be seen in table 6.</span></p>
<p><span class="font1">The probability value for the Questionnaire criteria can be seen in table 6.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 6. </span><span class="font1">The Probability of the Questionnaire criteria with k=8</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">Questionnaire</span></p></td><td colspan="2" style="vertical-align:middle;">
<p><span class="font1">Probability</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">Science</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Social Studies</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Science</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.833</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.15</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Social Studies</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.167</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.85</span></p></td></tr>
</table>
<p><span class="font1">The number of students used was 120 students who had been given questionnaires, it was recorded as many as 60 students were placed in the science studies majors and 60 more students were placed in the social studies major. Based on these data there were 50 students who chose the science studies major and were placed in the science studies majors, while there were 10 students who chose the social studies major but were placed in the science studies major. Then there were 9 students who chose the science studies major but were placed in the social studies majors while there were 51 students who chose the social studies major and were placed in the social studies major. Thus the probability of students who choose the science studies major can be calculated and placed at the science studies major of 0.833, the probability of students who choose the social studies major but placed in the science studies majors is 0.167. Whereas, the probability of students who choose the science studies major but placed at the social studies major is 0.15 while the probability of students who choose the social studies major and placed at the social studies major is 0.85.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark17"></a><span class="font1" style="font-weight:bold;"><a name="bookmark18"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font1">To see the consistency of the use of equal-width interval discretization in the Naive Bayes method, it was tested for some data, The following test of the implementation of unsupervised discretization on The Naive Bayes method by using sample 60 data can be seen in table 7.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 7. </span><span class="font1">Testing Results with 60 data</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font0">Amount of ‘K’ value</span></p></td><td colspan="5" style="vertical-align:middle;">
<p><span class="font0">Weighted Average</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">TP Rate</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">FP Rate</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">F-Measure</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.082</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">6</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.082</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.917</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.067</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">10</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.967</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.033</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.967</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.967</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.967</span></p></td></tr>
</table>
<p><span class="font1">From the test results using 60 sample data, the application of equal-width interval discretization technique on the Naive Bayes method with the value of k = 4 successfully classify the data with</span></p>
<p><span class="font1">the accuracy of 91.7%, while for the value k = 6, obtained a level of accuracy of 91.7%, then for value k = 8, the obtained accuracy of 93.3% and for the value k = 10, the accuracy rate obtained is 0.967%. meanwhile, testing is also done with 90 data, the test result can be seen in table 8 below.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 8. </span><span class="font1">Testing Results with 90 data</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:top;">
<p><span class="font0">Amount of ‘K’ value</span></p></td><td colspan="5" style="vertical-align:middle;">
<p><span class="font0">Weighted Average</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">TP Rate</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">FP Rate</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">F-Measure</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">6</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.922</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.078</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.922</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.922</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.922</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">8</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.067</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.933</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">10</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.889</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.111</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.889</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.889</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.889</span></p></td></tr>
</table>
<p><span class="font1">From the test result using 90 sample data, the application of equal-width interval discretization technique on Naive Bayes method with k = 4 value succeeded in classifying the data with 90% accuracy, while for k = 6, the accuracy level was 92.5%, then the value k = 8, the accuracy of 93.3% and k = 10, the accuracy of 9.25%. meanwhile, testing is also done with 120 data, the test result can be seen in table 9 below.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 9. </span><span class="font1">Testing Results with 120 data</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:top;">
<p><span class="font0">Amount of ‘K’ value</span></p></td><td colspan="5" style="vertical-align:middle;">
<p><span class="font0">Weighted Average</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">TP Rate</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">FP Rate</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">F-Measure</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.9</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">6</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.925</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.075</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.925</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.925</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">0.925</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">8</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.067</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.933</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">10</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.925</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.075</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.925</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.925</span></p></td><td style="vertical-align:top;">
<p><span class="font0">0.925</span></p></td></tr>
</table>
<p><span class="font1">The test result using 120 sample data, the application of equal-width interval discretization technique on Naive Bayes method with value k = 4 succeeded in classifying data with 90% accuracy, while for k = 6, the accuracy level was 92.2%, then for the value k = 8, the accuracy of 93.3% and k = 10, the accuracy of 88.9%.</span></p>
<p><span class="font1">The graph of the test results with some previous data can be seen in Figure 2 below:</span></p><img src="https://jurnal.harianregional.com/media/38405-2.jpg" alt="" style="width:377pt;height:231pt;">
<p><span class="font1" style="font-weight:bold;">Figure 2. </span><span class="font1">The test results of Unsupervised Discretization Implementation on the Naive Bayes method</span></p>
<p><span class="font1">from the figure 2 above can be seen the results of testing the application of equal-width interval discretization on the Naive Bayes method in predicting the suitability of students' majors. In the test with 60 sample data, the accuracy value of k = 10 was the best result with 58 successfully classified data correctly. Furthermore, in the test with 90 sample data, the best classification result is owned by the value of k = 8 with 84 data successfully classified correctly, and the last test with 120 sample data, got the best result at value k = 8 where there are 112 data successfully classified with correct.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark19"></a><span class="font1" style="font-weight:bold;"><a name="bookmark20"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font1">The conclusion that can be summarized in this study is the application of Unsupervised Discretization on the Naive Bayes method has quite an impact on the test results, where the criteria used for this test are: data on the average value of exact courses, data on the average value of non-exact courses, recommendation data and student questionnaire data. And the application of Unsupervised Discretization especially equal-width discretization to Naive Bayes method in predicting the suitability of the student majors increased from the result of accuracy in the previous study by 90% to 93.3%.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font1" style="font-weight:bold;"><a name="bookmark22"></a>5. &nbsp;&nbsp;&nbsp;Acknowledgments</span></h2></li></ul>
<p><span class="font1">Researchers would like to thank the Ministry of Research and Technology Higher Education Republic of Indonesia (KEMENRISTEKDIKTI) which has helped this research morally and financially.</span></p>
<h2><a name="bookmark23"></a><span class="font1" style="font-weight:bold;"><a name="bookmark24"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;A. Saleh, “KLASIFIKASI METODE NAIVE BAYES DALAM DATA MINING UNTUK MENENTUKAN KONSENTRASI SISWA ( STUDI KASUS DI MAS PAB 2 MEDAN),” in </span><span class="font1" style="font-style:italic;">Konferensi Nasional Pengembangan Teknologi Informasi dan Komunikasi (KeTIK) 2014</span><span class="font1">, 2014, pp. 200–207.</span></p></li>
<li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;X. Zhou, S. Wang, W. Xu, G. Ji, P. Phillips, P. Sun, and Y. Zhang, “Detection of</span></p></li></ul>
<p><span class="font1">Pathological Brain in MRI Scanning Based on Wavelet-Entropy and Naive Bayes</span></p>
<p><span class="font1">Classifier,” Springer, Cham, 2015, pp. 201–209.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[3] &nbsp;&nbsp;&nbsp;L. Jiang, C. Li, S. Wang, and L. Zhang, “Deep feature weighting for naive Bayes and its application to text classification,” </span><span class="font1" style="font-style:italic;">Engineering Application of Artificial Inteligence</span><span class="font1">, vol. 52, pp. 26–39, Jun. 2016.</span></p></li>
<li>
<p><span class="font1">[4] &nbsp;&nbsp;&nbsp;B. Tang, S. Kay, and H. He, “Toward Optimal Feature Selection in Naive Bayes for Text Categorization,” Feb. 2016.</span></p></li>
<li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;A. M. P. and D. S. R., “A sequential naïve Bayes classifier for DNA barcodes,” </span><span class="font1" style="font-style:italic;">Stat. Appl. Genet. Mol. Biol.</span><span class="font1">, vol. 13, no. 4, pp. 1–12, 2014.</span></p></li>
<li>
<p><span class="font1">[6] &nbsp;&nbsp;&nbsp;J. Wu, S. Pan, X. Zhu, Z. Cai, P. Zhang, and C. Zhang, “Self-adaptive attribute weighting for Naive Bayes classification,” </span><span class="font1" style="font-style:italic;">Expert Systems With Application</span><span class="font1">, vol. 42, no. 3, pp. 1487– 1502, Feb. 2015.</span></p></li>
<li>
<p><span class="font1">[7] &nbsp;&nbsp;&nbsp;N. Mohamad, N. Jusoh, Z. Htike, and S. Win, “Bacteria identification from microscopic morphology using naive bayes,” </span><span class="font1" style="font-style:italic;">International Journal of Computer Science, Engineering and Information Technology (IJCSEIT )</span><span class="font1">, vol. 4, no. 2, pp. 1–9, 2014.</span></p></li>
<li>
<p><span class="font1">[8] &nbsp;&nbsp;&nbsp;Y. Zhang, S. Wang, P. Phillips, and G. Ji, “Binary PSO with mutation operator for feature selection using decision tree applied to spam detection,” </span><span class="font1" style="font-style:italic;">Knowledge-Based Systems</span><span class="font1">, vol. 64, pp. 22–31, Jul. 2014.</span></p></li>
<li>
<p><span class="font1">[9] &nbsp;&nbsp;&nbsp;S. Kotsiantis, “Integrating Global and Local Application of Naive Bayes Classifier.,” </span><span class="font1" style="font-style:italic;">International Arab Journal of Information Technology</span><span class="font1">, vol. 11, no. 3, pp. 300–307, 2014.</span></p></li>
<li>
<p><span class="font1">[10] &nbsp;&nbsp;&nbsp;S. Palaniappan and T. Kim Hong, “Discretization of continuous valued dimensions in OLAP data cubes,” </span><span class="font1" style="font-style:italic;">International Journal of Computer Science and Network Security</span><span class="font1">, vol. 8, no. 11, pp. 116–126, 2008.</span></p></li>
<li>
<p><span class="font1">[11] &nbsp;&nbsp;&nbsp;I. Kareem and M. Duaimi, “Improved accuracy for decision tree algorithm based on unsupervised discretization,” </span><span class="font1" style="font-style:italic;">International Journal of Computer Science and Mobile</span></p></li></ul>
<p><span class="font1" style="font-style:italic;">Computing</span><span class="font1">, vol. 3, no. 6, pp. 176–183, 2014.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[12] &nbsp;&nbsp;&nbsp;G. Forman, “An Extensive Empirical Study of Feature Selection Metrics for Text Classification,” </span><span class="font1" style="font-style:italic;">The Journal of Machine Learning Research</span><span class="font1">, vol. 3, no. Mar, pp. 1289–1305,</span></p></li></ul>
<p><span class="font1">2003.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[13] &nbsp;&nbsp;&nbsp;Y. Yang and J. Pedersen, “A comparative study on feature selection in text categorization,” in </span><span class="font1" style="font-style:italic;">14th International Conference on Machine Learning</span><span class="font1">, 1997, pp. 412–420.</span></p></li>
<li>
<p><span class="font1">[14] &nbsp;&nbsp;&nbsp;A. Genkin, D. D. Lewis, and D. Madigan, “Large-Scale Bayesian Logistic Regression for Text Categorization,” </span><span class="font1" style="font-style:italic;">Technometrics</span><span class="font1">, vol. 49, no. 3, pp. 291–304, Aug. 2007.</span></p></li>
<li>
<p><span class="font1">[15] &nbsp;&nbsp;&nbsp;B. Tang and H. He, “ENN: Extended Nearest Neighbor Method for Pattern Recognition [Research Frontier],” </span><span class="font1" style="font-style:italic;">IEEE Computational Intelligence Magazine</span><span class="font1">, vol. 10, no. 3, pp. 52–60, Aug. 2015.</span></p></li>
<li>
<p><span class="font1">[16] &nbsp;&nbsp;&nbsp;A. Saleh, “Implementasi Metode Klasifikasi Naïve Bayes Dalam Memprediksi Besarnya Penggunaan Listrik Rumah Tangga,” </span><span class="font1" style="font-style:italic;">Creat. Inf. Technol. J.</span><span class="font1">, vol. 2, no. 3, pp. 207–217, 2015.</span></p></li>
<li>
<p><span class="font1">[17] &nbsp;&nbsp;&nbsp;A. Al-Ibrahim, “Discretization of Continuous Attributes in Supervised Learning algorithms,” </span><span class="font1" style="font-style:italic;">Res. Bull. Jordan ACM</span><span class="font1">, vol. 2, no. 4, pp. 158–166, 2011.</span></p></li>
<li>
<p><span class="font1">[18] &nbsp;&nbsp;&nbsp;D. Joiţa, “Unsupervised static discretization methods in data mining,” Titu Maiorescu University, 2010.</span></p></li></ul>
<p><span class="font1">113</span></p>