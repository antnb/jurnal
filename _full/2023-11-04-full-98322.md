---
layout: full_article
title: "Comparing Support Vector Machine and Naïve Bayes Methods with A Selection of Fast Correlation Based Filter Features in Detecting Parkinson's Disease"
author: "Yuniar Farida, Nurissaidah Ulinnuha, Silvia Kartika Sari, Latifatun Nadya Desinaini"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-98322 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-98322"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-98322"  
comments: true
---

<p><span class="font2" style="font-weight:bold;">LONTAR KOMPUTER VOL. 14, NO. 2 AUGUST 2023</span></p>
<p><span class="font2" style="font-weight:bold;">DOI : 10.24843/LKJITI.2023.v14.i02.p02</span></p>
<p><span class="font2" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 158/E/KPT/2021</span></p>
<p><span class="font2" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font2" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>Comparing Support Vector Machine and Naïve Bayes Methods with A Selection of Fast Correlation Based Filter Features in Detecting Parkinson's Disease</span></h1>
<p><span class="font2">Yuniar Farida<sup>a1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Nurissaidah Ulinnuha<sup>a2</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Hanimatim Mu’jizah<sup>a3</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Latifatun Nadya Desinaini<sup>a4</sup>, Silvia Kartika Sari<sup>a5</sup></span></p>
<p><span class="font2"><sup>a</sup>Department of Mathematics, Faculty of Sains and Technology, UIN Sunan Ampel Surabaya, Jl. Ahmad Yani 117, Surabaya, Indonesia 50275 </span><a href="mailto:1yuniar_farida@uinsby.ac.id"><span class="font2" style="text-decoration:underline;"><sup>1</sup>yuniar_farida@uinsby.ac.id</span><span class="font2"> </span></a><span class="font1" style="font-variant:small-caps;">(c</span><span class="font1">orresponding author) </span><a href="mailto:2nuris.ulinnuha@uinsby.ac.id"><span class="font2" style="text-decoration:underline;"><sup>2</sup>nuris.ulinnuha@uinsby.ac.id</span></a></p>
<p><a href="mailto:3hanimmujizah@gmail.com"><span class="font2" style="text-decoration:underline;"><sup>3</sup>hanimmujizah@gmail.com</span></a><span class="font2" style="text-decoration:underline;"> </span><a href="mailto:4nadya.desinaini@gmail.com"><span class="font2" style="text-decoration:underline;"><sup>4</sup>nadya.desinaini@gmail.com</span></a><span class="font2" style="text-decoration:underline;"> </span><a href="mailto:5silviakartikas08@gmail.com"><span class="font2" style="text-decoration:underline;"><sup>5</sup>silviakartikas08@gmail.com</span></a></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font2" style="font-style:italic;">Dopamine levels fall due to brain nerve cell destruction, producing Parkinson's symptoms. Humans with this illness experience central nervous system damage, which lowers the quality of life. This disease is not deadly, but when people's quality of life decreases, they cannot perform daily activities as people do. Even in one case, this disease can cause death indirectly. Contrast support vector machines (SVM) and naive Bayesian approaches with and without fast correlationbased filter (FCBF) feature selection, this study attempts to determine the optimum model to detect Parkinson's disease categorization. In this study, datasets from the UCI Machine Learning Repository are used. The results showed that SVM with FCBF achieved the highest accuracy among all the models tested. SVM with FCBF provides an accuracy of 86.1538%, sensitivity of 93.8775%, and specificity of 62.5000%. Both methods, SVM and Naive Bayes, have improved in performance due to FCBF, with SVM showing a more significant increase in accuracy. This research contributed to helping paramedics determine if a patient has Parkinson's disease or not using characteristics obtained from data, such as movement, sound, or other pertinent factors.</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font2" style="font-style:italic;">Parkinson's disease, Fast Correlation-Based Filter, Support Vector Machine, Naïve Bayes</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font2">Parkinson's disease still affects people, and its prevalence must not be understated. According to data from the World Health Organization (WHO), in 2017, 25% of individuals globally were affected with Parkinson's disease [1]. Parkinson's is a cell-based degenerative disease in which the tissue mechanisms in Parkinson's division grow malignant. The growth is an aggressive neoplasm with abnormal increases in excessive amounts, so it is the cause of damage to cell tissue in Parkinson's [2]. The second most common neurological ailment, Parkinson's disease, affects 2-3% of adults over 65. Loss of nerve cells in the substantia nigra, which transmits impulses that control movement, is the outcome of the disorder, which affects the brain's most profound neurological system. Reduced sense of smell, constipation, sleeplessness, limb tremors, and trouble moving are common symptoms among Parkinson's patients [3]. Parkinson's can be detected using voice recording. Patients were asked to pronounce vowels with a class 1 sound level measuring microphone 8 cm in front of the lips. The amplitude of the resulting signal will be digitally normalized to determine the difference in the patient's behavior. Multidimensional speech program (MDVP) increase was used to assess many aspects of sound variation, including harmonic-to-noise ratio (HNR), harmonic-to-harmonic ratio (NHR), amplitude (simmer), and period (jitter). Other parameters are also calculated to describe the degree of complexity of sound recordings' signals and fractal dimensions [4].</span></p>
<p><span class="font2">Previous studies have identified several risk factors for this illness. In males, the age range of 40 to 70 is higher for this condition [5]. Parkinson's patients had anxiety and hopelessness rates of 25.81 percent and 11.17 percent, respectively. In Cui et al.'s research [6], several risk variables that impact the quality of life for individuals with Parkinson's disease were found. Anxiety, dyskinesias, poor sleep, and increased motor function are risk factors for depression in Parkinson's disease patients. More severe autonomic dysfunction is a risk factor for PD patients with compression, but rapid eye movement behavior (RBD) is not.</span></p>
<p><span class="font2">Further studies by Mazon et al. [7], concentrating on Alzheimer's and Parkinson's, demonstrated the connection between metabolic alterations and neurodegenerative illnesses. He discussed the relationship between obesity and the onset of neurodegenerative diseases, such as Alzheimer's and Parkinson's. It has been shown that obesity significantly affects how Parkinson's and Alzheimer's disease progress. Obesity-induced metabolic alterations in the central nervous system (CNS) may cause apoptosis and cell necrosis, which can ultimately result in neuronal death. These alterations also impact the synaptic plasticity of neurons.</span></p>
<p><span class="font2">Monocyte chemoattractant protein-1 and APQ3-IR have been linked to Parkinson's disease in Indonesia, as have age, minimum pitch, maximum pitch, average pitch, jitter level, adiponectin, glucose, and shimmer. Information gathered about Parkinson's illness at the University Hospital of Coimbra supports this. This data considers age, minimum vocal basal, maximum vocal basal, average vocal basal, jitter, simmer (dB), glucose, and simmer: APQ3-IR and monocyte chemoattractant protein-1. Early detection efforts are being made to reduce the impact of Parkinson's. This effort can be made using classification methods. Support Vector Machines (SVM) and Nave Bayes (NB) are widely used classification methods in previous research. The SVM approach faces challenges in the pattern (curse of dimensionality), is efficient to apply, and can generalize patterns that do not fit into the class. The SVM model has distinct advantages for handling small samples of nonlinear and high-dimensional issues. It is less prone than the ANN model to become caught in locally optimal solutions. As a result, data classification, pattern recognition, regression analysis, and other processes frequently use the SVM model [8]. The NB technique, in comparison, has the benefits of quick calculation times, high accuracy, and straightforward algorithms. The NB algorithm benefits from simple logic and consistent performance because it operates under the assumption that all features are independent. Although it is frequently challenging to meet the independence criteria, the NB classifier performs well in practical applications [9].</span></p>
<p><span class="font2">SVM and NB are frequently employed in studies, as demonstrated by Roy et al. [10], who used NB classifiers, SVM-boosted trees, and random forests to separate early Parkinson's disease patients from the general population and perform early (or preclinical) diagnosis of Parkinson's disease. The results show that the SVM classifier performs best (accuracy 96.40%, sensitivity 97.03%, specificity 95.01%, area under ROC 98.88%). Combining non-motor, CSF, and imaging indices can support the preclinical diagnosis of Parkinson's disease.</span></p>
<p><span class="font2">Emon et al. [11] carried out a different investigation with Naive Bayes. They used various data mining techniques, including naive Bayes, K-nearest, and Random Forest classifiers, to diagnose hepatitis. The inquiry yielded the following results: K-nearest neighbors had a 95.8 percent accuracy rate with 10-fold cross-validation, Random Forest had a 98.6 percent accuracy rate, and Naive Bayes had a 93.2 percent accuracy rate. Bashar et al. [12] coupled Simulated Annealing (SA) and SVM to determine the elements influencing water consumption. The findings indicated that the SVM-SA's standard error was 0.578. The hybrid SVM-SA model performed better than other models.</span></p>
<p><span class="font2">Feature selection helps reduce irrelevant features so that the performance of classification algorithms can be improved. One of these methods used is the Fast-Based Correlation Filter (FCBF). This FCBF method can provide exemplary performance in time and accuracy on classification algorithms. Li et al. [13] identified WT-FCBF-LSTM (wavelet transform, fast correlation-based filter, and long short-term memory) was introduced as part of the combined model. The findings indicate that compared to his LSTM, WT-LSTM, and FCBF-LSTM models, the WT-FCBF-LSTM model has a greater prediction accuracy. The MRE and RMSE of the single express and split services forecasts are lower than the combined express and split services forecasts. This suggests that WT-FCBF-LSTM can effectively capture various geographic and temporal aspects of express and side-split services to improve prediction accuracy. Djellali et al.</span></p>
<p><span class="font2">[14] identified WDBC (Wisconsin State Diagnosed Breast Cancer), colon, hepatitis, diffuse large B-cell lymphoma (DLBCL), and lung cancer datasets were classified using feature selection methods using FCBF and particle swarm optimization (PSO). The study's findings demonstrate that the FCBF-PSO approach performs more accurately than the FCBF-GA technique, with the former with an accuracy value of 86.11% and the latter with an accuracy value of 83.33% for PSO without FCBF.</span></p>
<p><span class="font2">Parkinson's disease stands out uniquely for its distinct suite of biomedical sound measurements used to diagnose and differentiate it from other conditions. This sound measurement includes parameters such as average vocal fundamental frequency, fundamental frequency variation, amplitude variation, and more. This compilation of specific biomedical sound measurements differentiates Parkinson's disease from other conditions. FCBF is specifically used to identify the most relevant and discriminatory features for classification purposes. Finding the most critical characteristics to use in successfully separating people with Parkinson's disease from those who do not has been made easier using FCBF.</span></p>
<p><span class="font2">Our research explores the early detection of Parkinson's disease by comparing the Support Vector Machines (SVM) and Naïve Bayes (NB) methods. Although SVM and NB are robust, the complexity of Parkinson's disease necessitates a practical approach due to its complex features. We integrate a Fast Correlation-Based Filter (FCBF) for feature selection. Our study focuses on implementing the FCBF method in the context of Parkinson's disease detection. By comparing the performance of SVM and NB with and without FCBF, we aim to explain the effectiveness of these methods. This dataset has previously been the object of research by Avci to diagnose Parkinson's disease using the Kernel Extreme Learning Machine Genetic-Wavelet Algorithm [15].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font2" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h2></li></ul>
<p><span class="font2">This study uses two methods, SVM and Naïve Bayes, that will be compared to find the best model to detect Parkinson's disease classification with FCBF feature selection and without feature selection.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font2" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Fast Correlation-Based Filter (FCBF)</span></h2></li></ul>
<p><span class="font2">FCBF was developed by Yu and Liu [16] to perform feature selection. This algorithm has the principle that features related to classes but not excessive to other connecting features are good features, so two random variables can be measured in correlation using symmetrical uncertainty (</span><span class="font2" style="font-style:italic;">SU</span><span class="font2">), which is between 0 and 1. Equation 1 shows the SU equation [14], [17].</span></p>
<div>
<p><span class="font7" style="font-style:italic;">SU(X<sub>1</sub>V)</span><span class="font7"> = 2 ×</span></p>
</div><br clear="all">
<div>
<p><span class="font7">■ IG(X</span><span class="font4">∣</span><span class="font7">Y)</span></p>
<p><span class="font7" style="font-style:italic;">.H(X)+H(Y)</span></p>
</div><br clear="all">
<div>
<p><span class="font7">[</span></p>
</div><br clear="all">
<div>
<p><span class="font7">■]</span></p>
</div><br clear="all">
<div>
<p><span class="font2">(1)</span></p>
</div><br clear="all">
<p><span class="font7" style="font-style:italic;">H(X)</span><span class="font2"> is the entropy value of </span><span class="font2" style="font-style:italic;">X,</span><span class="font2"> and </span><span class="font7">H(X</span><span class="font4">∣</span><span class="font7">Y) </span><span class="font2">is the entropy value of </span><span class="font2" style="font-style:italic;">X</span><span class="font2"> if the </span><span class="font2" style="font-style:italic;">Y</span><span class="font2"> variable is known with </span><span class="font7">IG(X</span><span class="font4">∣</span><span class="font7">Y) </span><span class="font2">as the information gain.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font2" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Support Vector Machine (SVM)</span></h2></li></ul>
<p><span class="font2">SVM is a classification technique for predicting classes using machine-learning training [18]. The training process is performed with input data known to be labeled to create the model. The result of the pattern-shaped method is a dividing line of two classes, namely the +1 and -1 classes, called hyperplanes. Hyperplane optimization can be determined by measuring the distance between the hyperplane and the closest pattern within each category. This method's equation can be presented in Equation 2 [19], [20].</span></p>
<p><span class="font7" style="font-style:italic;">f(x)</span><span class="font7"> = </span><span class="font7" style="font-style:italic;">w(x)</span><span class="font7"> + </span><span class="font7" style="font-style:italic;">b</span></p>
<div>
<p><span class="font2">(2)</span></p>
</div><br clear="all">
<p><span class="font2">Where </span><span class="font2" style="font-style:italic;">w</span><span class="font2"> is the weight, </span><span class="font2" style="font-style:italic;">x</span><span class="font2"> is the input variable (data), and b is the bias. Edge values should be maximized to get the best hyperplane values. Figure 1 shows an ideal hyperplane image.</span></p>
<div><img src="https://jurnal.harianregional.com/media/98322-1.jpg" alt="" style="width:154pt;height:73pt;">
<p><span class="font2" style="font-weight:bold;">Figure 1. </span><span class="font2">SVM with hyperplane separating two classes [21]</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font2" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Naive Bayes (NB)</span></h2></li></ul>
<p><span class="font2">The NB algorithm uses an independent feature model, where in the same data, the features stand alone and have nothing to do with other components [22]. NB classifiers are based on Bayes' theorem, which states that each part contributes to the target class independently and equally. Bayes' theorem is the basic rule of the Naive Bayes classifier. Equation 3 will give the Bayes theorem [23], [24], [25].</span></p>
<div>
<p><span class="font7">P(H</span><span class="font4">∖</span><span class="font7">E) =</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">P(E</span><span class="font4" style="font-style:italic;">∖</span><span class="font7" style="font-style:italic;">H)</span><span class="font7"> × </span><span class="font7" style="font-style:italic;">P(H) </span><span class="font7">P(E)</span></p>
</div><br clear="all">
<div>
<p><span class="font2">(3)</span></p>
</div><br clear="all">
<p><span class="font2" style="font-style:italic;">P(H)</span><span class="font2"> is the likelihood that the initial (prior) hypothesis </span><span class="font2" style="font-style:italic;">H</span><span class="font2"> occurs without seeing any evidence. In contrast, </span><span class="font7" style="font-style:italic;">P(H</span><span class="font4" style="font-style:italic;">∖</span><span class="font7" style="font-style:italic;">E)</span><span class="font2"> is a hypothesis </span><span class="font2" style="font-style:italic;">H</span><span class="font2"> occurring if there is evidence that </span><span class="font2" style="font-style:italic;">E</span><span class="font2"> occurs. The likelihood that evidence </span><span class="font2" style="font-style:italic;">E</span><span class="font2"> will impact hypothesis </span><span class="font2" style="font-style:italic;">H</span><span class="font2"> is </span><span class="font7">P(E</span><span class="font4">∖</span><span class="font7">H)</span><span class="font2">. And </span><span class="font2" style="font-style:italic;">P(E)</span><span class="font2"> is the likelihood, absent consideration of additional assumptions or evidence, that the original (prior) evidence </span><span class="font2" style="font-style:italic;">E</span><span class="font2"> will occur [26]</span><span class="font0">, </span><span class="font2">[25]</span><span class="font0">. </span><span class="font2">As per Bayes' rule [25], the initial probability </span><span class="font2" style="font-style:italic;">(P(H))</span><span class="font2"> represents the possibility of a hypothesis before the observation of evidence. In contrast, the final probability </span><span class="font2" style="font-style:italic;">H</span><span class="font2">, </span><span class="font7" style="font-style:italic;">P(H</span><span class="font4" style="font-style:italic;">∖</span><span class="font7" style="font-style:italic;">E)</span><span class="font2"> represents the likelihood of an idea after the preservation of proof. This strategy also offers certain benefits and drawbacks based on the idea. The advantage of this Bayes theorem is that the amount of training data needed is small, can be used by quantitative and discrete data, can function well on various types of data, and can solve not only a single proof but also more than one other. For instance, if you are familiar with the multiple proofs </span><span class="font2" style="font-style:italic;">E</span><span class="font0" style="font-style:italic;">1</span><span class="font2">, </span><span class="font2" style="font-style:italic;">E</span><span class="font0" style="font-style:italic;">2,</span><span class="font2"> and </span><span class="font2" style="font-style:italic;">E</span><span class="font0" style="font-style:italic;">3</span><span class="font2">, you may write the final probability of Bayes' theorem using equation 4 [10], [25]:</span></p>
<div>
<p><span class="font7">P(H</span><span class="font4">∖</span><span class="font7">E<sub>1</sub>,E<sub>2</sub>,E<sub>3</sub>)</span></p>
</div><br clear="all">
<div>
<p><span class="font7">P(E<sub>1</sub>,E<sub>2</sub>,E<sub>3</sub></span><span class="font4">∖</span><span class="font7">H)×P(H)</span></p>
<p><span class="font7" style="font-style:italic;">P(E</span><span class="font5" style="font-style:italic;">1</span><span class="font7" style="font-style:italic;">,E</span><span class="font5" style="font-style:italic;">2</span><span class="font7" style="font-style:italic;">,E</span><span class="font6" style="font-style:italic;"><sub>3</sub></span><span class="font7" style="font-style:italic;">)</span></p>
</div><br clear="all">
<div>
<p><span class="font2">(4)</span></p>
</div><br clear="all">
<p><span class="font2">The advantages and disadvantages of the Bayes theorem are that it can only be used in statistical data, can't apply if the conditional probability is zero, and there must be early learning to determine decisions [10].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font2" style="font-weight:bold;"><a name="bookmark13"></a>2.4. &nbsp;&nbsp;&nbsp;SVM Kernel</span></h2></li></ul>
<p><span class="font2">The data in the input feature is transformed into features space using the kernel trick in the SVM learning model procedure. The kernel can translate data into kernel space, a higher-dimensional space, where this process can separate the data linearly [27]. Several kernels are used in SVM: linear, polynomial, RBF, and sigmoid.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font2" style="font-weight:bold;"><a name="bookmark15"></a>2.5. &nbsp;&nbsp;&nbsp;Hessian matrix</span></h2></li></ul>
<p><span class="font2">Each element of the Hessian matrix represents the second partial derivative of the function. The Hessian matrix </span><span class="font2" style="font-style:italic;">f(x)</span><span class="font2">, an n-variable function with a second partial derivative and a continuous derivative, can be represented in equation 6 [31].</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">H =</span></p></td><td style="vertical-align:top;">
<p><span class="font7" style="font-style:italic;">' d f</span></p>
<p><span class="font5" style="font-style:italic;"><sup>dx</sup>1</span></p>
<p><span class="font7" style="font-style:italic;">d<sup>2</sup>f</span></p>
<p><span class="font7" style="font-style:italic;">dx</span><span class="font6" style="font-style:italic;"><sub>2</sub></span><span class="font7" style="font-style:italic;">d</span><span class="font5" style="font-style:italic;">ι</span></p></td><td style="vertical-align:top;">
<p><span class="font7" style="font-style:italic;">d<sup>2</sup>f dx</span><span class="font6" style="font-style:italic;"><sub>1</sub></span><span class="font7" style="font-style:italic;">d</span><span class="font6" style="font-style:italic;"><sub>2 </sub></span><span class="font7" style="font-style:italic;">d<sup>2</sup>f dx</span><span class="font5" style="font-style:italic;">2</span></p></td><td style="vertical-align:top;">
<p><span class="font7" style="font-style:italic;">. &nbsp;&nbsp;&nbsp;</span><span class="font7" style="font-style:italic;text-decoration:underline;">d<sup>2</sup>f</span></p>
<p><span class="font7" style="font-style:italic;">dx</span><span class="font5" style="font-style:italic;">ι</span><span class="font7" style="font-style:italic;">d</span><span class="font6" style="font-style:italic;"><sub>n </sub></span><span class="font7" style="font-style:italic;">d<sup>2</sup>f dx</span><span class="font5" style="font-style:italic;">2</span><span class="font7" style="font-style:italic;">d</span><span class="font6" style="font-style:italic;"><sub>n</sub></span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">(5)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">.<sup>d2f</sup>.</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">d<sup>2</sup>f</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">d:</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">.dXγιd</span><span class="font5" style="font-style:italic;">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">dx</span><span class="font6" style="font-style:italic;"><sub>n</sub></span><span class="font7" style="font-style:italic;">d</span><span class="font6" style="font-style:italic;"><sub>2</sub></span></p></td><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">dx</span><span class="font5" style="font-style:italic;">n</span><span class="font7" style="font-style:italic;">.</span></p></td><td style="vertical-align:top;"></td></tr>
</table>
<p><span class="font2">This Hessian matrix evaluates the derivatives of two functions of more than one variable. More precisely, it is used to identify the static point function of two or more variables. For example, </span><span class="font7" style="font-style:italic;">f(x) = f(x</span><span class="font6" style="font-style:italic;"><sub>1</sub></span><span class="font7" style="font-style:italic;">,............,x</span><span class="font6" style="font-style:italic;"><sub>n</sub></span><span class="font7" style="font-style:italic;">)</span><span class="font2"> is a real-valued function whose second partial derivatives are all</span></p>
<p><span class="font2">continuous [32].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font2" style="font-weight:bold;"><a name="bookmark17"></a>2.6. &nbsp;&nbsp;&nbsp;Confusion matrix</span></h2></li></ul>
<p><span class="font2">A confusion matrix is an approach to understanding information that contains actual data and predictions based on classification findings. It is anticipated in classification to categorize data precisely and generate good results with few mistakes. As a result, this approach exists to help determine how effective categorization is [33]. The Confusion matrix produces the following primary results: accuracy, precision, specificity, and sensitivity. Table 1 displays the confusion matrix table. Before determining the preliminary results, each of the parameters in Table 1 must be recognized [19], [33], [34].</span></p>
<p><span class="font2" style="font-weight:bold;">Table 1. </span><span class="font2">Table of Confusion Matrix</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font2">Actual or Classification</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Positive</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Negative</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">TP or True Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">FN or False Negative</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Negative</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">FP or False Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">TN or True Negative</span></p></td></tr>
</table>
<p><span class="font2">Equations 5, 6, and 7 were used to calculate the accuracy, sensitivity, and specificity after obtaining each of the characteristics listed in Table 1.</span></p>
<p><span class="font7" style="font-style:italic;">TP</span><span class="font7"> + </span><span class="font7" style="font-style:italic;">TN Accuracy</span><span class="font7"> = —————-—— </span><span class="font7" style="font-style:italic;">TP + TN + FP + FN</span></p>
<div>
<p><span class="font2">(6)</span></p>
<p><span class="font2">(7)</span></p>
<p><span class="font2">(8)</span></p>
</div><br clear="all">
<p><span class="font7" style="font-style:italic;">TP</span></p>
<p><span class="font7" style="font-style:italic;">Sensitivity </span><span class="font6" style="font-style:italic;">= ——— </span><span class="font7" style="font-style:italic;"><sup>y</sup> TP+ FN</span></p>
<p><span class="font7" style="font-style:italic;">TN</span></p>
<p><span class="font7" style="font-style:italic;">Specificity </span><span class="font6" style="font-style:italic;">= ———— </span><span class="font7" style="font-style:italic;">TN + FP</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font2" style="font-weight:bold;"><a name="bookmark19"></a>2.7. &nbsp;&nbsp;&nbsp;Data collection</span></h2></li></ul>
<p><span class="font2">The UCI Machine Learning Repository provides access to numeric data from the Parkinson's Disease dataset created by Max Little at the University of Oxford in collaboration with the Colorado National Center for Speech and Language. There are as many as 22 parameters of speech signal recording lab results, with the number of people with Parkinson's as many as 147 people and normal people as many as 48 [35]. This data will be split into training and testing, with an 80:20 ratio. There are 156 training and 39 test data, including data from 29 Parkinson's patients and ten normal subjects. After data release, feature selection and classification procedures are used.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark20"></a><span class="font2" style="font-weight:bold;"><a name="bookmark21"></a>2.8. &nbsp;&nbsp;&nbsp;Training and testing data</span></h2></li></ul>
<p><span class="font2">To assess the effectiveness of a model or algorithm, the initial step in this study was to divide the data into training and testing sections using k-fold cross-validation, a statistical approach. The data is split into equal (or nearly equal) k-folds for k-fold cross-validation. In each subsequent round of training and testing, a specific data fold is used for testing, and the remaining k-1 folds are used for activity [36],[37]. We employed </span><span class="font2" style="font-style:italic;">k</span><span class="font2">-fold cross-validation with a </span><span class="font2" style="font-style:italic;">k</span><span class="font2"> value of 5 in this study.</span></p>
<p><span class="font2">The next stage is the feature selection process. This process reduces features that are not required to simplify the classification process. The selection of parts used is the FCBF method. Good accuracy is obtained from the threshold parameters entered into the FCBF. The FCBF data results fall into two categories: training data and test data. The data is also transferred to the classification phase using the SVM and NB methods.</span></p>
<p><span class="font2">The SVM method's first step is to input data on Parkinson's disease. After obtaining the data, the SVM kernel is calculated as the next step. The following computation process finds the Hessian matrix's importance and runs the sequential SVM training and SVM test processes' computation processes. The final step in the SVM process is to evaluate its classification.</span></p>
<p><span class="font2">The NB method first computes the probability of each label, and the case probability of each brand determines the possibilities of problematic labels (data test) and compares the probability results of each title. In addition, this study also classified Parkinson's disease data without the process of selecting features that will be compared to the level of accuracy. Based on these measures, the results of this method comparison are expected to obtain the best model for diagnosing Parkinson's disease.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark22"></a><span class="font2" style="font-weight:bold;"><a name="bookmark23"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font2">This study's output is the classifier's accuracy, which gauges how well the recognition process worked. The likelihood of the card succeeding increases with precision. Feature selection and classification are used in both stages of the identification process.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark24"></a><span class="font2" style="font-weight:bold;"><a name="bookmark25"></a>3.1. &nbsp;&nbsp;&nbsp;Feature selection</span></h2></li></ul>
<p><span class="font2">Feature selection aims to reduce redundant functionality and retain related components. The choice of features in this study uses FCBF with a threshold of 0.7, whose results will be used at the classification stage. A comparison of the parameter results with and without FCBF is shown in Table 2. From the table, we can see that there are 22 parameters and ten parameters after selecting the characteristic.</span></p>
<table border="1">
<tr><td colspan="3" style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Table 2. </span><span class="font2">Comparison of parameters using FCBF and without FCBF</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">No.</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Abbreviation</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Feature description</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP: FO (Hz)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Average vocal fundamental frequency</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP: Fhi (Hz)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Maximum vocal fundamental frequency</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP: Flo (Hz)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Minimum vocal fundamental frequency</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">4</span></p></td><td style="vertical-align:top;">
<p><span class="font1">MDVP: Jitter (%)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">MDVPjitter in percentage</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVPiJitter(Abs)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP absolute jitter in ms</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">6</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP: RAP</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP relative amplitude perturbation</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">MDVP: PPQ</span></p></td><td style="vertical-align:top;">
<p><span class="font1">MDVP five-point period perturbation quotient</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Jitter: DDP</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Average absolute difference Ofdifferences between jitter cycles</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">9</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP: Shimmer</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP local shimmer</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">10</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP: Shimmer (dB)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">MDVP local shimmer in dB</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">11</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Shimmer: APQ3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Three-Pointamplitude perturbation quotient</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">12</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Shimmer: APQ5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Five-point amplitude perturbation quotient</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">13</span></p></td><td style="vertical-align:top;">
<p><span class="font1">MDVP: APQ11</span></p></td><td style="vertical-align:top;">
<p><span class="font1">MDVP 11-point amplitude perturbation quotient</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">14</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">Shimmer: DDA</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Average absolute differences between the</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font1">amplitudes of consecutive periods</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">15</span></p></td><td style="vertical-align:top;">
<p><span class="font1">NHR</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Noise-to-harmonics ratio</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">16</span></p></td><td style="vertical-align:top;">
<p><span class="font1">HNR</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Harmonics-to-noise ratio</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">17</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">RPDE</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Recurrence period density entropy measure</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">18</span></p></td><td style="vertical-align:top;">
<p><span class="font1">D2</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Correlation dimension</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">19</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font1">DFA</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Signal fractal scaling exponent of detrended</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">fluctuation analysis</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">20</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Spread 1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Two nonlinear measures Offundamental</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">21</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Spread2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Frequency variation</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">22</span></p></td><td style="vertical-align:top;">
<p><span class="font1">PPE</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Pitch period entropy</span></p></td></tr>
</table>
<p><span class="font0">* The colored bar is the selected feature by FCBF</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark26"></a><span class="font2" style="font-weight:bold;"><a name="bookmark27"></a>3.2. &nbsp;&nbsp;&nbsp;Classification</span></h2></li></ul>
<p><span class="font2">Feature selection data fall into two classes. Class 0 indicates no Parkinson's disease, and class 1 exhibits symptoms. The data classification process is performed using SVM and NB methods. The initial SVM used is a linear kernel type. The results of accuracy testing by selecting FCBF features of 2 to 11 features for the SVM-Linear Kernel and Naive Bayes (NB) classifier are presented in Table 3. It can be observed that the accuracy of the SVM classifier varies for several different features, ranging from 84.6154% to 86.1538%. Likewise, the Naive Bayes classifier shows accuracy ranging from 74.8718% to 80.5128% as the number of features selected changes. It was found that the best accuracy of the two models, both SVM and Naive Bayes, occurs when the number of features is specified as two, with SVM accuracy reaching 86.1538% and NB accuracy of 80.5128%. The two features used in the classification model are spread1 and PEP.</span></p>
<table border="1">
<tr><td colspan="3" style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Table 3. </span><span class="font2">The results of the accuracy of the test are based on the number of features with FCBF</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Number of features</span></p></td><td style="vertical-align:top;">
<p><span class="font2">SVM Linear-FCBF</span></p></td><td style="vertical-align:top;">
<p><span class="font2">NB-FCBF</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">2</span></p></td><td style="vertical-align:top;">
<p><span class="font2">86.1538%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">80.5128%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.6410%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">77.9487%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">84.6154%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">78.9744%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.1282%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">77.4359%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">6</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.1282%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">78.4615%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.1282%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">80.0000%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.1282%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">77.9487%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">9</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.1282%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">76.9231%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">84.6154%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">74.8718%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">11</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">84.6154%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">74.8718%</span></p></td></tr>
</table>
<p><span class="font2">The next step is to test the SVM kernel type to find the kernel with the best performance. Table 4 shows the SVM-FCBF performance test results based on the kernel type. Among these kernels, the linear kernel achieved the highest accuracy of 86.1538%, followed by the polynomial and RBF kernels, which achieved an accuracy of 85.1282% each. However, the Sigmoid kernel has the lowest accuracy of 50.7692%.</span></p>
<p><span class="font2" style="font-weight:bold;">T</span><span class="font2" style="font-weight:bold;text-decoration:underline;">able 4. </span><span class="font2" style="text-decoration:underline;">SVM-FCBF performance test results based on kernel type</span><span class="font2"> Kernel</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font2">Parameter</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Linear</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Polynomial</span></p></td><td style="vertical-align:top;">
<p><span class="font2">RBF</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Sigmoid</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">86.1538%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.1282%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85.1282%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50.7692%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Sensitivity</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93.8776%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93.1973%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">95.2381%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">67.3469%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Specificity</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">62.5000%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">60.4167%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">54.1667%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">0.0000%</span></p></td></tr>
</table>
<p><span class="font2" style="font-weight:bold;">Table 5. </span><span class="font2">Results of accuracy, sensitivity, and specificity testing with and without FCBF are compared</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font2">Parameters</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">SVM</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">SVM-FCBF</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">NB</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">NB-FCBF</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85,6410%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">86.1538%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">70.7692%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">80.5128%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Sensitivity</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">95.2380%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93.8775%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">63.9455%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">81.6327%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Specificity</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">56.2500%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">62.5000%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">91.6667%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">78.9116%</span></p></td></tr>
</table>
<p><span class="font2">Table 5 compares the accuracy, sensitivity, and specificity results with and without FCBF for the SVM and NB methods. SVM with FCBF achieves the highest accuracy among all models. This suggests that when combined with FCBF feature selection, SVM balances correctly identifying positive cases and overall accuracy. Table 4 also shows that the SVM-FCBF increased the specificity by 6.25%, but the sensitivity decreased by 1.4%. Despite this, the sensitivity value</span></p>
<p><span class="font2">remains high above 90%. The high sensitivity indicates that the model effectively reduces false negatives, ensuring actual disease cases are not missed at diagnosis. If sensitivity is low, the model may misclassify individuals with Parkinson's as healthy, resulting in false negative results. This can delay or prevent early diagnosis and intervention. Meanwhile, NB without FCBF had the highest specificity, indicating that it is better at correctly identifying negative cases, but the sensitivity is lower. The results highlight the importance of feature selection (FCBF) in improving model performance. Both SVM and NB benefit from FCBF and SVM shows more significant accuracy improvements.</span></p>
<p><span class="font2">The running time results, both with and without the application of FCBF feature selection, are presented in Table 6. The running time for the SVM classifier shows a significant decrease when FCBF feature selection is used, mainly when the number of features selected is limited to only two. For the SVM classifier, the running time was significantly reduced from 3.4942 seconds without FCBF to 0.0529 seconds with FCBF. This reduction in running time can be attributed to a simplified computational process due to reduced feature dimensions achieved through FCBF.</span></p>
<p><span class="font2">However, the reduction in running time did not occur in the NB classifier. When FCBF feature selection is implemented, the running time for the NB classifier increases slightly from 0.0255 seconds without FCBF to 0.0383 seconds with FCBF. This may be because the computational efficiency of the NB algorithm is inherently less affected by reduced feature dimensions compared to SVM. On the other hand, the NB model gives the lowest results. Although FCBF aims to maintain relevant features, this may not align with the feature independence assumption in Naive Bayes. Therefore, the lower performance of Naive Bayes can be attributed to the model's inability to capture complex inter-feature relationships.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 6. </span><span class="font2">Results of running time with and without FCBF are compared</span></p>
<p><span class="font2">Parameters &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SVM &nbsp;&nbsp;&nbsp;SVM-FCBF &nbsp;&nbsp;&nbsp;&nbsp;NB &nbsp;&nbsp;&nbsp;&nbsp;NB-FCBF</span></p>
<p><span class="font2">Running time (s) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4942 &nbsp;&nbsp;&nbsp;&nbsp;0.0529 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0255 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0383</span></p>
<p><span class="font2">The results can be illustrated with the Characteristic Operating Receiver (ROC) graph. Figure 2 displays a Roc graphic image using the SVM-FCBF technique.</span></p><img src="https://jurnal.harianregional.com/media/98322-2.jpg" alt="" style="width:289pt;height:220pt;">
<p><span class="font2" style="font-weight:bold;">Figure 2. </span><span class="font2">ROC graphics of SVM-FCBF methods</span></p>
<p><span class="font2">The ROC chart describes the results of the confusion matrix, with horizontal lines being false positive and vertical being genuinely positive. The Area Under Curve (AUC) value obtained from this chart is 0.9, with a reasonable accuracy value. Table 4 displays the SVM-FCBF method's confusion matrix table.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 4. </span><span class="font2">Confusion matrix method SVM-FCBF</span></p>
<p><span class="font2">Actual / Classification Positive Negative</span></p>
<p><span class="font2">Positive &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;30 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;18</span></p>
<p><span class="font2">Negative &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;138</span></p>
<p><span class="font2">This dataset has previously been the object of research by Avci to diagnose Parkinson's disease using the Kernel Extreme Learning Machine Genetic-Wavelet Algorithm [15]. The study achieved the highest accuracy rate of 96.81%, where the accuracy score was much higher than the accuracy of this research model (SVM-FCBF), which reached 86.1538%. This may help clarify the possible avenues for future model performance improvements and the enhancements that can be made in this area.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark28"></a><span class="font2" style="font-weight:bold;"><a name="bookmark29"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font2">A system that implements the SVM and NB methods and identifies Parkinson's disease through a feature selection process using a fast correlation-based filter is a system that can perform accurate classification. The feature selection process successfully identifies relevant features that are later used in the identification process. The other step is classification preparation utilizing the SVM and NB strategies. Based on the results obtained, SVM-FCBF is the best result. The SVM-FCBF classification has an accuracy of 86.1538%, a sensitivity of 93.8775%, and a specificity of 62.5000%. As future studies intend to apply SVM and Naive Bayes to detect Parkinson's disease, these results could be used as a reference. The outcomes of this study can also be tested using a widely utilized method, namely the Deep Learning approach, which may obtain better research results.</span></p>
<h2><a name="bookmark30"></a><span class="font2" style="font-weight:bold;"><a name="bookmark31"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;WHO, </span><span class="font2" style="font-style:italic;">Atlas - Country resources for neurological disorders</span><span class="font2">, vol. 30, no. November. 2017. [Online]. Available: &nbsp;&nbsp;</span><a href="https://www.who.int/publications/i/item/atlas-country-resources-for-"><span class="font2">https://www.who.int/publications/i/item/atlas-country-resources-for-</span></a></p></li></ul>
<p><span class="font2">neurological-disorders</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[2] &nbsp;&nbsp;&nbsp;A. Elbaz, L. Carcaillon, S. Kab, and F. Moisan, &quot;EPIDEMIOLOGY OF PARKINSON' S DISEASE The Rotterdam Study,&quot; </span><span class="font2" style="font-style:italic;">Reveu Neurologique (Paris).</span><span class="font2">, vol. 172, no. 1, pp. 14–26, 2016, doi: 10.1016/j.neurol.2015.09.012.</span></p></li>
<li>
<p><span class="font2">[3] &nbsp;&nbsp;&nbsp;R. Pahwa and K. E. Lyons, </span><span class="font2" style="font-style:italic;">Handbook of Parkinson's Disease</span><span class="font2">, Fifth Edit. 2013.</span></p></li>
<li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;Y. Wu </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;Dysphonic Voice Pattern Analysis of Patients in Parkinson's Disease Using Minimum Interclass Probability Risk Feature Selection and Bagging Ensemble Learning Methods,&quot; </span><span class="font2" style="font-style:italic;">Computational and Mathematical Methods in Medicine</span><span class="font2">, vol. 2017, 2017, doi: 10.1155/2017/4201984.</span></p></li>
<li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;E. Muliawan, S. Jehosua, and R. Tumewah, “Diagnosis dan Terapi Deep Brain Stimulation pada Penyakit Parkinson,” </span><span class="font2" style="font-style:italic;">Jurnal Neurologi Manado SINAPSIS</span><span class="font2">, vol. 1, no. 1, pp. 67–84, 2018.</span></p></li>
<li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;S. S. Cui </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;Prevalence and risk factors for depression and anxiety in Chinese patients with Parkinson's disease,&quot; </span><span class="font2" style="font-style:italic;">BMC Geriatrics</span><span class="font2">, vol. 17, &nbsp;no. 1, pp. 1–10, &nbsp;2017, doi:</span></p></li></ul>
<p><span class="font2">10.1186/s12877-017-0666-2.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[7] &nbsp;&nbsp;&nbsp;J. N. Mazon, A. H. de Mello, G. K. Ferreira, and G. T. Rezin, &quot;The impact of obesity on neurodegenerative diseases,&quot; </span><span class="font2" style="font-style:italic;">Life &nbsp;Sciences</span><span class="font2">, &nbsp;vol. 182, pp. 22–28, &nbsp;2017, doi:</span></p></li></ul>
<p><span class="font2">10.1016/j.lfs.2017.06.002.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[8] &nbsp;&nbsp;&nbsp;J. Li, Y. Lei, and S. Yang, &quot;Mid-long term load forecasting model based on support vector machine optimized by improved sparrow search algorithm,&quot; </span><span class="font2" style="font-style:italic;">Energy Reports</span><span class="font2">, vol. 8, pp. 491– 497, 2022, doi: 10.1016/j.egyr.2022.02.188.</span></p></li>
<li>
<p><span class="font2">[9] &nbsp;&nbsp;&nbsp;G. Yang and X. Gu, &quot;Fault Diagnosis of Complex Chemical Processes Based on Enhanced Naive Bayesian Method,&quot; </span><span class="font2" style="font-style:italic;">IEEE Transactions on Instrumentation and Measurement</span><span class="font2">, vol. 69, no. 7, pp. 4649–4658, 2020, doi: 10.1109/TIM.2019.2954151.</span></p></li>
<li>
<p><span class="font2">[10] &nbsp;&nbsp;&nbsp;R. Prashanth, S. Dutta Roy, P. K. Mandal, and S. Ghosh, &quot;High-Accuracy Detection of Early Parkinson's Disease through Multimodal Features and Machine Learning,&quot; </span><span class="font2" style="font-style:italic;">International Journal of Medical Informatics</span><span class="font2">, vol. 90, pp. 13–21, 2016, doi: 10.1016/j.ijmedinf.2016.03.001.</span></p></li>
<li>
<p><span class="font2">[11] &nbsp;&nbsp;&nbsp;T. I. Trishna, S. U. Emon, R. R. Ema, G. I. H. Sajal, S. Kundu, and T. Islam, &quot;Detection of</span></p></li></ul>
<p><span class="font2">Hepatitis (A, B, C, and E) Viruses Based on Random Forest, K-nearest and Naïve Bayes Classifier,&quot; </span><span class="font2" style="font-style:italic;">2019 10th International Conference on Computing, Communication and Networking &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Technologies &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(ICCCNT)</span><span class="font2">, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pp. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1–7, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;doi:</span></p>
<p><span class="font2">10.1109/ICCCNT45670.2019.8944455.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[12] &nbsp;&nbsp;&nbsp;A. M. Bashar, H. Nozari, S. Marofi, M. Mohamadi, and A. Ahadiiman, &quot;Investigation of factors affecting rural drinking water consumption using intelligent hybrid models,&quot; </span><span class="font2" style="font-style:italic;">Water Science and Engineering.</span><span class="font2">, vol. 16, no. 2, pp. 175–183, 2022, doi: 10.1016/j.wse.2022.12.002.</span></p></li>
<li>
<p><span class="font2">[13] &nbsp;&nbsp;&nbsp;X. Li, Y. Zhang, M. Du, and J. Yang, &quot;The forecasting of passenger demand under hybrid ridesharing service modes: A combined model based on WT-FCBF-LSTM,&quot; </span><span class="font2" style="font-style:italic;">Sustainable Cities and Society.</span><span class="font2">, vol. 62, no. April, p. 102419, 2020, doi: 10.1016/j.scs.2020.102419.</span></p></li>
<li>
<p><span class="font2">[14] &nbsp;&nbsp;&nbsp;H. Djellali and S. Guessoum, &quot;Fast Correlation based Filter combined with Genetic Algorithm and Particle Swarm on Feature Selection Hayet,&quot; </span><span class="font2" style="font-style:italic;">2017 5th International Conference on Electrical Engineering - Boumerdes (ICEE-B)</span><span class="font2">, vol. 2017-Janua, pp. 1–6, 2017.</span></p></li>
<li>
<p><span class="font2">[15] &nbsp;&nbsp;&nbsp;D. Avci and A. Dogantekin, &quot;An Expert Diagnosis System for Parkinson's Disease Based on Genetic Algorithm-Wavelet Kernel-Extreme Learning Machine,&quot; </span><span class="font2" style="font-style:italic;">Parkinson's Disease</span><span class="font2">, vol. 2016, 2016, doi: 10.1155/2016/5264743.</span></p></li>
<li>
<p><span class="font2">[16] &nbsp;&nbsp;&nbsp;L. Yu and H. Liu, &quot;Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution,&quot; </span><span class="font2" style="font-style:italic;">ICML'03: Proceedings of the Twentieth International Conference on International Conference on Machine Learning</span><span class="font2">, vol. 2, pp. 856–863, 2003.</span></p></li>
<li>
<p><span class="font2">[17] &nbsp;&nbsp;&nbsp;Y. Zhang, G. Pan, Y. Zhao, Q. Li, and F. Wang, &quot;Short-term wind speed interval prediction based on artificial intelligence methods and error probability distribution,&quot; </span><span class="font2" style="font-style:italic;">Energy Conversion and Management. Manag.</span><span class="font2">, vol. 224, no. August, p. 113346, &nbsp;2020, doi:</span></p></li></ul>
<p><span class="font2">10.1016/j.enconman.2020.113346.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[18] &nbsp;&nbsp;&nbsp;D. N. Avianty, P. I. G. P. S. Wijaya, and F. Bimantoro, &quot;The Comparison of SVM and ANN Classifier for COVID-19 Prediction,&quot; </span><span class="font2" style="font-style:italic;">Lontar Komputer - Jurnal Ilmiah Teknologi Informasi</span><span class="font2">, vol. 13, no. 2, p. 128, 2022, doi: 10.24843/lkjiti.2022.v13.i02.p06.</span></p></li>
<li>
<p><span class="font2">[19] &nbsp;&nbsp;&nbsp;A. Z. Foeady, D. C. R. Novitasari, A. H. Asyhar, and M. Firmansjah, &quot;Automated Diagnosis System of Diabetic Retinopathy Using GLCM Method and SVM Classifier,&quot; </span><span class="font2" style="font-style:italic;">2018 5th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)</span><span class="font2">, pp. 154–160, 2019, doi: 10.1109/eecsi.2018.8752726.</span></p></li>
<li>
<p><span class="font2">[20] &nbsp;&nbsp;&nbsp;A. Manoharan, K. M. Begam, V. R. Aparow, and D. Sooriamoorthy, &quot;Artificial Neural Networks, Gradient Boosting and Support Vector Machines for electric vehicle battery state estimation: A review,&quot; </span><span class="font2" style="font-style:italic;">Journal of Energy Storage</span><span class="font2">, vol. 55, no. PA, p. 105384, 2022, doi: 10.1016/j.est.2022.105384.</span></p></li>
<li>
<p><span class="font2">[21] &nbsp;&nbsp;&nbsp;A. Roy and S. Chakraborty, &quot;Support vector machine in structural reliability analysis: A review,&quot; </span><span class="font2" style="font-style:italic;">Reliability Engineering &amp;&nbsp;System Safety</span><span class="font2">, vol. 233, no. January, p. 109126, 2023, doi: 10.1016/j.ress.2023.109126.</span></p></li>
<li>
<p><span class="font2">[22] &nbsp;&nbsp;&nbsp;L. P. Wanti, N. W. A. Prasetya, L. Sari, L. Puspitasari, and A. Romadloni, &quot;Comparison of Naive Bayes Method and Certainty Factor for Diagnosis of Preeclampsia,&quot; </span><span class="font2" style="font-style:italic;">Lontar Komputer - Jurnal Ilmiah Teknologi &nbsp;Informasi</span><span class="font2">, &nbsp;&nbsp;vol. 13, no. 2, p. 105, &nbsp;&nbsp;2022, doi:</span></p></li></ul>
<p><span class="font2">10.24843/lkjiti.2022.v13.i02.p04.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[23] &nbsp;&nbsp;&nbsp;Z. Deng, T. Han, Z. Cheng, J. Jiang, and F. Duan, &quot;Fault detection of petrochemical process based on space-time compressed matrix and Naive Bayes,&quot; </span><span class="font2" style="font-style:italic;">Process Safety and Environmental Protection</span><span class="font2">, vol. 160, pp. 327–340, 2022, doi: 10.1016/j.psep.2022.01.048.</span></p></li>
<li>
<p><span class="font2">[24] &nbsp;&nbsp;&nbsp;Q. Tan </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;A new sensor fault diagnosis method for gas leakage monitoring based on the naive Bayes and probabilistic neural network classifier,&quot; </span><span class="font2" style="font-style:italic;">Measurement</span><span class="font2">, vol. 194, no. 6, March, p. 111037, 2022, doi: 10.1016/j.measurement.2022.111037.</span></p></li>
<li>
<p><span class="font2">[25] &nbsp;&nbsp;&nbsp;Y. Farida and N. Ulinnuha, “Klasifikasi Mahasiswa Penerima Program Beasiswa Bidik Misi Menggunakan Naive Bayes,” </span><span class="font2" style="font-style:italic;">Systemic Information System and Informatics Journal</span><span class="font2">, vol. 4, no. 1, pp. 17–22, 2018, doi: 10.29080/systemic.v4i1.317.</span></p></li>
<li>
<p><span class="font2">[26] &nbsp;&nbsp;&nbsp;B. Rajoub, </span><span class="font2" style="font-style:italic;">Supervised and unsupervised learning</span><span class="font2">. Elsevier Inc., 2020. doi: 10.1016/b978-0-12-818946-7.00003-2.</span></p></li>
<li>
<p><span class="font2">[27] &nbsp;&nbsp;&nbsp;A. Roy, R. Manna, and S. Chakraborty, &quot;Support vector regression based metamodeling for structural reliability analysis,&quot; </span><span class="font2" style="font-style:italic;">Probabilistic Engineering Mechanics</span><span class="font2">, vol. 55, no. September 2018, pp. 78–89, 2019, doi: 10.1016/j.probengmech.2018.11.001.</span></p></li>
<li>
<p><span class="font2">[28] &nbsp;&nbsp;&nbsp;X. Lin, L. Zhao, C. Shang, W. He, W. Du, and F. Qian, &quot;Data-driven robust optimization for cyclic scheduling of ethylene cracking furnace system under uncertainty based on kernel learning,&quot; </span><span class="font2" style="font-style:italic;">Chemical Engineering Science</span><span class="font2">, vol. 260, p. 117919, &nbsp;2022, doi:</span></p></li></ul>
<p><span class="font2">10.1016/j.ces.2022.117919.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[29] &nbsp;&nbsp;&nbsp;D. Wilk-Kolodziejczyk, K. Regulski, and G. Gumienny, &quot;Comparative analysis of the properties of the nodular cast iron with carbides and the austempered ductile iron with use of the machine learning and the support vector machine,&quot; </span><span class="font2" style="font-style:italic;">The International Journal of Advanced Manufacturing Technology</span><span class="font2">, vol. 87, no. 1–4, pp. 1077–1093, 2016, doi:</span></p></li></ul>
<p><span class="font2">10.1007/s00170-016-8510-y.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[30] &nbsp;&nbsp;&nbsp;Z. Zhao </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;Multi support vector models to estimate solubility of Busulfan drug in supercritical carbon dioxide,&quot; </span><span class="font2" style="font-style:italic;">Journal of Molecular Liquids</span><span class="font2">, vol. 350, p. 118573, 2022, doi: 10.1016/j.molliq.2022.118573.</span></p></li>
<li>
<p><span class="font2">[31] &nbsp;&nbsp;&nbsp;W. Dong, S. Gao, and S. S. T. Yau, &quot;Hessian matrix non-decomposition theorem and its application to nonlinear filtering,&quot; </span><span class="font2" style="font-style:italic;">Nonlinear Analysis</span><span class="font2">, vol. 230, p. 113236, 2023, doi: 10.1016/j.na.2023.113236.</span></p></li>
<li>
<p><span class="font2">[32] &nbsp;&nbsp;&nbsp;M. Xu and C. Shi, &quot;A Hessian recovery-based finite difference method for biharmonic problems,&quot; </span><span class="font2" style="font-style:italic;">Applied Mathematics Letters</span><span class="font2">, vol. 137, no. 12271482, p. 108503, 2023, doi: 10.1016/j.aml.2022.108503.</span></p></li>
<li>
<p><span class="font2">[33] &nbsp;&nbsp;&nbsp;Y. Wang, Y. Jia, Y. Tian, and J. Xiao, &quot;Deep reinforcement learning with the confusionmatrix-based dynamic reward function for customer credit scoring,&quot; </span><span class="font2" style="font-style:italic;">Expert Systems with Applications</span><span class="font2">, vol. 200, no. March, p. 117013, 2022, doi: 10.1016/j.eswa.2022.117013.</span></p></li>
<li>
<p><span class="font2">[34] &nbsp;&nbsp;&nbsp;D. C. R. Novitasari, A. H. Asyhar, M. Thohir, A. Z. Arifin, H. Mu'jizah, and A. Z. Foeady, &quot;Cervical Cancer Identification Based Texture Analysis Using GLCM-KELM on Colposcopy Data,&quot; </span><span class="font2" style="font-style:italic;">2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)</span><span class="font2">, pp. 409–414, 2020, doi: 10.1109/ICAIIC48513.2020.9065252.</span></p></li>
<li>
<p><span class="font2">[35] &nbsp;&nbsp;&nbsp;Oxford, &nbsp;&nbsp;&nbsp;&quot;Parkinson's &nbsp;&nbsp;&nbsp;Data &nbsp;&nbsp;&nbsp;Set,&quot; &nbsp;&nbsp;&nbsp;</span><span class="font2" style="font-style:italic;">UCI &nbsp;&nbsp;&nbsp;Learning &nbsp;&nbsp;&nbsp;Repository</span><span class="font2">, &nbsp;&nbsp;&nbsp;2007.</span></p></li></ul>
<p><a href="https://archive.ics.uci.edu/ml/datasets/parkinsons"><span class="font2">https://archive.ics.uci.edu/ml/datasets/parkinsons</span></a><span class="font2"> (accessed Mar. 05, 2020).</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[36] &nbsp;&nbsp;&nbsp;Y. Jung, &quot;Multiple predicting K-fold cross-validation for model selection,&quot; </span><span class="font2" style="font-style:italic;">Journal of Nonparametric Statistics</span><span class="font2">, vol. 30, no. 1, pp. 197–215, &nbsp;&nbsp;2018, doi:</span></p></li></ul>
<p><span class="font2">10.1080/10485252.2017.1404598.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[37] &nbsp;&nbsp;&nbsp;P. Refaeilzadeh, L. Tang, H. Liu, L. Angeles, and C. D. Scientist, &quot;Encyclopedia of Database Systems,&quot; </span><span class="font2" style="font-style:italic;">Encyclopedia of Database Systems</span><span class="font2">, 2020, doi: 10.1007/978-1-4899-7993-3.</span></p></li></ul>
<p><span class="font2">90</span></p>