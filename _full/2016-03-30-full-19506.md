---
layout: full_article
title: "Optimasi Naive Bayes Dengan Pemilihan Fitur Dan Pembobotan Gain Ratio"
author: "I Guna Adi Socrates, Afrizal Laksita Akbar, Mohammad Sonhaji Akbar, Agus Zainal Arifin, Darlis Herumurti"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-19506 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-19506"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-19506"  
comments: true
---

<p><span class="font3" style="font-weight:bold;">LONTAR KOMPUTER VOL. 7, NO.1, APRIL 2016</span></p>
<p><span class="font3" style="font-weight:bold;">DOI: 10.24843/LKJITI.2016.v07.i01.p03</span></p>
<p><span class="font3" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font3" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font4" style="font-weight:bold;"><a name="bookmark1"></a>Optimasi Naïve Bayes Dengan Pemilihan Fitur Dan Pembobotan </span><span class="font4" style="font-weight:bold;font-style:italic;">Gain Ratio</span></h1>
<ul style="list-style:none;"><li>
<p><span class="font3">I. &nbsp;&nbsp;&nbsp;Gusti. A. Socrates<sup>1</sup>, Afrizal L. Akbar<sup>2</sup>, M. Sonhaji Akbar</span></p></li></ul>
<p><span class="font3">Teknik Informatika, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia</span></p>
<p><a href="mailto:socrates15@mhs.if.its.ac.id"><span class="font3"><sup>1</sup>socrates15@mhs.if.its.ac.id</span></a></p>
<p><a href="mailto:afrizal.la@gmail.com"><span class="font3"><sup>2</sup>afrizal.la@gmail.com</span></a></p>
<p><a href="mailto:mson.akbar@gmail.com"><span class="font3"><sup>3</sup>mson.akbar@gmail.com</span></a></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstrak</span></p>
<p><span class="font3" style="font-style:italic;">Naïve Bayes merupakan salah satu metode data mining yang umum digunakan dalam klasifikasi dokumen berbasis text. Kelebihan dari metode ini adalah algoritma yang sederhana dengan kompleksitas perhitungan yang rendah. Akan tetapi, pada metode Naïve Bayes terdapat kelemahan dimana sifat independensi dari fitur Naïve Bayes tidak dapat selalu diterapkan sehingga akan berpengaruh pada tingkat akurasi perhitungan. Maka dari itu, metode Naïve Bayes perlu dioptimasi dengan cara pemberian bobot mengunakan Gain Ratio. Namun, pemberian bobot pada Naïve Bayes menimbulkan permasalahan pada penghitungan probabilitas setiap dokumen, dimana fitur yang tidak merepresentasikan kelas yang diuji banyak muncul sehingga terjadi kesalahan klasifikasi. Oleh karena itu, pembobotan Naïve Bayes masih belum optimal. Paper ini mengusulkan optimasi metode Naïve Bayes mengunakan pembobotan Gain Ratio yang ditambahkan dengan metode pemilihan fitur pada kasus klasifikasi teks. Hasil penelitian ini menunjukkan bahwa optimasi metode Naïve Bayes menggunakan pemilihan fitur dan pembobotan menghasilkan akurasi sebesar 94%.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Kata Kunci</span><span class="font3" style="font-style:italic;">: Data Mining, Naïve Bayes, Weighted Naïve Bayes, Gain Ratio, Pemilihan Fitur.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font3" style="font-style:italic;">Naïve Bayes is one of data mining methods that are commonly used in text-based document classification. The advantage of this method is a simple algorithm with low</span></p>
<p><span class="font3" style="font-style:italic;">computation complexity. However, there is weaknesses on Naïve Bayes methods where independence of Naïve Bayes features can’t be always implemented that would affect the accuracy of the calculation. Therefore, Naïve Bayes methods need to be optimized by assigning weights using Gain Ratio on its features. However, assigning weights on Naïve Bayes’s features cause problems in calculating the probability of each document which is caused by there are many features in the document that not represent the tested class. Therefore, the weighting Naïve Bayes is still not optimal. This paper proposes optimization of Naïve Bayes method using weighted by Gain Ratio and feature selection method in the case of text classification. Results of this study pointed-out that Naïve Bayes optimization using feature selection and weighting produces accuracy of 94%.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Keywords</span><span class="font3" style="font-style:italic;">: Data Mining, Naïve Bayes, Weighted Naïve Bayes, Gain Ratio, Feature Selection.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font3" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Pendahuluan</span></h2></li></ul>
<p><span class="font3">Klasifikasi merupakan proses pengidentifikasian obyek ke dalam sebuah kelas, kelompok, atau kategori berdasarkan prosedur, karakteristik dan definisi yang telah ditentukan sebelumnya [1]. Salah satu bentuk klasifikasi yaitu klasifikasi dokumen atau teks. Klasifikasi dokumen atau teks adalah bidang penelitian dalam pengolahan informasi. Tujuan dari klasifikasi dokumen adalah mengembangkan sebuah metode dalam menentukan atau mengkategorikan suatu dokumen ke dalam satu atau lebih kelompok secara otomatis berdasarkan isi dokumen [2]. Pada era ini pengelompokkan teks atau dokumen digunakan untuk proses pencarian sebuah dokumen.</span></p>
<p><span class="font3">Maka dari itu, kebutuhan untuk pengelompokkan dokumen secara cepat dan mudah sangat penting. Sedangkan saat ini, pengelompokkan dokumen masih menggunakan cara manual.</span></p>
<p><span class="font3">Pengelompokkan dokumen dilakukan dengan cara pemberian label terhadap kategori dokumen. Dibutuhkan waktu yang cukup lama dalam mengklasifikasikan dokumen. Maka dari itu, dibutuhkan metode yang dapat digunakan dalam proses klasifikasi atau pengelompokkan dokumen secara cepat dan akurat.</span></p>
<p><span class="font3">Salah satu metode klasifikasi yang biasa digunakan adalah </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3">. Klasifikasi </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> pertama kali dikemukakan oleh Revered Thomas Bayes. Penggunaan metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> sudah dikenalkan sejak tahun 17</span><a href="http://www.pdfcomplete.com/cms/hppl/tabid/108/Default.aspx?r=q8b3uige22"><span class="font3">02-17</span></a><span class="font3">61. </span><span class="font3" style="font-style:italic;">Naive Bayes</span><span class="font3"> (atau dikenal sebagai </span><span class="font3" style="font-style:italic;">Simple Bayes</span><span class="font3">) menurut Lewis, Hand dan Yu merupakan pendekatan yang sangat sederhana dan sangat efektif untuk </span><span class="font3" style="font-style:italic;">classification learning</span><span class="font3"> [3][4]. Sedangkan menurut Kononenko dan Langley menyimpulkan bahwa </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> merupakan kemungkinan label kelas data atau bisa diasumsikan sebagai atribut kelas yang diberi label [5][6].</span></p>
<p><span class="font3">Menurut Hamzah Naïve Bayes memiliki beberapa kelebihan, yaitu algoritma yang sederhana, lebih cepat dalam penghitungan dan berakurasi tinggi [7]. Akan tetapi, pada metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> juga memiliki kelemahan dimana sebuah probabilitas tidak bisa mengukur seberapa besar tingkat keakuratan sebuah prediksi. Maka dari itu, metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> perlu dioptimasi dengan cara pemberian bobot mengunakan </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3">. Pemberian bobot pada </span><span class="font3" style="font-style:italic;">Naïve Bayes </span><span class="font3">menimbulkan permasalahan pada penghitungan probabilitas setiap dokumen. Dimana fitur yang tidak merepresentasikan kelas yang diuji banyak muncul sehingga terjadi kesalahan klasifikasi. Oleh karena itu, pembobotan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> masih belum optimal.</span></p>
<p><span class="font3">Maka dari itu, Paper ini mengusulkan optimasi metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> mengunakan pembobotan </span><span class="font3" style="font-style:italic;">Gain</span><span class="font3"> Ratio yang ditambahkan dengan metode pemilihan fitur pada kasus pemilihan teks.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font3" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Metode Penelitian</span></h2></li></ul>
<p><span class="font3">Metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> merupakan salah satu algoritma yang efektif dan efisien dalam proses klasifikasi [3][4]. Pada Gambar 1 menampilkan metode usulan </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> dengan menggunakan </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3">.</span></p><img src="https://jurnal.harianregional.com/media/19506-1.jpg" alt="" style="width:426pt;height:38pt;">
<p><span class="font3" style="font-weight:bold;">Gambar 1. </span><span class="font3">Alur Metode Penelitian</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font3" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Dataset</span></h2></li></ul>
<p><span class="font3">Dataset yang digunakan dalam penelitian ini diambil dari media online yaitu kompas, detik, dan tempo. Kemudian dilakukan proses penentuan kata dasar, penentuan kata umum yang sering muncul atau stopwords</span><span class="font3" style="font-style:italic;">,</span><span class="font3"> dan penentuan kategori. Proses pengolahan dataset dapat dilihat pada Gambar 2.</span></p><img src="https://jurnal.harianregional.com/media/19506-2.jpg" alt="" style="width:426pt;height:37pt;">
<p><span class="font3" style="font-weight:bold;">Gambar 2. </span><span class="font3" style="font-style:italic;">Dataset</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font3" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Preprocessing</span></h2></li></ul>
<p><span class="font3" style="font-style:italic;">Preprocessing</span><span class="font3"> adalah proses awal pada klasifikasi dokumen yang bertujuan untuk menyiapkan data agar menjadi terstruktur. Hasil dari </span><span class="font3" style="font-style:italic;">preprocessing</span><span class="font3"> akan berupa nilai numerik sehingga dapat dijadikan sebagai sumber data yang dapat diolah lebih lanjut. </span><span class="font3" style="font-style:italic;">Preprocessing</span><span class="font3"> ini terbagi menjadi beberapa proses yang terdiri dari </span><span class="font3" style="font-style:italic;">case folding</span><span class="font3">, </span><span class="font3" style="font-style:italic;">tokenizing, filtering, stemming</span><span class="font3"> dan penghitungan bobot kata.</span></p>
<p><span class="font3">Pada Gambar 3 terdapat proses </span><span class="font3" style="font-style:italic;">preprocessing. Case folding</span><span class="font3"> merupakan tahap awal dari </span><span class="font3" style="font-style:italic;">preprocessing text</span><span class="font3"> yang </span><span class="font3" style="font-style:italic;">mengubah</span><span class="font3"> karakter huruf teks menjadi huruf kecil semua [8]. Karakter yang diterima hanya ‘a’ hingga ‘z’. Karakter selain huruf akan dihilangkan dan dianggap sebagai </span><span class="font3" style="font-style:italic;">delimiter</span><span class="font3">. </span><span class="font3" style="font-style:italic;">Tokenizing</span><span class="font3"> adalah tahap pemotongan </span><span class="font3" style="font-style:italic;">string input</span><span class="font3"> berdasarkan tiap kata yang menyusunnya [9]. </span><span class="font3" style="font-style:italic;">Filtering</span><span class="font3"> adalah proses menentukan kata-kata (</span><span class="font3" style="font-style:italic;">terms</span><span class="font3">) apa saja yang akan digunakan untuk merepresentasikan dokumen. Selain untuk menggambarkan isi dokumen, </span><span class="font3" style="font-style:italic;">term </span><span class="font3">ini juga berguna untuk membedakan dokumen yang satu dengan dokumen lainnya pada koleksi dokumen. Proses ini dilakukan dengan mengambil kata-kata penting dari hasil </span><span class="font3" style="font-style:italic;">token</span><span class="font3"> dan menghapus </span><span class="font3" style="font-style:italic;">stop words</span><span class="font3">. </span><span class="font3" style="font-style:italic;">Stop words</span><span class="font3"> adalah kata-kata yang tidak deskriptif sehingga dapat dibuang atau dihilangkan dan tidak berpengaruh ke dalam proses [8]. Dalam bahasa Indonesia, contoh </span><span class="font3" style="font-style:italic;">stop words</span><span class="font3"> seperti “yang”, “dan”, “dari”, “di”, “seperti” dan lainnya. Tahap </span><span class="font3" style="font-style:italic;">stemming </span><span class="font3">adalah tahap mencari </span><span class="font3" style="font-style:italic;">root</span><span class="font3"> (akar) kata dari kata hasil </span><span class="font3" style="font-style:italic;">filtering</span><span class="font3">. Pada tahap ini dilakukan proses pengambilan berbagai bentukan kata ke dalam suatu representasi yang sama. </span><span class="font3" style="font-style:italic;">Stem</span><span class="font3"> (akar kata) merupakan bagian dari kata yang tersisa setelah dihilangkan imbuhannya (awalan dan akhiran). Contoh kata beri adalah </span><span class="font3" style="font-style:italic;">stem</span><span class="font3"> dari memberi, diberikan, memberikan dan pemberian.</span></p>
<div><img src="https://jurnal.harianregional.com/media/19506-3.png" alt="" style="width:426pt;height:25pt;">
<p><span class="font3" style="font-weight:bold;">Gambar 3. </span><span class="font3" style="font-style:italic;">Preprocessing</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font3" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Penghitungan bobot</span></h2></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font3">a.</span><span class="font3" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Bayes</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Naive bayes</span><span class="font3"> adalah metode yang digunakan dalam statistika untuk menghitung peluang dari suatu hipotesis, </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> menghitung peluang suatu kelas berdasarkan pada atribut yang dimiliki dan menentukan kelas yang memiliki probabilitas paling tinggi. </span><span class="font3" style="font-style:italic;">Naive bayes</span><span class="font3"> mengklasifikasikan kelas berdasarkan pada probabilitas sederhana dengan mangasumsikan bahwa setiap atribut dalam data tersebut bersifat saling terpisah. Metode </span><span class="font3" style="font-style:italic;">Naive Bayes</span><span class="font3"> merupakan salah satu metode yang banyak digunakan berdasarkan beberapa sifatnya yang sederhana, metode </span><span class="font3" style="font-style:italic;">Naive Bayes</span><span class="font3"> mengklasifikasikan data berdasarkan probabilitas </span><span class="font3" style="font-style:italic;">P</span><span class="font3"> atribut </span><span class="font3" style="font-style:italic;">x</span><span class="font3"> dari setiap kelas </span><span class="font3" style="font-style:italic;">y </span><span class="font3">data. Pada model probablitas setiap kelas </span><span class="font3" style="font-style:italic;">k</span><span class="font3"> dan jumlah atribut </span><span class="font3" style="font-style:italic;">a</span><span class="font3"> yang dapat dituliskan seperti Persamaan (1) [2] berikut.</span></p>
<p><span class="font8">^(y⅛<sup>l</sup>^ι<sup>,</sup>^2<sup>,</sup> </span><span class="font8" style="font-style:italic;">-.</span><span class="font15" style="font-style:italic;"><sup>χ</sup><sub>a</sub>)</span></p>
<div>
<p><span class="font3">(1)</span></p>
</div><br clear="all">
<p><span class="font3">Penghitungan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> yaitu probabilitas dari kemunculan dokumen </span><span class="font3" style="font-style:italic;">x</span><span class="font1" style="font-style:italic;">a</span><span class="font3"> pada kategori kelas </span><span class="font3" style="font-style:italic;">y</span><span class="font1" style="font-style:italic;">k </span><span class="font3" style="font-style:italic;">P(x</span><span class="font1" style="font-style:italic;">a</span><span class="font3" style="font-style:italic;">|y</span><span class="font1" style="font-style:italic;">k</span><span class="font3" style="font-style:italic;">)</span><span class="font3">, dikali dengan probabilitas kategori kelas </span><span class="font3" style="font-style:italic;">P(y</span><span class="font1" style="font-style:italic;">k</span><span class="font3" style="font-style:italic;">)</span><span class="font3">. Dari hasil kali tersebut kemudian dilakukan pembagian terhadap probabilitas kemunculan dokumen </span><span class="font3" style="font-style:italic;">P(x</span><span class="font1" style="font-style:italic;">a</span><span class="font3" style="font-style:italic;">).</span><span class="font3"> Sehingga didapatkan rumus penghitungan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> dituliskan pada Persamaan (2) [2].</span></p>
<div>
<p><span class="font10" style="font-style:italic;font-variant:small-caps;">p</span><span class="font10" style="font-style:italic;"> (y<sub>k</sub></span><span class="font5" style="font-style:italic;">∣</span><span class="font10" style="font-style:italic;"><sup>χ</sup><sub>a</sub>)</span></p>
</div><br clear="all">
<div>
<p><span class="font2" style="font-weight:bold;">fo⅛y‰</span><span class="font12" style="font-weight:bold;">∣</span><span class="font2" style="font-weight:bold;">∆2 </span><span class="font10" style="font-style:italic;">P(χ<sub>a</sub>)</span></p>
</div><br clear="all">
<div>
<p><span class="font3">(2)</span></p>
</div><br clear="all">
<p><span class="font3">Kemudian dilakukan proses pemilihan kelas yang optimal maka dipilih nilai peluang terbesar dari setiap probabilitas kelas yang ada. Sehingga didapatkan rumus untuk memilih nilai terbesar pada Persamaan (3) [10].</span></p>
<div>
<p><span class="font10" style="font-style:italic;">y(x<sub>l</sub>)</span><span class="font10"> = arg max</span></p>
</div><br clear="all">
<div>
<p><span class="font10"><sup>p</sup>(y)∏√(w</span></p>
</div><br clear="all">
<div>
<p><span class="font3">(3)</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font3">b.</span><span class="font3" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Weighted Naive Bayes</span></p></li></ul>
<p><span class="font3">Menurut Hilden, Ferreira, dan Hall pembobotan atribut kelas dapat meningkatkan pengaruh prediksi [11][12][13]. Dengan memperhitungkan bobot atribut terhadap kelas, maka yang menjadi dasar ketepatan klasisifikasi bukan hanya probabilitas melainkan juga dari bobot setiap atribut terhadap kelas. Pembobotan </span><span class="font3" style="font-style:italic;">Naïve Bayes </span><span class="font3">dihitung dengan cara menambahkan bobot </span><span class="font3" style="font-style:italic;">w</span><span class="font1" style="font-style:italic;">i</span><span class="font3"> pada setiap atribut. Sehingga didapatkan rumus untuk pembobotan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> dituliskan pada Persamaan (4).</span></p>
<div>
<p><span class="font10">P(y,x) = P(y) ∏ P(X<sub>1</sub></span><span class="font11">∣</span><span class="font10">y)</span></p>
</div><br clear="all">
<div>
<p><span class="font6" style="font-style:italic;">w<sub>l</sub></span></p>
</div><br clear="all">
<div>
<p><span class="font3">(4)</span></p>
</div><br clear="all">
<p><span class="font3">Pembobotan dapat dirumuskan menggunakan </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> [10]. Dimana dari setiap atribut </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> dikali jumlah data </span><span class="font3" style="font-style:italic;">n</span><span class="font3"> kemudian dibagi dengan rata-rata </span><span class="font3" style="font-style:italic;">Gain Ratio </span><span class="font3">semua atribut.</span></p>
<div>
<p><span class="font6" style="font-style:italic;">W<sub>1</sub></span><span class="font10"> =</span></p>
</div><br clear="all">
<p><span class="font10" style="font-style:italic;">GainRatio(I) </span><span class="font5" style="font-style:italic;">↑</span><span class="font10" style="font-style:italic;">~<sub>n </sub>a ∑ι=ι GamRatio(ι)</span></p>
<div>
<p><span class="font3">(5)</span></p>
</div><br clear="all">
<p><span class="font3">Atribut dari </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> sendiri merupakan hasil bagi dari </span><span class="font3" style="font-style:italic;">Mutual</span><span class="font3"> Information dan </span><span class="font3" style="font-style:italic;">Entropy</span><span class="font3">. </span><span class="font3" style="font-style:italic;">Mutual Information</span><span class="font3"> (MI) merupakan nilai ukur yang menyatakan keterikatan atau ketergantungan antara dua variabel atau lebih. Unit pengukur yang umum digunakan untuk menghitung MI adalah bit, sehingga menggunakan logaritma (</span><span class="font3" style="font-style:italic;">log)</span><span class="font3"> basis 2. Secara formal, MI digunakan antara 2 variabel A dan B yang didefinisikan oleh Kulback dan Leibler [1 4], [1 5] . Selain MI, </span><span class="font3" style="font-style:italic;">Entropy</span><span class="font3"> digunakan sebagai pembagi dari MI yang digunakan untuk menentukan atribut mana yang terbaik atau optimal. Penghitungan </span><span class="font3" style="font-style:italic;">Mutual Information</span><span class="font3"> dituliskan pada Persamaan 6 [1 4][15].</span></p>
<p><span class="font10" style="font-style:italic;">MI(x<sub>b</sub>y) = ∑</span><span class="font3"> ∑ </span><span class="font10" style="font-style:italic;">P(<sup>x</sup></span><span class="font6" style="font-style:italic;"><sup>ι</sup></span><span class="font10" style="font-style:italic;">,<sup>y</sup>^<sup>0g</sup>l<sup>p</sup>~</span><span class="font5" style="font-style:italic;">∖</span><span class="font10" style="font-style:italic;">P^ </span><span class="font3">Z-JyZ-Jx<sub>1</sub> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font10" style="font-style:italic;">P(x<sub>1</sub>)P(y)</span></p>
<p><span class="font3">(6)</span></p>
<p><span class="font3">Sebelum mendapatkan nilai </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> dilakukan pencarian nilai </span><span class="font3" style="font-style:italic;">Entropy E</span><span class="font3">. </span><span class="font3" style="font-style:italic;">Entropy </span><span class="font3">digunakan untuk menentukan seberapa informatif sebuah masukan atribut untuk menghasilkan keluaran atribut. Penghitungan </span><span class="font3" style="font-style:italic;">Entropy</span><span class="font3"> dengan menjumlahkan probabilitas dituliskan pada Persamaan (7).</span></p>
<p><span class="font10" style="font-style:italic;">E(x<sub>l</sub>) = ∑ ^x^iog—<sup>1</sup>—</span></p>
<p><span class="font10" style="font-style:italic;">^-<sup>i</sup> X</span><span class="font6" style="font-style:italic;">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font10" style="font-style:italic;"><sup>P(x</sup></span><span class="font6" style="font-style:italic;">1)</span></p>
<p><span class="font3">(7)</span></p>
<p><span class="font3">Maka dari itu penghitungan </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> adalah hasil dari penghitungan </span><span class="font3" style="font-style:italic;">Mutual Information</span><span class="font3"> dibagi dengan hasil penghitungan </span><span class="font3" style="font-style:italic;">Entropy</span><span class="font3"> Penghitungan </span><span class="font3" style="font-style:italic;">Gain Ratio </span><span class="font3">dituliskan pada Persamaan (8).</span></p>
<p><span class="font10" style="font-style:italic;"><sup>M</sup>i(x<sub>b</sub>y}</span><span class="font10"> &nbsp;∑</span><span class="font3"><sup>y</sup></span><span class="font10">∑</span><span class="font3"><sup>xι </sup></span><span class="font10" style="font-style:italic;"><sup>P(x</sup></span><span class="font6" style="font-style:italic;">^</span><span class="font10" style="font-style:italic;"><sup>,y) logp</sup>(⅛p(<sub>y</sub>)</span></p>
<p><span class="font10" style="font-style:italic;">GainRatio (ι) = ———-— =</span><span class="font8">----------------z-----</span></p>
<p><span class="font10" style="font-style:italic;"><sup>ε</sup>(<sup>x</sup></span><span class="font6" style="font-style:italic;">'</span><span class="font10" style="font-style:italic;">) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font14" style="font-variant:small-caps;">Σ</span><span class="font9" style="font-variant:small-caps;">,</span><span class="font13" style="font-variant:small-caps;">i</span><span class="font7" style="font-variant:small-caps;"><sup>p</sup></span><span class="font14" style="font-variant:small-caps;">(x<sub>1</sub>)10<sub>8</sub>p⅛)</span></p>
<div>
<p><span class="font3">(8)</span></p>
</div><br clear="all">
<p><span class="font3">Proses penghitungan </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> menggunakan </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> dibagi menjadi dua tahap. Tahap pertama adalah proses </span><span class="font3" style="font-style:italic;">training</span><span class="font3"> (pelatihan). Pada proses </span><span class="font3" style="font-style:italic;">training </span><span class="font3">diambil data latih kemudian dilakukan preprocessing. Setelah itu hitung peluang kata </span><span class="font3" style="font-style:italic;">(term)</span><span class="font3"> perkategori dan hitung peluang kategori </span><span class="font3" style="font-style:italic;">(class).</span><span class="font3"> Kemudian dicari nilai </span><span class="font3" style="font-style:italic;">Gain Ratio </span><span class="font3">menggunakan Persamaan 8. Proses </span><span class="font3" style="font-style:italic;">training</span><span class="font3"> dapat dilihat pada Gambar 4.</span></p><img src="https://jurnal.harianregional.com/media/19506-4.jpg" alt="" style="width:132pt;height:361pt;">
<p><span class="font3" style="font-weight:bold;">Gambar 4. </span><span class="font3">Proses Training</span></p>
<p><span class="font3">Tahap kedua adalah proses </span><span class="font3" style="font-style:italic;">testing</span><span class="font3"> (pelatihan). Pada proses </span><span class="font3" style="font-style:italic;">testing</span><span class="font3"> diambil data uji kemudian dilakukan preprocessing. Setelah itu ambil nilai </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> tiap kata dan kategori</span><span class="font3" style="font-style:italic;">.</span><span class="font3"> Setelah itu, dilakukan proses perankingan kata sebanyak </span><span class="font3" style="font-style:italic;">R</span><span class="font3"> (jumlah kata yang ditentukan)</span><span class="font3" style="font-style:italic;">.</span><span class="font3"> Dari kata sebanyak </span><span class="font3" style="font-style:italic;">R</span><span class="font3"> yang diambil dilakukakn proses penghitungan </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3">. Kemudian dicari nilai </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> menggunakan Persamaan 4. Proses </span><span class="font3" style="font-style:italic;">testing</span><span class="font3"> dapat dilihat pada Gambar 5.</span></p>
<div><img src="https://jurnal.harianregional.com/media/19506-5.jpg" alt="" style="width:132pt;height:396pt;">
</div><br clear="all">
<p><span class="font3" style="font-weight:bold;">Gambar 5. </span><span class="font3">Proses Testing</span></p>
<div>
<p><span class="font3">c.</span></p>
</div><br clear="all">
<p><span class="font3" style="font-style:italic;">Metode Evaluasi</span></p>
<p><span class="font3">Pada tahap evaluasi bertujuan untuk mengetahui tingkat akurasi dari hasil penggunaan metode </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3">. Dari evaluasi akan tersedia informasi mengenai seberapa besar akurasi yang telah dicapai. Pada proses pengujian dikenal sebagai Matriks </span><span class="font3" style="font-style:italic;">Confusion</span><span class="font3"> yang merepresentasikan kebenaran dari sebuah klasifikasi. Tabel Matriks </span><span class="font3" style="font-style:italic;">Confusion</span><span class="font3"> dapat dilihat pada Tabel 1.</span></p>
<p><span class="font3" style="font-weight:bold;">Tabel 1. </span><span class="font3">Matriks Confusion</span></p>
<table border="1">
<tr><td colspan="4" style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Hasil Prediksi</span></p>
<p><span class="font3" style="font-weight:bold;">+ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Kenyataan</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">+</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-style:italic;">True Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-style:italic;">False Positive</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3" style="font-style:italic;">False Negative</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3" style="font-style:italic;">True Negative</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<p><span class="font0">■</span><span class="font3" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;True Positive</span><span class="font3"> (TP) menunjukkan bahwa dokumen yang &nbsp;termasuk dalam</span></p></li></ul>
<p><span class="font3">hasil pengelompokkan oleh sistem memang merupakan anggota kelas.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">■ &nbsp;</span><span class="font3" style="font-style:italic;">False Positive</span><span class="font3"> (FP) menunjukkan bahwa dokumen yang &nbsp;termasuk dalam</span></p></li></ul>
<p><span class="font3">hasil pengelompokkan oleh sistem ternyata seharusnya bukan merupakan anggota kelas.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">■ &nbsp;</span><span class="font3" style="font-style:italic;">False Negative</span><span class="font3"> (FN) menunjukkan bahwa dokumen yang tidak termasuk dalam</span></p></li></ul>
<p><span class="font3">hasil pengelompokkan oleh sistem ternyata seharusnya merupakan anggota kelas.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">■ &nbsp;</span><span class="font3" style="font-style:italic;">True Negative</span><span class="font3"> (TN) menunjukkan bahwa dokumen yang tidak termasuk dalam</span></p></li></ul>
<p><span class="font3">hasil pengelompokkan oleh sistem ternyata seharusnya bukan merupakan anggota kelas.</span></p>
<p><span class="font3">Untuk menghitung tingkat akurasi digunakan Persamaan 9 [16].</span></p>
<div>
<p><span class="font10" style="font-style:italic;">Akurasi =</span></p>
</div><br clear="all">
<p><span class="font10" style="font-style:italic;">TP</span><span class="font10"> + </span><span class="font10" style="font-style:italic;">TN</span></p>
<p><span class="font10" style="font-style:italic;">TP + TN + FP + FN</span></p>
<div>
<p><span class="font3">(9)</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font3" style="font-weight:bold;"><a name="bookmark13"></a>3. &nbsp;&nbsp;&nbsp;Eksperimen dan Hasil</span></h2></li></ul>
<p><span class="font3">Pengujian hasil menggunakan metode </span><span class="font3" style="font-style:italic;">Wighted Naïve Bayes</span><span class="font3"> dilakukan dengan membandingkan hasil percobaan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> tanpa menggunakan pembobotan. Perbandingan dilakukan terhadap dokumen berita sejumlah 65 dokumen pada uji coba 1 dan 145 dokumen pada uji coba 2. Hasil yang dibandingkan adalah akurasi data yang dihasilkan dengan menghitung selisih antara </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> dan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> biasa. Penghitungan akurasi tersebut dapat dilihat pada Persamaan 9.</span></p>
<p><span class="font3">Dilakukan uji coba 1 terhadap metode usulan dengan menggunakan data latih sebanyak 35 dokumen dan data uji sebanyak 30 dokumen. Pada uji coba 2, data uji yang digunakan sebanyak 110 dokumen dan data latih yang digunakan sama seperti uji coba 1. Dimana, pada data latih terdapat 7 kategori, yaitu Sepak Bola, Otomotif, Kesehatan, Teknologi, Ekonomi, Politik, dan Hukum. Pada masing-masing kategori berisi 5 dokumen.</span></p>
<p><span class="font3">Dari hasil uji coba 1 didapatkan hasil akurasi </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> sebesar 92% sedangkan pada Weighted </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> sebesar 94%. Selain itu, dari hasil uji coba 2 didapatkan hasil akurasi </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> sebesar 92% dan </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> sebesar 84%. Hasil akurasi dapat dilihat pada Tabel 2.</span></p>
<p><span class="font3" style="font-weight:bold;">Tabel 2. </span><span class="font3">Hasil Akurasi</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Metode</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Akurasi %</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Uji Coba 1</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Uji Coba 2</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Naïve Bayes</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">92</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">92</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Weighted Naïve Bayes</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">94</span></p></td><td style="vertical-align:top;">
<p><span class="font2">84</span></p></td></tr>
</table>
<p><span class="font3">Berdasarkan uji coba 2, dilakukan proses pemilihan fitur sebanyak </span><span class="font3" style="font-style:italic;">R</span><span class="font3"> (50, 30, dan 10 </span><span class="font3" style="font-style:italic;">term </span><span class="font3">terbaik). Dari hasil pemilihan fitur menggunakan 50 dan 30 </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> terbaik didapatkan akurasi sebesar 91% untuk metode usulan dan 95% untuk metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> biasa. Sedangkan ketika menggunakan 10 </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> terbaik didapatkan akurasi sebesar 94% untuk metode usulan dan 91% untuk metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> biasa. Hasil uji coba terhadap pemilihan fitur dapat dilihat pada Tabel 3.</span></p>
<p><span class="font3" style="font-weight:bold;">Tabel 3. </span><span class="font3">Pemilihan Fitur</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Term Terbaik</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Metode Usulan %</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Naïve Bayes %</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">50</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">91</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">95</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">30</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">91</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">95</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">94</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">91</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font3" style="font-weight:bold;"><a name="bookmark15"></a>4. &nbsp;&nbsp;&nbsp;Pembahasan</span></h2></li></ul>
<p><span class="font3">Dari hasil uji coba 1 didapatkan nilai akurasi </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> sebesar 92% sedangkan nilai akurasi untuk metode yang diusulkan atau </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> sebesar 94%. Hasil metode yang diusulkan lebih tinggi disebabkan oleh pemberian bobot pada probabilitas dari setiap kata pada dokumen terhadap kategori. Pemberian bobot pada probabilitas mengakibatkan jarak antar peluang satu kata terhadap kategori semakin jauh. Hasil dari penelitian yang diusulkan sesuai</span></p>
<p><span class="font3">dengan penelitian Hilden, Ferreira dan Hall yang berpendapat bahwa pembobotan atribut kelas dapat meningkatkan pengaruh prediksi [11][12][13].</span></p>
<p><span class="font3">Akan tetapi pada uji coba 2, akurasi pada metode yang diusulkan cenderung rendah dibandingkan dengan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> biasa. Hal ini dikarenakan </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> yang sering muncul pada seluruh kategori dokumen menghasilkan nilai </span><span class="font3" style="font-style:italic;">Gain Ratio</span><span class="font3"> yang tinggi dan mengakibatkan terjadinya kesalahan klasifikasi. Setelah diketahui hasil akurasi pada uji coba 2 rendah. Maka, dilakukan proses pemilihan fitur terbaik untuk mengatasi kesalahan klasifikasi yang disebabkan oleh sering munculnya </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> pada seluruh dokumen. Dari hasil uji coba pemilihan fitur menggunakan 50 dan 30 </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> terbaik didapatkan akurasi sebesar 91% untuk metode usulan dan 95% untuk metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> biasa. Hal ini dikarenakan </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> yang sering muncul pada kelas lain terdapat pula pada kelas yang diuji. Sedangkan ketika menggunakan 10 </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> terbaik didapatkan akurasi sebesar 94% untuk metode usulan dan 91% untuk metode </span><span class="font3" style="font-style:italic;">Naïve Bayes </span><span class="font3">biasa. Hal ini dikarenakan </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> yang digunakan pada kelas yang diuji merepresentasikan kelas tersebut. Sehingga pada uji coba ini diketahui bahwa pemilihan fitur terbaik dapat mengurangi jumlah </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> yang sering muncul pada kelas lain.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font3" style="font-weight:bold;"><a name="bookmark17"></a>5. &nbsp;&nbsp;&nbsp;Kesimpulan</span></h2></li></ul>
<p><span class="font3">Metode </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> dapat mengoptimalkan nilai akurasi metode </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> biasa. Hal ini dapat dilihat dari hasil akurasi </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> sebesar 94% dibandingkan</span></p>
<p><span class="font3">dengan </span><span class="font3" style="font-style:italic;">Naïve Bayes</span><span class="font3"> biasa sebesar 92%. </span><span class="font3" style="font-style:italic;">Weighted Naïve Bayes</span><span class="font3"> dapat menghasilkan tingkat akurasi yang lebih tinggi dikarenakan setiap probabilitas dari atribut diberi bobot yang</span></p>
<p><span class="font3">menghasilkan nilai yang lebih tinggi. Ketika dilakukan pemilihan fitur mengunkan 10 </span><span class="font3" style="font-style:italic;">term</span><span class="font3"> terbaik didapatkan akurasi sebesar 94% untuk metode usulan dan 91% untuk metode Naïve </span><span class="font3" style="font-style:italic;">Bayes </span><span class="font3">biasa. Hal ini dapat disimpulkan bahwa pemilihan fitur dapat mengatasi kesalahan klasifikasi.</span></p>
<h2><a name="bookmark18"></a><span class="font3" style="font-weight:bold;"><a name="bookmark19"></a>Daftar Pustaka</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;U. S. F. dan W. Service, “Definitions of the Terms and Phrases of Amer-,” </span><span class="font3" style="font-style:italic;">English</span><span class="font3">, 2013.</span></p></li></ul>
<p><span class="font3">[Online]. Available: </span><a href="http://www.fws.gov/stand/defterms.html"><span class="font3">http://www.fws.gov/stand/defterms.html</span></a><span class="font3">. [Accessed: 12-Dec-2015].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;L. Tenenboim, B. Shapira, and P. Shoval, “Ontology-based classification of news in an</span></p></li></ul>
<p><span class="font3">electronic newspaper,” </span><span class="font3" style="font-style:italic;">Inf. Syst.</span><span class="font3">, 2008.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;D. D. Lewis, </span><span class="font3" style="font-style:italic;">Naive(Bayes)at forty: The independence assumption in information</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">retrieval</span><span class="font3">. 1998.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;&nbsp;D. J. Hand and K. M. Yu, “Idiot’s Bayes - Not so stupid after all?,” </span><span class="font3" style="font-style:italic;">Int. Stat. Rev.</span><span class="font3">, 2001.</span></p></li>
<li>
<p><span class="font3">[5] &nbsp;&nbsp;&nbsp;I. Konokenko, “Comparison of inductive and naive Bayesian learning approaches to</span></p></li></ul>
<p><span class="font3">automatic knowledge acquisition,” </span><span class="font3" style="font-style:italic;">Current trends Knowledge Acquisition</span><span class="font3">, pp. 190–197, 1990.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[6] &nbsp;&nbsp;&nbsp;P. Langley and S. Sage, “Induction of Selective Bayesian Classifiers,” </span><span class="font3" style="font-style:italic;">Proceedings</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Tenth International Conference on Uncertainty in Artificial Inteligence</span><span class="font3">, 1994.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[7] &nbsp;&nbsp;&nbsp;A. Hamzah, “Klasifikasi Teks Dengan Naïve Bayes Classifier (NBC) Untuk</span></p></li></ul>
<p><span class="font3">Pengelompokan Teks Berita Dan Abstract Akademis,” </span><span class="font3" style="font-style:italic;">Prosiding Seminar Nasional Aplikasi Sains dan Teknologi Periode III</span><span class="font3">, 2012.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[8] &nbsp;&nbsp;&nbsp;S. Garcia, “Search Engine Optimisation Using Past Queries,” School of Computer</span></p></li></ul>
<p><span class="font3">Science and Information Technology, 2007.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[9] &nbsp;&nbsp;&nbsp;&nbsp;P. Baldi, P. Frasconi, and P. Smyth, “Modeling the Internet and the Web: Probabilistic</span></p></li></ul>
<p><span class="font3">Methods and Algorithms,” </span><span class="font3" style="font-style:italic;">Information Processing and Management</span><span class="font3">, 2003.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[10] &nbsp;&nbsp;H. Zhang and S. Sheng, “Learning weighted naive bayes with accurate ranking,” in</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Proceedings - Fourth IEEE International Conference on Data Mining, ICDM 2004</span><span class="font3">, 2004.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[11] &nbsp;&nbsp;J. Hilden and B. Bjerregaard, </span><span class="font3" style="font-style:italic;">Computer-aided diagnosis and the atypical case</span><span class="font3">. North</span></p></li></ul>
<p><span class="font3">Holland Publishing Co., 1976.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[12] &nbsp;&nbsp;&nbsp;J. T. A. S. Ferreira, D. G. T. Denison, and D. J. Hand, “Weighted naive Bayes modelling for data mining,” </span><span class="font3" style="font-style:italic;">citeseerx</span><span class="font3">, pp. 1–20, 2001.</span></p></li>
<li>
<p><span class="font3">[13] &nbsp;&nbsp;&nbsp;M. Hall, “A Decision Tree-Based Attribute Weighting Filter for Naive Bayes,” </span><span class="font3" style="font-style:italic;">ACM</span><span class="font3">, vol. 20, no. 2, pp. 120–126, 2007.</span></p></li>
<li>
<p><span class="font3">[14] &nbsp;&nbsp;S. Kullback and R. A. Leibler, “On Information and Sufficiency,” </span><span class="font3" style="font-style:italic;">The Annals of</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Mathematical Statistic</span><span class="font3">, vol. 22, no. 1, pp. 79–86, 1951.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[15] &nbsp;&nbsp;A. Renyi, “On Information and &nbsp;Sufficiency,” in </span><span class="font3" style="font-style:italic;">Proceedings of the 4th Berkeley</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">symposium on Mathematics</span><span class="font3">, 1961, pp. 547–561.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[16] &nbsp;&nbsp;&nbsp;N. Hermaduanti and S. Kusumadewi, “Sistem Pendukung Keputusan Berbasis SMS Untuk Menentukan Status Gizi Dengan Metode K-Nearest Neighbor,” in </span><span class="font3" style="font-style:italic;">Seminar Nasional Aplikasi Teknologi Informasi (SNATI)</span><span class="font3">, 2008, pp. 49–56.</span></p></li></ul>
<p><span class="font3">30</span></p>