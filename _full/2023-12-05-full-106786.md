---
layout: full_article
title: "The Influence Of Applying Stopword Removal And Smote On Indonesian Sentiment Classification"
author: "Arif Bijaksana Putra Negara"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-106786 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-106786"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-106786"  
comments: true
---

<p><span class="font3" style="font-weight:bold;">LONTAR KOMPUTER VOL. 14, NO. 3 DECEMBER 2023</span></p>
<p><span class="font3" style="font-weight:bold;">DOI : 10.24843/LKJITI.2023.v14.i03.p05</span></p>
<p><span class="font3" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 158/E/KPT/2021</span></p>
<p><span class="font3" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font3" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font5" style="font-weight:bold;"><a name="bookmark1"></a>The Influence Of Applying Stopword Removal And Smote On Indonesian Sentiment Classification</span></h1>
<p><span class="font3">Arif Bijaksana Putra Negara<sup>a1</sup></span></p>
<p><span class="font3"><sup>a</sup>Universitas Tanjungpura</span></p>
<p><a href="mailto:1arifbpn@untan.ac.id"><span class="font3"><sup>1</sup>arifbpn@untan.ac.id</span></a></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font3" style="font-style:italic;">Information, like public opinions or responses, can be obtained through Twitter tweets. These opinions can expressed as a sentiment. Sentiments can be positive, neutral, or negative. Sentiment analysis (opinion mining) on a text can performed through text classification. This research aims to determine the influence of implementing Stopword Removal and SMOTE on the sentiment classification model for Indonesian tweets. The algorithms used in this research are Logistic Regression and Random Forest. Based on the evaluation, the best classification model in this research was achieved by implementing the Random Forest algorithm along with SMOTE, with an f1-score value of 75.03%. Meanwhile, implementing the Random Forest algorithm and Stopword Removal achieved the worst classification model, with an f1-score value of 68.09%. Implementing Stopword Removal in both algorithms has a negative impact in the form of a decrease in the resulting f1-score. Meanwhile, the performance of SMOTE provides a positive impact in the form of an increase in the resulting f1-score. This happened since Stopword Removal could reduce information and alter the meaning of processed tweets, causing the tweet to lose its sentiment.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font3" style="font-style:italic;">Sentiment Analysis, Stopword Removal, Grid Search, SMOTE, Logistic Regression, Random Forest</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark2"></a><span class="font3" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h3></li></ul>
<p><span class="font3">In producing information, a collection of factual and actual data must be managed. The dissemination of information can be considered highly rapid, owing to the abundance of digital channels/platforms accessible for expressing ideas and opinions. Social media stands as the most widely used digital platform, thus making disseminating information through social media more efficient and swifter. Twitter is a popular social media platform in Indonesia. Twitter provides a space for its users to interact/discuss through short text messages as a means of its utilization. Thus, the data generated is known as tweets, which, when processed, will result in information. One is public sentiment/opinion, which can serve as a reference for reciprocal societal responses. Sentiments can classified into three categories: positive, neutral, and negative. Sentiment data is typically gathered manually, including methods such as distributing questionnaires. However, in practice, it can be pretty time-consuming and labor-intensive, so tweet data retrieval is more efficient to use. Sentiment classification can be achieved by applying classification methods to text, known as sentiment analysis or opinion mining.</span></p>
<p><span class="font3">Generally, there are four stages of sentiment analysis: text preprocessing, data vectorization, modeling, and evaluation. Data Cleaning and Modeling stages play a crucial role in sentiment analysis, as both generate the dataset and classification model. The dataset that has been developed is used as training data to obtain a classification model. An accuracy evaluation is performed on this classification model. The text preprocessing stage is conducted on tweet data to handle noise or disturbances, such as using non-standard words, abbreviations, and slang. Stopword Removal is one of the methods commonly applied at the Text Preprocessing stage [1]. Stopword Removal involves eliminating words that are pretty common and frequently appear but do not significantly impact the meaning of a text or sentence. The implementation of Stopword Removal is expected to yield a better dataset. Several parties have conducted research related to the performance of Stopword Removal. This includes an experiment involving the application of Stopword Removal and Stemming using the LSTM (Long Short Term-Memory) algorithm,</span></p>
<p><span class="font3">where the highest accuracy value was obtained when Stopword Removal and Stemming were not applied, with an accuracy score and f1-score of 0.82 [1].</span></p>
<p><span class="font3">The modeling stage involves generating a classification model from the input dataset and utilizing a classification algorithm. The parameters of the algorithms are optimized by applying Hyperparameter Tuning. Grid Search is an algorithm that can select a combination of hyperparameters to achieve the highest accuracy value. The parameter values that are obtained are implemented in the classification model. Classification modeling is performed using the Logistic Regression and Random Forest algorithms. This is because, in their implementation, both algorithms can produce good accuracy values and process large amounts of data [2]. Research on implementing Grid Search as a Hyperparameter compared KNN and Logistic Regression Algorithms for classifying emotions in Indonesian tweets with the performance of TF-IDF and Grid Search [3]. The highest accuracy result value was achieved with the Logistic Regression algorithm and the implementation of TF-IDF and Grid Search, yielding an accuracy and f1-score of 65% and 66%, respectively. In General, the performance of methods and algorithms applied for sentiment classification tends to be suboptimal when the utilized dataset is imbalanced [4]. Imbalanced data was handled with SMOTE (Synthetic Minority Oversampling Technique). SMOTE is a technique used to address imbalanced data issues by generating synthetic new data from the minority type in the dataset, thus achieving a balance between classes [5]. With the presence of SMOTE, the dataset will not be biased towards the majority class. Therefore, it is expected to optimize the performance of classification methods and algorithms. Research related to the implementation of SMOTE was the analysis of sentiment in Tokopedia's Twitter tweets using the Naïve Bayes and Random Forest algorithms; it was found that the performance of SMOTE could increase the accuracy values for the Naïve Bayes and Random Forest algorithms by 3.4% and 1.55% respectively. The highest accuracy value achieved by the Random Forest algorithm with the implementation of SMOTE is 86.89% [6].</span></p>
<p><span class="font3">Based on the presented description, this research will analyze the impact of applying Stopword Removal and SMOTE on the resulting f1-score values. This research employs two classification algorithms, Logistic Regression and Random Forest, for machine classification modeling. Then, the f1-score values produced by both algorithms will be compared to determine which algorithm's application is more optimal for performing sentiment analysis on Indonesian language tweets.</span></p><img src="https://jurnal.harianregional.com/media/106786-1.jpg" alt="" style="width:314pt;height:240pt;">
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font3" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h3></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">Figure 1.</span><span class="font3"> &nbsp;&nbsp;&nbsp;Research Methods</span></p></li></ul>
<p><span class="font3">Figure 1 shows the steps of methods that were carried out in this research. There are eight consecutive method stages to be performed. The process starts with data gathering, text preprocessing, vectorization, balancing dataset, splitting dataset, modeling, validation, and evaluation.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark6"></a><span class="font3" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Data Gathering</span></h3></li></ul>
<p><span class="font3">In data gathering, the data collected consists of Indonesian text data taken from the research conducted by Ridi Ferdiana, Fahim Jatmiko, Desi Dwi Purwanti, Artmita Sekar Tri Ayu, and Wiliam Fajar Dicka in 2019, entitled 'Indonesian Dataset for Sentiment Analysis' [7]. The data was collected using the Twitter Streaming API over four months, starting from September to December 2018, using Indonesian standard conjunction words as keywords such as adalah, yaitu, juga, and seperti. This dataset consists of 10,820 labeled sentences categorized into three sentiment classes: 3,228 positive sentences, 3,556 neutral sentences, and 4,036 negatives. The Indonesian Sentiment Analysis Dataset is an Excel dataset stored in (.csv) format. The sentence samples in the dataset can be seen in Table 1.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 1. </span><span class="font3">Dataset Indonesia</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">No</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Tweet</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Label</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">persahabatan ialah saat kita memberi namun tak mengharapkan balasan</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Netral</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Rajin makan tahu bisa pintar</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Positif</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">buset dah aku udah pasrah aja klo ketemu yg beginian</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Negatif</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark8"></a><span class="font3" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Text Pre-processing</span></h3></li></ul>
<p><span class="font3">Text preprocessing refers to steps or techniques used to clean, organize, and transform raw text into a more easily processed form by natural language processing (NLP) models or other computational systems. The goal is to enhance the quality of text data and facilitate further analysis or processing. This involves lowercasing, tokenization, text cleaning, stopword removal, stemming or lemmatization, normalization, and vectorization. The ultimate aim is to simplify and reduce the complexity of the text, making it easier for models to extract relevant patterns or information.</span></p>
<p><span class="font3">The text preprocessing steps in this research include the following.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">a. &nbsp;&nbsp;&nbsp;Cleaning</span></p></li></ul>
<p><span class="font3">In the cleaning process, the tweets in the Indonesian Dataset for Sentiment Analysis are cleaned by removing punctuation or delimiters, numbers, symbols, and usernames [1].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">b. &nbsp;&nbsp;&nbsp;Case Folding</span></p></li></ul>
<p><span class="font3">In the case folding process, the characters of each word in the data are standardized by converting all letters in each word to lowercase [8].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">c. &nbsp;&nbsp;&nbsp;Normalization</span></p></li></ul>
<p><span class="font3">In the normalization process, changes and language normalization are applied to words, where non-standard words, abbreviations, and words in colloquial and slang language are transformed into words that adhere to the proper rules of writing in the Indonesian language, as per the guidelines of the 'Kamus Besar Bahasa Indonesia' (KBBI) [9].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">d. &nbsp;&nbsp;&nbsp;Tokenizing</span></p></li></ul>
<p><span class="font3">In tokenizing, the sentences are split into words or tokens using white spaces or spaces [10].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">e. &nbsp;&nbsp;&nbsp;Stopword Removal</span></p></li></ul>
<p><span class="font3">In the stopword removal process, words that often commonly occur but are insignificant and irrelevant are removed, such as conjunctions and possessive and personal pronouns [1].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">f. &nbsp;&nbsp;&nbsp;Stemming</span></p></li></ul>
<p><span class="font3">In the stemming process, the words are transformed into base forms by removing prefixes and suffixes [11].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">g. &nbsp;&nbsp;&nbsp;Rejoin</span></p></li></ul>
<p><span class="font3">In the rejoin process, the words or tokens resulting from the stemming process are recombined into a complete sentence [11]</span></p>
<p><span class="font3">The results of the Data Cleaning stage or text Preprocessing on the sample dataset can be seen in Table 2.</span></p>
<p><span class="font3" style="font-weight:bold;text-decoration:underline;">Table 2. </span><span class="font3" style="text-decoration:underline;">Text Preprocessing</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">No</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Teks &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Preprocessing</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Tampang kriminal ,pasti malu keluarganya punya anak gak punya akhlak begini. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3"><sup>e au</sup></span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Tampang kriminal pasti malu keluarganya punya anak</span></p>
<p><span class="font2">Cleaning gak punya akhlak begini</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">3</span></p></td><td style="vertical-align:top;">
<p><span class="font2">tampang kriminal pasti malu keluarganya punya anak</span></p>
<p><span class="font2">gak punya akhlak begini &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3"><sup>ase o ng</sup></span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">tampang kriminal pasti malu keluarganya punya anak</span></p>
<p><span class="font2">Normalization</span></p>
<p><span class="font2">enggak punya akhlak begini</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">['tampang', 'kriminal', 'pasti', 'malu', 'keluarganya', 'punya',</span></p>
<p><span class="font2">'anak', 'enggak', 'punya', 'akhlak', 'begini'] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3"><sup>Tokenizing</sup></span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">6</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">['tampang', &nbsp;&nbsp;'kriminal', &nbsp;&nbsp;'malu', &nbsp;&nbsp;'keluarganya', &nbsp;&nbsp;'anak',</span></p>
<p><span class="font3"><sub>'akhlak']</sub> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font2">Stopword Removal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">7</span></p>
<p><span class="font2">8</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">['tampang', 'kriminal', 'malu', 'keluarga', 'anak', 'akhlak'] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Stemming</span></p>
<p><span class="font2">tampang kriminal malu keluarga anak akhlak &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rejoin</span></p></td></tr>
</table>
<p><span class="font3" style="font-weight:bold;">2.3. Frequency Distribution</span></p>
<p><span class="font3">Frequency Distribution is carried out to determine the number of occurrences or frequency of a particular word. This research performs the frequency counting process using the FreqDist function in the NLTK library. Frequency distribution for text preprocessing data and text</span></p><img src="https://jurnal.harianregional.com/media/106786-2.jpg" alt="" style="width:429pt;height:164pt;">
<p><span class="font3">preprocessing + stopword removal data can be seen in Figure 2.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">Figure 2.</span><span class="font3"> &nbsp;&nbsp;&nbsp;Frequency Distribution</span></p></li></ul>
<p><span class="font3">The number of vocabulary and tokens in text preprocessing data and text preprocessing + stopword removal data can be found in Table 3.</span></p>
<table border="1">
<tr><td colspan="2" style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">Table 3. </span><span class="font3">Detail Vocabulary &amp;&nbsp;Token</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:bottom;">
<p><span class="font2">Total Vocabulary &amp;&nbsp;Token Dataset</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Total Vocabulary</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">7.617</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Total Token</span></p></td><td style="vertical-align:top;">
<p><span class="font2">148.961</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:bottom;">
<p><span class="font2">Total Vocabulary &amp;&nbsp;Token Dataset Stopword</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Total Vocabulary</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">7.506</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Total Token</span></p></td><td style="vertical-align:top;">
<p><span class="font2">73.623</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark10"></a><span class="font3" style="font-weight:bold;"><a name="bookmark11"></a>2.4. &nbsp;&nbsp;&nbsp;Vectorization</span></h3></li></ul>
<p><span class="font3">In Vectorization, word occurrence vectors in documents are created using TF-IDF weighting. TF-IDF (Term Frequency – Inverse Document Frequency) is a method of weighting and vectorizing each word (Term) in text data into numerical values by combining two modeling concepts, Term Frequency (TF) and Inverse Document Frequency (IDF). TF (Term Frequency) represents the</span></p>
<p><span class="font3">frequency of a word's occurrence in a sentence within text data. At the same time, IDF (Inverse Document Frequency) calculates how a word is distributed across text data [12]. This weighting is performed because computers only understand and process data numerically. In this research, the TF-IDF vectorization process is performed using the TfidfVectorizer() and fit_transform() functions in the Scikit-Learn library. The results of weighting/vectorization for the dataset Text Preprocessing and Text Preprocessing + Stopword Removal can be seen in Figure 3. It consists of three parts: document index, word index, TF-IDF score, and Vocabulary Content.</span></p>
<div>
<p><span class="font0">Vectorization Text Preprocessing</span></p><img src="https://jurnal.harianregional.com/media/106786-3.jpg" alt="" style="width:92pt;height:146pt;">
</div><br clear="all">
<p><span class="font8" style="font-weight:bold;">tΓj97≡93S∏665βi3i D.z9S7*seiwβ62i9n ft.192⅞lfl99849322U 0.298⅛M85176541β33 0.616717536B9SW92</span></p>
<p><span class="font8" style="font-weight:bold;">A. 2a2⅞Mθl ISSliftB ⅛.41MMSJMW0744S 0.24MJJ7532βe⅛5SJ 0.1 TMlMSBlTflBfltf?</span></p>
<p><span class="font8" style="font-weight:bold;">BaSiftftTftMTWTSMft ft .094M5Wβ⅛S994 MS 0,15295977065595*76 0.19481970837704607 ⅛.17i26901i8⅛797M 0.4⅛75mi62⅛97⅛i ft,76 7« 10)7966M7 5 0.4S6780B387150ββ46 0.3aj6WM974B89i56 0.5127827587963659 0,5S7507MMWH67 B-MMlTiTMtfSJftl S,«t«M5WM53571 B.51M215962J2S6i4</span></p>
<p><span class="font6">Vocabulary Content!</span></p>
<p><span class="font6">{⅛ku': 133, ⅛e∣a∣u': 6227, 'ρertahanin': 5344, 'dapaf: 1308, Tsalau': 2969, 'memeli 4234, ⅛ngga∣i': 1856, 'salah': 6030, <sup>,</sup>jadi<sup>,</sup>: 2 713, 'dengan': 13/9, 'begitu': 651, 'atas': 410, juga': 2695, Tau': 6868, 'kita': 3366, <sup>l</sup>Ckpan': 1391, ’bakal': 491. ⅛ah∣π<sup>r</sup>: 6031/lagi': 3634, ‘laki’: 3647. 'sejati'; 6195. 'akan<sup>-</sup>: 110, ’pergi': 5301, 'dan': 1294, 'cari': 1072. 'bahagia': 468, 'baru': 577, 'orang': 5001, 'lain': 3644, 'karena': 3049, ’rasa’; 5723, 'diri’: 1612, ‘lebih’: 3745, 'harga': 2 357, 'daripada': 1319, 'pasang': 5157, 'yang': 7560, 'khianat': 3 328,</span></p>
<div><img src="https://jurnal.harianregional.com/media/106786-4.jpg" alt="" style="width:124pt;height:156pt;">
<p><span class="font0">+ Stopword Removal</span></p>
<p><span class="font8" style="font-weight:bold;">∙.44Wl*7Maj)W55 B.3S134ll24∞l354e &gt;,WJ7tSWWlH Φ.2∙α7⅞7M2M461β5 ⅛. ISflMftflTeftBftflMTS ». JMMftJi WTfll SM ». IS J*4S3⅛SBa⅛54βM</span></p>
<p><span class="font8" style="font-weight:bold;">VantMPeneitfTt ft. SMHiSflTJHSftOi</span></p>
<p><span class="font8" style="font-weight:bold;">8.1KS3i9i4KTSM17 ft.l7V87i3JWβlfl844?</span></p>
<p><span class="font8" style="font-weight:bold;">i.iUJ»?lt»MtMJ 6.14577714a⅛935T9 ». 1 W5⅛⅛M6717&lt;</span><span class="font7" style="font-weight:bold;">∣</span><span class="font8" style="font-weight:bold;">M ∙, ITnnaMiiMBMa ft. 1*M9M6MJ1⅛41M e. law riB¾eeβJS323</span></p>
<p><span class="font8" style="font-weight:bold;">»,2Z»5*i»7ww7M5 !.MftOOSll 1 IftTTBflftft J-16M21M1¾99452 »-163Wt61«?il2fl6 ».14111462 WftTll J S lft⅛756β62inT87</span></p>
<p><span class="font6">Vocabulary content:</span></p>
<p><span class="font6">Vpertahanin': 5280, 'dapat': 1292, ’salah': 5962, 'atas': 403, 'tau': 6771, 'depan': 1372, 'salah in’: 5963, 'laki': 3603, 'sejati'; 6118, 'pergi'; 5238, 'cari'; 1056, 'bahagia': 460, 'orang'; 4940, 'harga'; 2332, 'pasang*; 5095, 'khianat': 3286, 'personal': 5277, 'agenda': 75, 'ρi∣ih': 5339<sub>r</sub> 'duluan': 1718, Theilaon': 6326<sub>r</sub> 'band': 511, Yavorif: 1905, 'generasi': 2108,</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">Figure 3.</span><span class="font3"> &nbsp;&nbsp;&nbsp;Vectorization Dataset</span></p></li></ul>
<ul style="list-style:none;"><li>
<h3><a name="bookmark12"></a><span class="font3" style="font-weight:bold;"><a name="bookmark13"></a>2.5. &nbsp;&nbsp;&nbsp;Balancing Dataset</span></h3></li></ul>
<p><span class="font3">The Indonesian Dataset for Sentiment Analysis is balanced using SMOTE. SMOTE or Synthetic Minority Oversampling Technique is a technique introduced by Nithes V Chawla to address imbalanced datasets [5]. By augmenting the minority class data through synthetic data generated from replicating the minority class instances, SMOTE balances the distribution of minority and majority class data in the dataset. The synthetic data or new samples are obtained by finding the k-nearest neighbors of each data point in the minority class and then creating replicas of those data points [13]. The result of dataset balancing can be seen in Figure 4.</span></p>
<div><img src="https://jurnal.harianregional.com/media/106786-5.jpg" alt="" style="width:426pt;height:29pt;">
<p><span class="font3" style="font-weight:bold;">Figure 4. </span><span class="font3">SMOTE</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h3><a name="bookmark14"></a><span class="font3" style="font-weight:bold;"><a name="bookmark15"></a>2.6. &nbsp;&nbsp;&nbsp;Splitting Dataset</span></h3></li></ul>
<p><span class="font3">The dataset is divided into two parts: training data and testing data. Training data is used to train the system to recognize the desired patterns. Testing data is used to evaluate the trained system's performance. In this research, the dataset is divided into 90% training data and 10% testing data; the splitting of training and testing data in the dataset can be seen in Figure 5.</span></p>
<div><img src="https://jurnal.harianregional.com/media/106786-6.jpg" alt="" style="width:428pt;height:32pt;">
<p><span class="font3" style="font-weight:bold;">Figure 5. </span><span class="font3">Splitting Dataset</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">2.7. &nbsp;&nbsp;&nbsp;Tuning Hyperparameter</span></p></li></ul>
<p><span class="font3">Tuning Hyperparameters is a process to optimize the performance of machine learning by selecting the best and optimal hyperparameters. Then, the chosen hyperparameters will be implemented in the machine learning classification algorithm modeling. In this research, the method for Hyperparameter Tuning is Grid Search. Grid Search is an algorithm that is applied to select the best variations of parameters by working through the process of combining all the input parameters. In its implementation, Grid Search typically involves defining a dictionary to store all the hyperparameters that need to be combined or searched for first. Then, this algorithm will perform model calculations based on all the stored hyperparameters. After that, the bestperforming hyperparameter combination for the machine modeling will be obtained based on the resulting f1-score values [14]. The hyperparameters that will undergo Hyperparameter Tuning are the C values in Logistic Regression and the values for Estimators, Max_depth, Max_features, and Criterion in Random Forest.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark16"></a><span class="font3" style="font-weight:bold;"><a name="bookmark17"></a>2.8. &nbsp;&nbsp;&nbsp;Modelling</span></h3></li></ul>
<p><span class="font3">Machine Modeling designed in this research employs two classification algorithm approaches: Logistic Regression and Random Forest. Sixteen classification machine models are constructed, consisting of eight Logistic Regression models and eight Random Forest models.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark18"></a><span class="font3" style="font-weight:bold;"><a name="bookmark19"></a>2.8.1. &nbsp;&nbsp;&nbsp;Logistic Regression</span></h3></li></ul>
<p><span class="font3">Logistic Regression is a data analysis technique in statistics designed to determine the relationship between a dependent variable and one or more independent variables. This technique is also known as a regression model. In applying logistic Regression, the dependent variable used is categorical (nominal or ordinal), while the independent variable is categorical or continuous [15].</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark20"></a><span class="font3" style="font-weight:bold;"><a name="bookmark21"></a>2.8.2. &nbsp;&nbsp;&nbsp;Random Forest</span></h3></li></ul>
<p><span class="font3">Random Forest is an Ensemble Classifier algorithm, which, in its implementation, combines several methods by combining multiple Decision Trees. Its functioning involves combining and performing majority voting on the outcomes of each Decision Tree, ultimately resulting in the final classification class/decision [16]. The Decision Trees constructed by Random Forest are formed through random data sampling and considering all the features. Decision Trees consist of root, internal, and leaf nodes created by considering information gained to determine the root node and rules[17].</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark22"></a><span class="font3" style="font-weight:bold;"><a name="bookmark23"></a>2.8.3. &nbsp;&nbsp;&nbsp;Testing Scenario</span></h3></li></ul>
<p><span class="font3">The testing scenarios constructed in this research consist of four model scenarios for each algorithm. The details of the scenarios can seen in Table 4.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 4. </span><span class="font3">Testing Scenario</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Testing Scenario</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Stopword Removal</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Smote</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Classification Algorithm</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Scenario 1</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">LG / RF</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Scenario 2</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">ѵ</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">LG / RF</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Scenario 3</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">ѵ</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">LG / RF</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Scenario 4</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">ѵ</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">ѵ</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">LG / RF</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark24"></a><span class="font3" style="font-weight:bold;"><a name="bookmark25"></a>2.9. &nbsp;&nbsp;&nbsp;Evaluation</span></h3></li></ul>
<p><span class="font3">In this research, the evaluation is conducted on the training data using 10-fold cross-validation (CV) and on the testing data using Confusion Matrix. 10-Fold Cross Validation divides the dataset into ten parts (folds), where one part (fold) later becomes training data (validation fold), and the remaining nine parts (fold) become test data (train fold). Measurements are repeated iteratively until each part (fold) out of the ten parts (folds) has been used as the training data (validation fold). Then, the average accuracy value of the 10-fold cross-validation (CV) conducted on the training dataset is a benchmark for the validation results. Confusion Matrix is used to evaluate the performance of a classification model. Within the confusion matrix, there is information related to the actual classification and predictions made by the classification model, allowing for the</span></p>
<p><span class="font3">calculation of accuracy, precision, and recall values as benchmarks for the performance produced by the classification model [18].</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark26"></a><span class="font3" style="font-weight:bold;"><a name="bookmark27"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h3></li></ul>
<p><span class="font3">This section contains the results and discussion of the conducted research. Details and specific results using methods and algorithms can be presented as descriptions, charts, or figures.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark28"></a><span class="font3" style="font-weight:bold;"><a name="bookmark29"></a>3.1. &nbsp;&nbsp;&nbsp;Tuning Hyperparameter</span></h3></li></ul>
<p><span class="font3">The results of Tuning Hyperparameter using Grid Search on the Logistic Regression and Random Forest algorithms can observed in Table 5. It includes detailed values for the parameter C in the Logistic Regression algorithm and the parameters Estimators, Max-depth, Max-features, and Criterion in the Random Forest algorithm.</span></p>
<table border="1">
<tr><td colspan="4" style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">Table 5. </span><span class="font3">Tuning Hyperparameter</span></p></td></tr>
<tr><td colspan="4" style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Logistic Regression</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Scenario</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Hyperparameter</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">C</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td colspan="2" style="vertical-align:top;">
<ul style="list-style:none;"><li>
<p><span class="font2">1. &nbsp;&nbsp;&nbsp;Default</span></p></li>
<li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Stopword Removal</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Smote</span></p></li>
<li>
<p><span class="font2">4. &nbsp;&nbsp;&nbsp;Stopword Removal + Smote</span></p></li></ul></td><td style="vertical-align:top;">
<p><span class="font2">1.9100000000000008</span></p>
<p><span class="font2">1.4100000000000004</span></p>
<p><span class="font2">2.4</span></p>
<p><span class="font2">2.7400000000000015</span></p></td></tr>
<tr><td colspan="4" style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Random Forest</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Scenario</span></p></td><td style="vertical-align:top;"></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Hyperparameter</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Estimators</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Max-depth</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Max-features &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Criterion</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p>
<p><span class="font2">2</span></p>
<p><span class="font2">3</span></p>
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">500</span></p>
<p><span class="font2">400</span></p>
<p><span class="font2">500</span></p>
<p><span class="font2">450</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">193</span></p>
<p><span class="font2">211</span></p>
<p><span class="font2">197</span></p>
<p><span class="font2">155</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Log2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Entropy</span></p>
<p><span class="font2">Log2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Entropy</span></p>
<p><span class="font2">Log2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Entropy</span></p>
<p><span class="font2">Log2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gini</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark30"></a><span class="font3" style="font-weight:bold;"><a name="bookmark31"></a>3.2. &nbsp;&nbsp;&nbsp;Evaluation</span></h3></li></ul>
<p><span class="font3">The evaluation in this research refers to the results of 10-fold cross-validation and the confusion matrix. It is based on the achievement of the highest f1-score value, the comparison of each scenario to the default scenario, and the influence of applying each method in the scenario for each algorithm. Then, the results from both algorithms are compared to determine the best performance achieved between them.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark32"></a><span class="font3" style="font-weight:bold;"><a name="bookmark33"></a>3.2.1. &nbsp;&nbsp;&nbsp;Logistic Regression</span></h3></li></ul>
<p><span class="font3">The evaluation results of Logistic Regression are shown in Table 6; it is observed that overall, each scenario experiences an increase in the f1-score evaluation value from testing data to training data, except for the fourth scenario. Therefore, the fourth scenario is experiencing overfitting. The scenario that achieved the highest f1-score value in Logistic Regression is the one with SMOTE implementation (Scenario 3), which is 72.70%. Meanwhile, the scenario with the lowest f1-score is the one with Stopword Removal implementation (Scenario 2), which is 69.23% for the f1-score. Based on the comparison of each scenario to the default scenario, it is found that the scenario that experienced the highest increase in f1-score value is the one with SMOTE implementation (Scenario 3), which is +0.80%. Meanwhile, the scenario that experienced the highest decrease in the f1-score value is the one with Stopword Removal implementation, which is -2.67% (Scenario 2). Implementing Stopword Removal on the Logistic Regression algorithm decreases the resulting f1-score. The highest decrease in f1-score based on the application of Stopword Removal is with the combination of Stopword Removal and SMOTE (Scenario 4), which is -3.21%, while the lowest decrease in f1-score is with the Stopword Removal combination (Scenario 2), which is -2.67%. The implementation of SMOTE on the Logistic Regression algorithm increases the resulting f1-score. The highest increase in f1-score based on the application of SMOTE is with the SMOTE combination (Scenario 3), which is +0.80%, while the lowest increase in f1-score is with the combination of SMOTE and Stopword Removal (Scenario 4), which is +0.26%. The best scenario obtained for the Logistic Regression algorithm is with the implementation of SMOTE (Scenario 3), which results in a f1-score of 72.70% and a</span></p>
<p><span class="font3">+0.80% increase in the f1-score. On the other hand, the worst scenario is with the implementation of Stopword Removal (Scenario 2), which yields an f1-score of 69.23% and a -2.67% decrease in the f1-score.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 6. </span><span class="font3">Evaluation of Logistic Regression</span></p>
<table border="1">
<tr><td colspan="5" style="vertical-align:bottom;">
<p><span class="font2">Training and Testing Data Evaluation Result</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Testing Metric</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Scenario</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font2">Data</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Δ Gap</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Train</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Test</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.46%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">71.90%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.44</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">67.47%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.13%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.66</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Accuracy</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.65%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.67%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.02</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70.84%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.45%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-1.39</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.75%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.27%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.52</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">67.80%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.37%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.57</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Precision</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.85%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.86%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.01</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">71.05%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.70%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-1.35</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.23%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">71.69%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.46</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">67.28%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.16%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.84</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Recall</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.65%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.67%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.02</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70.84%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.44%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-1.40</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.39%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">71.90%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.51</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">67.41%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.23%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.82</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">F1-score</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.69%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.70%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.01</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">70.85%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">69.49%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-1.36</span></p></td></tr>
</table>
<table border="1">
<tr><td colspan="3" style="vertical-align:bottom;">
<p><span class="font2">Comparison of Each Scenario to the Default Scenario</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Testing Metric</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Δ Scenario-Default</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Accuracy</span></p></td><td style="vertical-align:bottom;">
<ul style="list-style:none;"><li>
<p><span class="font2">1. &nbsp;&nbsp;&nbsp;Default</span></p></li>
<li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Stopword Removal</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Smote</span></p></li>
<li>
<p><span class="font2">4. &nbsp;&nbsp;&nbsp;Stopword Removal + Smote</span></p></li></ul></td><td style="vertical-align:bottom;">
<p><span class="font2">-</span></p>
<p><span class="font2">-2.77 % 0.77 % -2.45 %</span></p></td></tr>
</table>
<p><span class="font2">Comparison of Each Scenario to the Default Scenario</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Testing Metric</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Δ Scenario-Default</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">1. Default</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Precision</span></p></td><td style="vertical-align:top;">
<p><span class="font2">2. Stopword Removal</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-2.90 %</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">3. Smote</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0.59 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4. Stopword Removal + Smote</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.57 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">1. Default</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font2">2. Stopword Removal</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-2.53 %</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">3. Smote</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0.98 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4. Stopword Removal + Smote</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.25 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">1. Default</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">F1-score</span></p></td><td style="vertical-align:bottom;">
<ul style="list-style:none;"><li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Stopword Removal</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Smote</span></p></li></ul></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.67 %</span></p>
<p><span class="font2">+0.80 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4. Stopword Removal + Smote</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.41 %</span></p></td></tr>
</table>
<table border="1">
<tr><td colspan="4" style="vertical-align:bottom;">
<p><span class="font2">The Influence of Applying Each Method in the Scenario</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Testing Metric</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Scenario</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Stopword Removal</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Smote</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font2">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.77 %</span></p>
<p><span class="font2">Scenario(2-1)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Δ Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.77%</span></p>
<p><span class="font2">Scenario(3-1)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-3.22 %</span></p></td><td style="vertical-align:top;">
<p><span class="font2">+0.32%</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-3)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-2)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.90 %</span></p>
<p><span class="font2">Scenario(2-1)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Δ Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">3</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.59%</span></p>
<p><span class="font2">Scenario(3-1)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-3.16 %</span></p></td><td style="vertical-align:top;">
<p><span class="font2">+0.33%</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-3)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-2)</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:bottom;">
<p><span class="font2">Δ Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font2">2</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-2.53 %</span></p>
<p><span class="font2">Scenario(2-1)</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">-</span></p>
<p><span class="font2">+0.98%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(3-1)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-3.23 %</span></p></td><td style="vertical-align:top;">
<p><span class="font2">+0.28%</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-3)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-2)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.67 %</span></p>
<p><span class="font2">Scenario(2-1)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Δ F1-score</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.80%</span></p>
<p><span class="font2">Scenario(3-1)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-3.21 %</span></p></td><td style="vertical-align:top;">
<p><span class="font2">+0.26%</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-3)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Scenario(4-2)</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark34"></a><span class="font4" style="font-weight:bold;"><a name="bookmark35"></a>3.2.2. &nbsp;&nbsp;&nbsp;Random Forest</span></h2></li></ul>
<p><span class="font3">The evaluation results of Random Forest are shown in Table 7; it is observed that all scenarios experience an increase in the f1-score evaluation value from testing data to training data, indicating that there are no scenarios experiencing overfitting. The scenario that achieved the highest f1-score value in Random Forest is the one with SMOTE implementation (Scenario 3), which is 75.03%. Meanwhile, the scenario with the lowest f1-score is the one with Stopword Removal implementation (Scenario 2), which is 68.73%. Based on comparing each scenario to the default scenario in the Random Forest algorithm, the scenario that experienced the highest increase in f1-score value is the one with SMOTE implementation (Scenario 3), which is +6.11%. Meanwhile, the highest decrease in the f1-score value is the one with Stopword Removal implementation (Scenario 2), which is -0.19%. Implementing Stopword Removal on the Random Forest algorithm decreases the resulting f1-score. The highest decrease in f1-score based on the application of Stopword Removal is with the combination of Stopword Removal and SMOTE (Scenario 4), which is -2.03%, while the lowest decrease in f1-score is with the Stopword Removal combination (Scenario 2), which is -0.19%. The implementation of SMOTE on the Random Forest algorithm increases the resulting f1-score. The highest increase in f1-score based on the application of SMOTE is with the SMOTE combination (Scenario 3), which is +6.11%, while the lowest increase in f1-score is with the combination of SMOTE and Stopword Removal (Scenario 4), which is +4.27%. The best scenario obtained for the Random Forest algorithm is with the implementation of SMOTE (Scenario 3), which results in a f1-score of 75.03% and a +6.11% increase in the f1-score. On the other hand, the worst scenario is with the implementation of Stopword Removal (Scenario 2), which yields a f1-score of 68.73% and a -0.19% decrease in the f1-score.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 7. </span><span class="font3">Evaluation of Random Forest</span></p>
<p><span class="font2">Training and Testing Data Evaluation Result</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Testing Metric</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Scenario</span></p></td><td colspan="2" style="vertical-align:top;">
<p><span class="font2">Data</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Δ Gap</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Train</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Test</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.24%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.41%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.17</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">66.23%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.85%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.62</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Accuracy</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">74.85%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">74.98%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.13</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.61%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">73.00%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.39</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.69%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70.78%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.09</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">67.64%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70.19%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.55</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Precision</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">75.08%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">75.23%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.15</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">73.29%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">73.62%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.33</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">67.40%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.54%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.14</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">65.61%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.31%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.70</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Recall</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">74.85%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">74.98%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.13</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.61%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.99%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.38</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">67.79%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.92%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+1.13</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">66.00%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.73%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+2.73</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">F1-score</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">74.89%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">75.03%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">+0.14</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">72.61%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">73.00%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">+0.39</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Testing Metric</span></p></td><td colspan="4" style="vertical-align:bottom;">
<p><span class="font2">Comparison of Each Scenario to the Default Scenario</span></p>
<p><span class="font2">Scenario &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Δ Scenario-Default</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Accuracy</span></p>
<p><span class="font2">Precision</span></p>
<p><span class="font2">Recall</span></p>
<p><span class="font2">F1-score</span></p></td><td style="vertical-align:bottom;">
<ul style="list-style:none;"><li>
<p><span class="font2">1. &nbsp;&nbsp;&nbsp;Default</span></p></li>
<li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Stopword</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Smote</span></p></li>
<li>
<p><span class="font2">4. &nbsp;&nbsp;&nbsp;Stopword</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font2">1. &nbsp;&nbsp;&nbsp;Default</span></p></li>
<li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Stopword</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Smote</span></p></li>
<li>
<p><span class="font2">4. &nbsp;&nbsp;&nbsp;Stopword</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font2">1. &nbsp;&nbsp;&nbsp;Default</span></p></li>
<li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Stopword</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Smote</span></p></li>
<li>
<p><span class="font2">4. &nbsp;&nbsp;&nbsp;Stopword</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font2">1. &nbsp;&nbsp;&nbsp;Default</span></p></li>
<li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Stopword</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Smote</span></p></li>
<li>
<p><span class="font2">4. &nbsp;&nbsp;&nbsp;Stopword</span></p></li></ul></td><td style="vertical-align:bottom;">
<p><span class="font2">Removal</span></p>
<p><span class="font2">Removal + Smote</span></p>
<p><span class="font2">Removal</span></p>
<p><span class="font2">Removal + Smote</span></p>
<p><span class="font2">Removal</span></p>
<p><span class="font2">Removal + Smote</span></p>
<p><span class="font2">Removal</span></p>
<p><span class="font2">Removal + Smote</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font2">-</span></p>
<p><span class="font2">-0.56 % 5.57 % 3.59 %</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-0.59 % 4.45 % 2.84 %</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-0.23 % 6.44 % 4.45 %</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-0.19 % +6.11 % +4.08 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td colspan="3" style="vertical-align:bottom;">
<p><span class="font2">The Influence of Applying Each Method in the Scenario</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Testing Metric</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font2">Scenario</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Stopword Removal</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Smote</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ Accuracy</span></p>
<p><span class="font2">Δ Precision</span></p>
<p><span class="font2">Δ Recall</span></p>
<p><span class="font2">Δ F1-score</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">1</span></p>
<p><span class="font2">2</span></p>
<p><span class="font2">3</span></p>
<p><span class="font2">4</span></p>
<p><span class="font2">1</span></p>
<p><span class="font2">2</span></p>
<p><span class="font2">3</span></p>
<p><span class="font2">4</span></p>
<p><span class="font2">1</span></p>
<p><span class="font2">2</span></p>
<p><span class="font2">3</span></p>
<p><span class="font2">4</span></p>
<p><span class="font2">1</span></p>
<p><span class="font2">2</span></p>
<p><span class="font2">3</span></p>
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-</span></p>
<p><span class="font2">-0.56 % Scenario(2-1)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-1.98 % Scenario(4-3)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-0.59 % Scenario(2-1)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-1.61 % Scenario(4-3)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-0.23% Scenario(2-1)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-1.99 % Scenario(4-3)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-0.19 % Scenario(2-1)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-2.03 % Scenario(4-3)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">+5.57 % Scenario(3-1) +4.15 %</span></p>
<p><span class="font2">Scenario(4-2)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">+4.45 % Scenario(3-1) +3.43 % Scenario(4-2)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">+6.44 % Scenario(3-1) +4.68 % Scenario(4-2)</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">-</span></p>
<p><span class="font2">+6.11 % Scenario(3-1) +4.27 % Scenario(4-2)</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark36"></a><span class="font3" style="font-weight:bold;"><a name="bookmark37"></a>3.3. &nbsp;&nbsp;&nbsp;Comparison of Logistic Regression and Random Forest Evaluations</span></h3></li></ul>
<p><span class="font3">The comparison of evaluation results between algorithms has been shown in Table 8. The highest f1-score value produced by both algorithms is in the Random Forest algorithm with the implementation of SMOTE (Scenario 3), which is 75.03%. Meanwhile, the lowest f1-score value produced is in the Random Forest algorithm with the implementation of Stopword Removal (Scenario 2), which is 68.73%. In models where SMOTE is not applied (Scenarios 1 and 2), the Logistic Regression algorithm produces better f1-scores. The highest f1-score value obtained is</span></p>
<p><span class="font3">71.90% in the first scenario, which involves the implementation of a Hyperparameter Tuning (Default). On the other hand, in models where SMOTE is applied (Scenarios 3 and 4), the Random Forest algorithm yields better f1-scores. The highest f1-score value achieved is 75.03% in the third scenario, which involves the implementation of SMOTE. Comparing each scenario to the default scenario for both algorithms shows that the highest increase in f1-score value is achieved in the Random Forest algorithm (scenario 3) with the implementation of SMOTE, which is +6.11%. The highest decrease in the f1-score value is observed in the Logistic Regression algorithm (scenario 2) with the implementation of Stopword Removal, which is -2.67%. Implementing Stopword Removal to both algorithms results in a decrease in the f1-score. The model that experiences the highest decline in f1-score based on the implementation of Stopword Removal in this research is the Logistic Regression algorithm with the combination of Stopword Removal and SMOTE (Scenario 4), which is -3.21%. Meanwhile, the lowest decrease is observed in the Random Forest algorithm with the implementation of Stopword Removal (Scenario 2), which is -0.19%. Additionally, applying Stopword Removal to both algorithms is better done as it leads to the lowest decrease in f1-score. The implementation of SMOTE in both algorithms can increase the f1-score. The model that experiences the highest increase in f1-score based on the implementation of SMOTE in this research is the Random Forest algorithm with the implementation of SMOTE (Scenario 3), which is +6.11%. Meanwhile, the lowest increase is observed in the Logistic Regression algorithm with the combination of Stopword Removal and SMOTE (Scenario 4), which is +0.26%. The best sentiment analysis classification model for Indonesian tweets obtained in this research uses the Random Forest algorithm with the implementation of SMOTE (Scenario 3), which achieves a f1-score of 75.03% and a +6.11% increase in the f1-score. On the other hand, the worst is obtained using the Random Forest algorithm with the implementation of Stopword Removal (Scenario 2), which results in a f1-score of 68.73% and a -0.19% increase in the f1-score. The combined implementation of Stopword Removal and SMOTE (Scenario 4) in both algorithms does not result in the best f1-score and the highest increase in f1-score. This is due to the application of Stopword Removal, which removes frequently occurring but deemed insignificant words that significantly impact the meaning of a sentence based on the utilized stoplist, such as conjunctions and pronouns. In this research, the stoplist for Stopword Removal is generated using the NLTK library, which is more suitable for document classification rather than sentiment analysis. Consequently, applying Stopword Removal using NLTK's stoplist in sentiment classification can diminish the information in a sentence, causing it to lose its true meaning (sentiment), leading to suboptimal performance in classification, i.e., a decrease in the f1-score.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 8. </span><span class="font3">Evaluation of Logistic Regression vs Random Forest</span></p>
<p><span class="font2">Comparison of Logistic Regression and Random Forest Evaluation</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Testing Metric</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Scenario</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font2">Algoritmn</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Δ F1-Score (LR-RF)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Logistic Regression</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Random Forest</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">71.90%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.41%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2.49 %</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Accuracy</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.13%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.85%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0.28 %</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.67%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">74.98%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.31 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.45%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">73.00%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-3.55 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.27%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70.78%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">1.49 %</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Precision</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.37%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70.19%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-0.82 %</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.86%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">75.23%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.37 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.70%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">73.62%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-3.92 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">71.69%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.54%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">3.15 %</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Recall</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.16%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.31%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0.85 %</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.67%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">74.98%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.31 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.44%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.99%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-3.55 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">71.90%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.92%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2.98 %</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">F1-score</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">69.23%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">68.73%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0.50 %</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72.70%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">75.03%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">-2.33 %</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">69.49%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">73.00%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">-3.51 %</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Testing Metric</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Comparison of Scenario to Default Scenario in Both Algorithms</span></p>
<p><span class="font3"><sup>Algoritmn</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font2">Δ F1-Score</span></p>
<p><span class="font3"><sup>Scenario &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logistic</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font2">Random Forest &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LR-RF)</span></p>
<p><span class="font2">Regression</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.77 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.56 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.21 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.77 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.57 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-4.80 %</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.45 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.59 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-6.04 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.90 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.59 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.31 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.59 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.45 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.86 %</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.57 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.84 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-5.41 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font2">--</span></p>
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.53 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.23 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.30 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.98 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.44 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-5.46 %</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.25 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.45 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-6.70 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">F1-score</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.67 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.19 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.48 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.80 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+6.11 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-5.31%</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.41 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+4.08 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-6.49%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Testing Metric</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Comparison of Stopword Removal in Both Algorithms</span></p>
<p><span class="font2">Algoritmn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3"><sub>Δ F1-Score</sub></span></p>
<p><span class="font2">Scenario &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logistic</span></p>
<p><span class="font2">Random Forest &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LR-RF)</span></p>
<p><span class="font2">Regression</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.77 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.56 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.21 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.22 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1.98 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1.24 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.90 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.59 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.31 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.16 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1.61 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1.55 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.23 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.23% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.00 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.67 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1.99 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.68 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ F1-score</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.67 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.19% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.48 %</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.21 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-2.03 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-1.18 %</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Testing Metric</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Comparison of SMOTE in Both Algorithms</span></p>
<p><span class="font2">Algoritmn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3"><sub>Δ F1-Score</sub></span></p>
<p><span class="font2">Scenario &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Logistic</span></p>
<p><span class="font2">Random Forest &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(LR-RF)</span></p>
<p><span class="font2">Regression</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.77% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+5.57 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-4.80 %</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.32% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+4.15 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.83 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.59% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+4.45 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.89 %</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.33% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+3.43 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-3.10 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.98% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+6.44 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-5.46 %</span></p>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.28% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+4.68 % &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-4.40 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Δ F1-score</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-</span></p>
<p><span class="font2">3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.80% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+6.11% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-5.31 %</span></p></td></tr>
</table>
<p><span class="font2">4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+0.26% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+4.27% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-4.01 %</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark38"></a><span class="font3" style="font-weight:bold;"><a name="bookmark39"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h3></li></ul>
<p><span class="font3">Based on the research, the conclusion is that the best sentiment analysis classification model for Indonesian tweets in this research is achieved using the Random Forest algorithm with SMOTE applied. It resulted in an f1-score of 75.03%, showing an improvement of +6.11% in the f1-score value. Meanwhile, the worst is achieved using the Random Forest algorithm with Stopword Removal applied. It resulted in an f1-score of 68.73%, showing a decrease of -0.19% in the f1-score value. Secondly, the implementation of Stopword Removal on Logistic Regression and Random Forest algorithms can lead to a reduction in the f1-score values. This is because Stopword Removal can potentially reduce information and alter the meaning of the processed tweets, causing them to lose their sentiment. Furthermore, implementing the NLTK stoplist used for Stopword Removal in this research is more optimally effective for document classification than sentiment classification, so implementing a more suitable stoplist for sentiment classification can be an option. The highest decrease in the f1-score is observed in the Logistic Regression algorithm by applying Stopword Removal and SMOTE. In contrast, the lowest reduction in the f1-score is kept in the Random Forest algorithm with Stopword Removal. Using Stopword Removal for both algorithms is preferable, resulting in the lowest decrease in f1-score. Thirdly, implementing SMOTE on Logistic Regression and Random Forest algorithms generally increases the f1-score values. The dataset used in each scenario that applies SMOTE to both algorithms has a balanced class distribution, preventing tendencies or biases in sentiment classification towards the majority class. The Random Forest algorithm obtained the highest increase in the f1-score with the implementation of SMOTE, amounting to +6.11%. Meanwhile, the lowest increase in f1-score is observed in the Logistic Regression algorithm with the combined implementation of SMOTE and Stopword Removal, amounting to +0.26%.</span></p>
<h3><a name="bookmark40"></a><span class="font3" style="font-weight:bold;"><a name="bookmark41"></a>References</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;A. Santosa, I. Purnamasari, and R. Mayasari, “Pengaruh stopword removal dan stemming terhadap performa klasifikasi teks komentar kebijakan new normal menggunakan algoritma,” </span><span class="font3" style="font-style:italic;">Jurnal Sains Komputer &amp;&nbsp;Informatika</span><span class="font3">, vol. 6, no. 1, pp. 81–93, 2022, doi: 10.30645/j-sakti.v6i1.427.</span></p></li>
<li>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;I. Firmansyah, J. T. Samudra, D. Pardede, and Z. Situmorang, “Komparasi random forest dan logistic regression dalam klasifikasi penderita covid-19 berdasarkan gejalanya,” </span><span class="font3" style="font-style:italic;">Journal of Science and Social Research</span><span class="font3">, vol. 5, no. 3, p. 595, 2022, doi: 10.54314/jssr.v5i3.994.</span></p></li>
<li>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;A. B. P. Negara, H. Muhardi, and F. Sajid, “Perbandingan Algoritma Klasifikasi terhadap Emosi Tweet Berbahasa Indonesia,” </span><span class="font3" style="font-style:italic;">Jurnal Edukasi dan Penelitian Informatika</span><span class="font3">, vol. 7, no. 2, p. 242, 2021, doi: 10.26418/jp.v7i2.48198.</span></p></li>
<li>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;M. Noveanto, H. Sastypratiwi, H. Muhardi, and J. H. Hadari Nawawi Pontianak, “Uji akurasi klasifikasi emosi pada lirik lagu bahasa indonesia,” </span><span class="font3" style="font-style:italic;">Jurnal Sistem dan Teknologi Informasi</span><span class="font3">, vol. 10, no. 3, pp. 311–318, 2022, doi: 10.26418/justin.v10i3.56804.</span></p></li>
<li>
<p><span class="font3">[5] &nbsp;&nbsp;&nbsp;C. Cahyaningtyas, Y. Nataliani, and I. R. Widiasari, “Analisis sentimen pada rating aplikasi shopee menggunakan metode decision tree berbasis smote,” </span><span class="font3" style="font-style:italic;">AITI: Jurnal Teknologi Informasi</span><span class="font3">, vol. 18, no. 2, pp. 173–184, 2021, doi: 10.24246/aiti.v18i2.173-184.</span></p></li>
<li>
<p><span class="font3">[6] &nbsp;&nbsp;&nbsp;A. Andreyestha and Q. N. Azizah, “Analisa sentimen kicauan twitter tokopedia dengan optimalisasi data tidak seimbang menggunakan algoritma smote,” </span><span class="font3" style="font-style:italic;">Infotek: Jurnal Informatika dan Teknologi</span><span class="font3">, vol. 5, no. 1, pp. 108–116, 2022, doi: 10.29408/jit.v5i1.4581.</span></p></li>
<li>
<p><span class="font3">[7] &nbsp;&nbsp;&nbsp;R. Ferdiana, F. Jatmiko, D. D. Purwanti, A. S. T. Ayu, and W. F. Dicka, “Dataset indonesia untuk analisis sentimen,” </span><span class="font3" style="font-style:italic;">Jurnal Nasional Teknik Elektro dan Teknologi Informasi</span><span class="font3">, vol. 8, no. 4, p. 334, 2019, doi: 10.22146/jnteti.v8i4.533.</span></p></li>
<li>
<p><span class="font3">[8] &nbsp;&nbsp;&nbsp;Y. Sari, “Pengenalan Natural Language Toolkit (NLTK),” Yogyakarta, 2019.</span></p></li>
<li>
<p><span class="font3">[9] &nbsp;&nbsp;&nbsp;I. F. Rozi, R. Ardiansyah, and N. Rebeka, “Penerapan Normalisasi Kata Tidak Baku Menggunakan Levenshtein Distance pada Analisa Sentimen Layanan PT . KAI di Twitter,” </span><span class="font3" style="font-style:italic;">Seminar Informatika Aplikatif</span><span class="font3">, pp. 106–112, 2019.</span></p></li>
<li>
<p><span class="font3">[10] &nbsp;&nbsp;&nbsp;M. S. Anwar, I. M. I. Subroto, and S. Mulyono, “Sistem pencarian e-journal menggunakan metode stopword removal dan stemming,” </span><span class="font3" style="font-style:italic;">Prosiding KONFERENSI ILMIAH MAHASISWA UNISSULA &nbsp;&nbsp;&nbsp;&nbsp;(KIMU) &nbsp;&nbsp;&nbsp;&nbsp;2</span><span class="font3">, &nbsp;&nbsp;&nbsp;&nbsp;pp. &nbsp;&nbsp;&nbsp;&nbsp;58–70, &nbsp;&nbsp;&nbsp;&nbsp;2019, &nbsp;&nbsp;&nbsp;&nbsp;[Online]. &nbsp;&nbsp;&nbsp;&nbsp;Available:</span></p></li></ul>
<p><a href="https://jurnal.unissula.ac.id/index.php/kimueng/article/view/8420"><span class="font3">https://jurnal.unissula.ac.id/index.php/kimueng/article/view/8420</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[11] &nbsp;&nbsp;&nbsp;M. Darwis, G. T. Pranoto, Y. E. Wicaksana, and Y. Yaddarabullah, “Implementation of TF-IDF Algorithm and K-mean Clustering Method to Predict Words or Topics on Twitter,” </span><span class="font3" style="font-style:italic;">Jurnal Informatika dan Sains</span><span class="font3">, vol. 3, no. 2, pp. 49–55, 2020, doi: 10.31326/jisa.v3i2.831.</span></p></li>
<li>
<p><span class="font3">[12] &nbsp;&nbsp;&nbsp;S. Khairunnisa, A. Adiwijaya, and S. Al Faraby, “Pengaruh text preprocessing terhadap analisis sentimen komentar masyarakat pada media sosial twitter,” </span><span class="font3" style="font-style:italic;">Jurnal Media Informatika Budidarma</span><span class="font3">, vol. 5, no. 2, pp. 406–414, 2021, doi: 10.30865/mib.v5i2.2835.</span></p></li>
<li>
<p><span class="font3">[13] &nbsp;&nbsp;&nbsp;E. M. O. N. Haryanto, A. K. A. Estetikha, and R. A. Setiawan, “Implementasi smote untuk mengatasi imbalanced data pada sentimen analisis sentimen hotel di nusa tenggara barat dengan menggunakan algoritma svm,” </span><span class="font3" style="font-style:italic;">Jurnal Informasi Interaktif</span><span class="font3">, vol. 7, no. 1, p. 16, 2022.</span></p></li>
<li>
<p><span class="font3">[14] &nbsp;&nbsp;&nbsp;M. Azhar and H. F. Pardede, “Klasifikasi Dialek Pengujar Bahasa Inggris Menggunakan Random Forest,” </span><span class="font3" style="font-style:italic;">Jurnal Media Informatika Budidarma</span><span class="font3">, vol. 5, no. 2, pp. 439–446, 2021, doi: 10.30865/mib.v5i2.2754.</span></p></li>
<li>
<p><span class="font3">[15] &nbsp;&nbsp;&nbsp;Ramli, D. Yuniarti, and R. Goejantoro, “Perbandingan metode klasifikasi regresi logistik dengan jaringan saraf tiruan,” </span><span class="font3" style="font-style:italic;">Jurnal Eksponensial</span><span class="font3">, vol. 4, no. 1, pp. 17–24, 2013.</span></p></li>
<li>
<p><span class="font3">[16] &nbsp;&nbsp;&nbsp;A. Ferdita Nugraha, R. F. A. Aziza, and Y. Pristyanto, “Penerapan metode stacking dan random forest untuk meningkatkan kinerja klasifikasi pada proses deteksi web phishing,” </span><span class="font3" style="font-style:italic;">Jurnal Infomedia: Teknik Informatika, Multimedia &amp;&nbsp;Jaringan</span><span class="font3">, vol. 7, no. 1, pp. 39–44, 2022, doi: 10.30811/jim.v7i1.2959.</span></p></li>
<li>
<p><span class="font3">[17] &nbsp;&nbsp;&nbsp;H. Nalatissifa, W. Gata, S. Diantika, and K. Nisa, “Perbandingan kinerja algoritma klasifikasi naive bayes, support vector machine (svm), dan random forest untuk prediksi ketidakhadiran di tempat kerja,” </span><span class="font3" style="font-style:italic;">Jurnal Informatika Universitas Pamulang</span><span class="font3">, vol. 5, no. 4, pp. 578–584, 2021, doi: 10.32493/informatika.v5i4.7575.</span></p></li>
<li>
<p><span class="font3">[18] &nbsp;&nbsp;&nbsp;S. Khomsah and Agus Sasmito Aribowo, “Model text-preprocessing komentar youtube dalam bahasa indonesia,” </span><span class="font3" style="font-style:italic;">Rekayasa Sistem dan Teknologi Informasi</span><span class="font3">, vol. 4, no. 10, pp. 648– 654, 2020, doi: 10.29207/resti.v4i4.2035.</span></p></li></ul>
<p><span class="font3">185</span></p>