---
layout: full_article
title: "Spoof Detection Using Local Binary Pattern In Face"
author: "Vani Dave"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-58992 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-58992"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-58992"  
comments: true
---

<p><span class="font9">Jurnal Ilmu Komputer VOL. XIII No. 1</span></p>
<p><span class="font9">p-ISSN: 1979-5661</span></p>
<p><span class="font9">e-ISSN: 2622-321X</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font10"><a name="bookmark1"></a>Spoof Detection Using Local Binary Pattern In Face</span></h1>
<p><span class="font9">Anshika Shukla<sup>a1</sup>,Vani Dave<sup>b2</sup>,Ayush Mishra<sup>a3</sup></span></p>
<p><span class="font9">Assistant Professor <sup>a</sup>Department of computer science and Engineering, Kanpur Institute of Technology, India</span></p>
<p><a href="mailto:1anshikashukla4@gmail.com"><span class="font9"><sup>1</sup>anshikashukla4@gmail.com</span></a></p>
<p><span class="font9">Assistant Professor <sup>b</sup>Master In Computer Application Kanpur institute of technology <sup>2</sup>vani_dave@rediffmail.com(corresponding author)</span></p>
<p><span class="font9">Research Scholar <sup>a</sup>Department of computer science and Engineering, Kanpur Institute of Technology,India </span><a href="mailto:3ayush8stp@gmail.com"><span class="font9"><sup>3</sup>ayush8stp@gmail.com</span></a></p>
<p><span class="font9">Abstract</span></p>
<p><span class="font9">Spoofing attack is an attempt to acquire some other’s identity or access right by using a biometric evidence of authorized user. Among all biometric systems facial identity is one of the widely used method that is prone to such spoofing attacks using a simple photograph of the user.</span></p>
<p><span class="font9">The paper focuses and takes the problem area of face spoofing attacks into account by detecting spoof faces and real faces. We are using the local binary pattern (LBP) for providing the solution of spoofing problem and with the help of these patterns we inspect primarily two types of attacks i.e. printed photograph and photos displayed using digital screen. For this, we will use the local database maintained by us having the images labeled as real and spoof for the data required.</span></p>
<p><span class="font9">We conclude that local binary pattern will reduce the total error rate and will show the moderate output when used across a wide set of attack types. This will enhance the efficiency of the system for detection of spoofing by using the deep learning techniques.</span></p>
<p><span class="font9">Keywords :Spoofing ,Local Binary Pattern, CASIA, NUAA, Hyperplane, Support Vector machine</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">1. &nbsp;&nbsp;&nbsp;Introduction</span></p></li></ul>
<p><span class="font9">Spoofing attack is a way of cyber attack in which a person tries to override the biometric authentication of a valid user by presenting a counterfeit biometric evidence .In this attack attacker does not need any knowledge about the algorithm used in the biometric system .The biometric based verification systems are mostly not resistant to spoofing attack due to the reason of their designing as they are designed only to recognize identities without checking their liveliness. Instead some authentication systems which are using biometric authentication are also not able to implement the anti spoofing scheme in a very sophisticated way.</span></p>
<p><span class="font9">Attacking on biometric system in different possible ways will require different level of difficulty for the attacker to create a spoof identity. In biometric systems like fingerprint recognition and iris recognition we require the artificial spoof evidence that can counterfeit the real identity and this requires a great expertise but the generation of fake evidence in face spoofing attack is easy and can be done by using a simple photograph of valid user . The biometric evidences can be easily by passed by either using these images or using a pre- recorded video.</span></p>
<p><span class="font9">As this kind of attack came into knowledge of the biometric community, various geeks provided their pay to check the liveliness of the person by adding various sensors to the biometric system. These systems detect the liveliness of person by asking user to perform some tasks or make a particular kind of gesture. But all</span></p>
<p><span class="font9">these sensors are external hardware that are required to perform this detection hence making a completely automated detection system is cheaper way as compared to these systems.</span></p>
<p><span class="font9">This approach we have opted in this is to find the Local Binary Patterns ( LBP) in the given image data and extract the feature from that LBP image by creating the histogram from the LBP data . These histogram will be used to perform the training of the model to predict the real spoof facial evidence.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">2. &nbsp;&nbsp;&nbsp;Existing anti spoofing methods and techniques</span></p></li></ul>
<p><span class="font9">For the implementation of anti spoofing there are various techniques and methods used. These methods follow the three basic ways to perform the spoofing detection: The first one is by assessing the texture of the subject image captured by the sensor of the system as it checks the complete texture of image to find the variation between real and spoof image. The second one is by detecting the liveliness of the environment during the capturing of image which checks the scene if it is live or pre- recorded video clip. The third one is a combination in which we use the texture based technique and the liveliness based detection together. Taking the first approach into account the spoof detection method using feature texture of the image was made when this was mentioned in a paper that the text of real image and spoof image varies on the basis of frequency distribution. As in capturing of any image the two main process comes in account are Illumination and reflectance so the frequency distribution of any image completely depends upon the reflectance and this is found that the reflectance of recaptured image shows various difference between the real once captured scene on the basis of their frequency distribution . So the previous work is done by using this frequency distribution and training the classifier by this frequency distribution the image. These classifiers further give their prediction for the data.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">3. &nbsp;&nbsp;&nbsp;Methodology and Experiments</span></p></li></ul>
<p><span class="font9">The paper here presents the anti-spoofing method using the described same concept of texture analysis of the live captured frames/images. In this method we will primarily use three concept that will be required for the whole method to be implemented. As in this method we are using the Local Binary Patterns (LBP) to train the algorithm I.e. SVM . We will brief the information about the LBP, SVM and the dataset we will be using for the implementation of this method.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">3.1 &nbsp;&nbsp;&nbsp;Local Binary Pattern (LBP)</span></p></li></ul>
<p><span class="font9">LBP is a pattern which is extracted from the image by processing its pixel in a specific logic format so that they change their value to binary values I.e. 0 &amp;&nbsp;1. LBP method provides the labeling of the pixels by finding the difference of neighborhood of each pixel and outputs that image area as the binary number. Because of its high discrimination power and an ease and simplicity in computation, This operator has got a better popularity in its approach to be used in various applications. It has become a very unique approach for textual analysis other than the traditionally used textual analysis approach. The one of the most important property or feature of LBP is that it shows its robust behaviour for the unicolor/bicolor in grey scale images.</span></p>
<p><span class="font9">For example, by the intensity difference in illumination .</span></p>
<p><span class="font9">The value of the LBP code of a pixel (xc,yc) is given by: </span><span class="font3">^ </span><span class="font3" style="text-decoration:underline;">1</span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font9">1, i </span><span class="font9" style="font-style:italic;">I ^</span><span class="font9"> ^&gt; </span><span class="font9" style="font-style:italic;">O</span><span class="font9">;</span></p>
<p><span class="font9">LBPKR= </span><span class="font9" style="font-style:italic;">∑^(,SP</span><span class="font9"> <sup>-</sup> </span><span class="font9" style="font-style:italic;">PO<sup>2</sup></span><span class="font9"> s(x)=[<sub>0</sub>, </span><span class="font9" style="font-style:italic;"><sub>βtherwise</sub>.</span></p>
<div><img src="https://jurnal.harianregional.com/media/58992-1.png" alt="" style="width:19pt;height:17pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/58992-2.jpg" alt="" style="width:73pt;height:73pt;">
<p><span class="font9">1.Sample</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/58992-3.jpg" alt="" style="width:19pt;height:22pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/58992-4.jpg" alt="" style="width:72pt;height:71pt;">
<p><span class="font9">2.Difference</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/58992-5.jpg" alt="" style="width:19pt;height:20pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/58992-6.jpg" alt="" style="width:73pt;height:72pt;">
</div><br clear="all">
<div>
<p><span class="font9">3.Threshold</span></p>
</div><br clear="all">
<h2><a name="bookmark2"></a><span class="font5"><a name="bookmark3"></a>1*1+1*2+1*4+1*8+0*16+0*32+0*64+0*128=15</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font9">4. &nbsp;&nbsp;&nbsp;Multiply by powers of two and sum</span></p></li></ul>
<p><span class="font9">The LBP algorithm we are applying here requires a total of four tunable parameters:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">1. &nbsp;&nbsp;&nbsp;Radius: It is used to handle and create the round/circular LBP and decides to represent the radius of that around the centered selected pixel. The default and most probable value is taken as 1.</span></p></li>
<li>
<p><span class="font9">2. &nbsp;&nbsp;&nbsp;Neighbors: The total number of points that are considered to build the rounded local binary pattern are termed as number of neighbors. The increase in neighbor count will increase the computational cost hence to reduce the cost we use less neighbor sample. The default value is taken as 8.</span></p></li>
<li>
<p><span class="font9">3. &nbsp;&nbsp;X-Set: The number of blocks in the horizontal side I.e. x-axis. The more cells we will use, the better</span></p></li></ul>
<p><span class="font9">and finer the grid will become and the dimensions of the resulting feature vector will inhance and be raised. The default value is taken as 8.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">4. &nbsp;&nbsp;&nbsp;Y-Set: The number of blocks in the vertical side I.e. y-axis. The more cells we will use, the better and finer the grid will become and the dimensions of the resulting feature vector will enhance and will be raised. The default value is taken as 8.</span></p></li></ul>
<p><span class="font9">The logic and mathematics to form the Lbp image from original uses the radius and pixel to be considered as input to then get the metrics of image according to the given inputs. This is then calculated as per the logic</span></p>
<p><span class="font9">LBP(p,r) = sigma-in range p=0 to p-1(gp-gc)</span></p>
<p><span class="font9">Taking the base threshold as 2^p</span></p>
<table border="1">
<tr><td colspan="3" style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">example</span></p></td><td colspan="5" style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">thresholded</span></p></td><td colspan="4" style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">weights</span></p></td><td colspan="3" style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">convolved</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">IO</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">25</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">S</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">4</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">12</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">15</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">17</span></p></td><td style="vertical-align:bottom;">
<p><span class="font9">—o-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7">τ⅛</span></p></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">12S</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">S</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">0</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font8" style="font-weight:bold;">8</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">9</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">2</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">15</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">°</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">1</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">64</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">32</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">16</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">O</span></p></td><td style="vertical-align:top;">
<p><span class="font8" style="font-weight:bold;">16</span></p></td></tr>
</table>
<p><span class="font8" style="font-weight:bold;">LBP = 2 + S + 16 = 26</span></p>
<p><span class="font8" style="font-weight:bold;">C = (25+17+15)∕3 - (lO+8+12+9+2)∕5 = -22</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">3.2 &nbsp;&nbsp;&nbsp;Extracting the histogram</span></p></li></ul>
<p><span class="font9">The histogram is prepared on the basis of frequency distribution of lbp image formed from the actual image. This is done after the image is converted to LBP format and hence this is done in mainly two ways: By taking the frequency of the pixel value and plotting on a histogram or by taking the probability of the frequency of pixel value. In this paper we are using the first way to form the histogram which will be required as input data to feed the algorithm. This histogram will be made by using the pixel data provided after the image is converted to lbp.</span></p><img src="https://jurnal.harianregional.com/media/58992-7.jpg" alt="" style="width:259pt;height:174pt;">
<p><span class="font9">Now this LBP histogram will be used as an input feed for the algorithm and this histogram shows variation in frequency distribution of real image and recaptured image on the basis of reflectance.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">3.3 &nbsp;&nbsp;&nbsp;Algorithm Used: Support Vector Machine (SVM)</span></p></li></ul>
<p><span class="font9">The support vector machine algorithm helps to find a specific plane in an N-dimensional space that classifies the different data points in a distinct way. The N-Dimensional space refers to space having N features. To separate the two classes we have various planes available in the same feature set . The main objective of this algorithm is to find a specific plane that could provide the maximum possible margin between these data points, i.e. the distance between data points of the two different classes must be maximum. Maximizing this margin distance between two different classes data points creates reinforcement so that the next points that are to be tested gets more accuracy as per the last updated details.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">3.4 &nbsp;&nbsp;&nbsp;Hyperplane</span></p></li></ul>
<p><span class="font9">Hyperplanes are the virtual boundaries made across the data points to classify their classes. These data points are provided to their specific classes as per their belonging to either side of the plane. As the planes are completely dependent upon the features provided hence the dimension is dependent upon the features . Lets assume that there are two features then we can consider a line as the hyperplane. In similar way if there are a total of three features then the hyperplane will be a plane. If we think of features more than the three then it will be a difficult task to decide the hyperplane of it.</span></p><img src="https://jurnal.harianregional.com/media/58992-8.jpg" alt="" style="width:277pt;height:107pt;">
<ul style="list-style:none;"><li>
<p><span class="font9">3.5 &nbsp;&nbsp;&nbsp;Support Vectors</span></p></li></ul>
<p><span class="font9">The support vectors are the data points that are much nearby to the actual hyperplane and they provide the support to the hyperplane and adjust the position of hyperplane as per the accuracy. These support vectors are used to maximize the margin between the data points that is used in the classifier. If we will remove the support vectors the location of hyperplane will be automatically adjusted and that too with a bad accuracy and less margin. These support vector and hyperplane points help us in building our SVM as only a single linear vector i.e hyperplane can’t maximize the margin and hence these support vectors provide a support to our linear decision boundary to maximize the margin.</span></p>
<div><img src="https://jurnal.harianregional.com/media/58992-9.jpg" alt="" style="width:205pt;height:103pt;">
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font9">4. Training and Accuracy</span></p></li></ul>
<p><span class="font9">The SVM is trained using the data that is provided as input feed in form of histogram. This data is input by the histogram associated with its label. The training and testing/validation will require a huge dataset as SVM requires a huge data for better accuracy. The data here will be feed in a specific format and after the training of algorithm the validation will be done using some other data to check its response to new or fresh data. This is done to check the EER% which can lead to a better and much accurate algorithm to be used.</span></p>
<p><span class="font9" style="font-weight:bold;">LBPJg<sub>3</sub> + &lt;&nbsp;LBPg<sub>3</sub> + LDA LBPg<sub>3</sub> + SVM LBP </span><span class="font6" style="font-weight:bold;">∣</span><span class="font9" style="font-weight:bold;">7</span><span class="font6" style="font-weight:bold;">∣</span><span class="font9" style="font-weight:bold;"> + SVM</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font0">dev</span></p></td><td style="vertical-align:top;">
<p><span class="font0">test</span></p></td><td style="vertical-align:top;">
<p><span class="font0">dev</span></p></td><td style="vertical-align:top;">
<p><span class="font0">test</span></p></td><td style="vertical-align:top;">
<p><span class="font0">dev</span></p></td><td style="vertical-align:top;">
<p><span class="font0">test</span></p></td><td style="vertical-align:top;">
<p><span class="font0">dev</span></p></td><td style="vertical-align:top;">
<p><span class="font0">test</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">31.24</span></p></td><td style="vertical-align:top;">
<p><span class="font0">34.01</span></p></td><td style="vertical-align:top;">
<p><span class="font0">19.60</span></p></td><td style="vertical-align:top;">
<p><span class="font0">17.17</span></p></td><td style="vertical-align:top;">
<p><span class="font0">14.84</span></p></td><td style="vertical-align:top;">
<p><span class="font0">15.16</span></p></td><td style="vertical-align:top;">
<p><span class="font0">13.90</span></p></td><td style="vertical-align:top;">
<p><span class="font0">13.87</span></p></td></tr>
</table>
<p><span class="font9">This EER% clearly shows that the error rate is less if we use the SVM with the huge input data for feed and this can enhance the accuracy rate as well. The SVM used in this method is tested as per other algorithms and the comparison between the accuracy of dataset used is mentioned. These error rate and accuracy details are claimed in referred journal by author using the same classifier and databases. The accuracy of model/classifier varies as we switch from NUAA database to CASIA database. This accuracy is enhanced when SVM is used with the CASIA database and the outputs vary in a better way.</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font7">NUAA</span></p></td><td style="vertical-align:top;">
<p><span class="font7">CASIA-FASD</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font7">Dev &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Test</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7">Dev</span></p>
<p><span class="font7">Test</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font7">LBP<sup>u2</sup></span><span class="font4">3X3 </span><span class="font7">+ LDA</span></p></td><td style="vertical-align:top;">
<p><span class="font7">0.06 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;18.32</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7">17.08</span></p>
<p><span class="font7">21.01</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font7">LBP<sup>u2</sup></span><span class="font4">3X3 </span><span class="font7">+ SVM</span></p></td><td style="vertical-align:top;">
<p><span class="font7">0.11 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;19.03</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7">16.00</span></p>
<p><span class="font7">18.17</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font7">LBP[7] + SVM *</span></p></td><td style="vertical-align:top;">
<p><span class="font7">0.11 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;13.17</span></p></td><td style="vertical-align:bottom;">
<p><span class="font7">15.43</span></p>
<p><span class="font7">18.21</span></p></td></tr>
</table>
<p><span class="font9">5. Database</span></p>
<p><span class="font9">As we know that any algorithm can work efficiently and with a good accuracy when is given a huge amount of data as input feed. Hence here we require a lot of data for the training and validation of the algorithm i.e. SVM. The data here is set of images having two classes that are: Real images that are taken live and can be considered as once captured images and the second class consist of images that are spoof and can be considered as fake/recaptured images.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">5.1 &nbsp;&nbsp;&nbsp;NUAA</span></p></li></ul>
<p><span class="font9">The database used in some traditional methods was NUAA and it consist of 15 subjects in the dataset, every one of them consist of real face of the subject, and photograph of them. Real face is taken from webcam with natural expression and frontally face the camera, there is no movement such as eye blink, this is used to make the real face similar like the photograph.</span></p><img src="https://jurnal.harianregional.com/media/58992-10.jpg" alt="" style="width:248pt;height:192pt;">
<p><span class="font9">Each column from the different section 1, section 2 and section 3. In each row, the left side image set are from a real human face and the right side image set from a photo. This dataset consist of various types of changes in the images of different subjects and these changes are like their gender variation, the intensity of light , use of spectacles etc . All the images in the dataset are of same resolution of 640 x 480 pixels.</span></p><img src="https://jurnal.harianregional.com/media/58992-11.jpg" alt="" style="width:238pt;height:117pt;">
<ul style="list-style:none;"><li>
<p><span class="font9">5.2 &nbsp;&nbsp;&nbsp;CASIA</span></p></li></ul>
<p><span class="font9">CASIA Face Image Database Version 5.0 (or CASIA-FaceV5) is the latest updated CASIA dataset for faces and it consists of a total of 2,500 colored facial images of 500 different subjects(persons). The images of faces in this dataset are captured in a single session using the specific Logitech USB camera. The subjects in this dataset are not professional research scholars but they are normal people like graduate students, workers, waiters etc . The images provided in this dataset consist of similar features as they all have the same format of BMP extension and they are 16 bit color images. The resolution of these images is 640*480. There are also some of the differences found in the images of this dataset and this is like intensity of light variation, the posture of person, the expression shown by subject, the distance etc.</span></p><img src="https://jurnal.harianregional.com/media/58992-12.jpg" alt="" style="width:360pt;height:145pt;">
<p><span class="font9">The images of the dataset of CASIA are stored in a specific format and it provides the whole data into subsets and if we wish to download the cropped images we can get them within 150 MB and the downloaded data is also in specified format.</span></p>
<p><span class="font9">The actual dataset has a more detailed and complete images without cropped and this dataset is available in more than two forms , one of which is so small in size for the testing purpose of for the demo and the second one is cropped images and the real ones as well. These images differ in various aspects like size , quality etc.</span></p>
<p><span class="font9">This data is more relevant and appropriate that provides a better accuracy in prediction and this paper uses the CASIA dataset for a better quantity and quality of data.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">6. &nbsp;&nbsp;&nbsp;CONCLUSION</span></p></li></ul>
<p><span class="font9">This paper provides a way to study the approach to face anti-spoofing method using the CASIA database available for face biometric research and SVM classifier to work on that data. In here we have used the training images in LBP format which then is transformed to histogram having the frequency distribution for feeding the algorithm. The algorithm provides a better efficiency and a reduced error rate with efficient approach. In this we have used only the prediction for images and frames but it can be enhanced by using the video content in account for the training purpose as it will check the liveliness in a better way and the error rate can be further reduced and accuracy can be enhanced.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">7. &nbsp;&nbsp;&nbsp;Acknowledgement</span></p></li></ul>
<p><span class="font9">We would like to thank our deep sense of gratitude to college , that provided us an opportunity to write a paper . We are very thankful to the Management and the Director of the Institute for the help they provided us during the writing the content of this paper .We would also like to give special thanks to our HOD”s of CSE &amp;&nbsp;IT &amp;&nbsp;MCA Dept. and collogues of the college for their true encouragement and guidance in the completion of the paper.</span></p>
<p><span class="font9">References:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font9">[1] &nbsp;&nbsp;&nbsp;Portions of the research in this paper use the CASIA-FaceV5 collected by the Chinese Academy of Sciences' Institute of Automation (CASIA) Images for Data set are referred from “CASIA-FaceV5, </span><a href="http://biometrics.idealtest.org/"><span class="font9" style="text-decoration:underline;">http://biometrics.idealtest.org/</span></a><span class="font9"> ”</span></p></li>
<li>
<p><span class="font9">[2] &nbsp;&nbsp;&nbsp;P. V. Reddy, A. Kumar, S. Rahman, and T. S. Mundra, “A new anti spoofing approach for biometric devices,” Biomedical Circuits and Systems, IEEE Transactions on, vol. 2, no. 4, pp. 328–s337, 2008.</span></p></li>
<li>
<p><span class="font9">[3] &nbsp;&nbsp;&nbsp;S. Parveen, S. Ahmad, S. Mumtazah, M. Hanafi, W. Adnan, and W. Azizun, “Face anti-spoofing methods.” Current Science (00113891), vol. 108, no. 8, 2015.</span></p></li>
<li>
<p><span class="font9">[4] &nbsp;&nbsp;&nbsp;Z. Zhang, J. Yan, S. Liu, Z. Lei, D. Yi, and S. Z. Li, “A face anti spoofing database with diverse attacks,” in Biometrics (ICB), 2012 5th IAPR International Conference on. IEEE, 2012, pp. 26–31.</span></p></li>
<li>
<p><span class="font9">[5] &nbsp;&nbsp;&nbsp;J. Galbally and S. Marcel, “Face anti-spoofing based on general image quality assessment,” in Pattern Recognition (ICPR), 2014 22nd International Conference on. IEEE, 2014, pp. 1173–1178.</span></p></li>
<li>
<p><span class="font9">[6] &nbsp;&nbsp;&nbsp;T. de Freitas Pereira, J. Komulainen, A. Anjos, J. M. De Martino,A. Hadid, M. Pietikäinen, and S. Marcel, “Face liveness detection using dynamic texture,” EURASIP Journal on Image and Video Processing,vol. 2014, no. 1, p. 2, 2014.</span></p></li>
<li>
<p><span class="font9">[7] &nbsp;&nbsp;&nbsp;J. Komulainen, A. Hadid, and M. Pietikainen, “Context based face anti-spoofing,” in Biometrics: Theory, Applications and Systems (BTAS),2013 IEEE Sixth International Conference on. IEEE, 2013, pp. 1–8.</span></p></li>
<li>
<p><span class="font9">[8] &nbsp;&nbsp;&nbsp;K. Patel, H. Han, and A. K. Jain, ‘‘Cross-database face anti spoofing with robust feature representation,’’ in Proc. Chin. Conf. Biometric Recognit . Cham, Switzerland: Springer, 2016, pp. 611–619</span></p></li>
<li>
<p><span class="font9">[9] &nbsp;&nbsp;&nbsp;Z. Wang, C. Zhao, Y. Qin, Q. Zhou, G. Qi, J. Wan, and Z. Lei, ‘‘Exploiting temporal and depth information for multi-frame face anti-spoofing,’’ 2018,arXiv:1811.05118. [Online]. Available: </span><a href="https://arxiv.org/abs/1811.05118"><span class="font9" style="text-decoration:underline;">https://arxiv.org/abs/1811.05118</span></a></p></li>
<li>
<p><span class="font9">[10] &nbsp;&nbsp;&nbsp;Gang Pan, Zhaohui Wu and Lin Sun, Liveness Detection for Face Recognition, Recent Advances in Face Recognition, I-Tech, on Page(s): 236, December, 2008</span></p></li>
<li>
<p><span class="font9">[11] &nbsp;&nbsp;&nbsp;R. Duda, P. Hart, and D. Stork, Pattern Classification, 2nd ed. John Wiley &amp;&nbsp;Sons, New York, 2001.</span></p></li>
<li>
<p><span class="font9">[12] &nbsp;&nbsp;&nbsp;D. Wen, H. Han, and A. K. Jain, “Face spoof detection with image distortion analysis,” Information Forensics and Security, IEEE Trans-actions on, vol. 10, no. 4, pp. 746–761, 2015.</span></p></li>
<li>
<p><span class="font9">[13] &nbsp;&nbsp;&nbsp;C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector machines,” ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1–27:27, 2011, software available at </span><a href="http://www.csie.ntu.edu.tw/"><span class="font9">http://www.csie.ntu.edu.tw/</span></a><span class="font9"> </span><span class="font2">∼</span><span class="font9"> cjlin/libsvm.</span></p></li>
<li>
<p><span class="font9">[14] &nbsp;&nbsp;&nbsp;K. Kollreider, H. Fronthaler, and J. Bigun, Non-intrusive liveness detection by face images, Image and Vision Computing, vol. 27(3), pp. 233-244, 2009. Scholarpedia :</span></p></li></ul>
<p><span class="font5">46</span></p>