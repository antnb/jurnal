---
layout: full_article
title: "O ONTOLOGY-BASED PARAGRAPH EXTRACTION AND CAUSALITY DETECTION-BASED SIMILARITY FOR ANSWERING WHY-QUESTION"
author: "A A I N Eka Karyawati"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-39731 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-39731"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-39731"  
comments: true
---

<p><span class="font8">Jurnal Ilmiah</span></p><a name="caption1"></a>
<h2><a name="bookmark0"></a><span class="font8" style="font-weight:bold;"><a name="bookmark1"></a>ILMU KOMPUTER</span></h2>
<p><span class="font8">Universitas Udayana</span></p>
<p><span class="font8" style="font-weight:bold;">Vol. XI, No. 1, April 2018</span></p>
<div>
<p><span class="font8" style="font-weight:bold;">ISSN 1979 - 5661</span></p>
</div><br clear="all">
<p><span class="font9" style="font-weight:bold;">ONTOLOGY-BASED PARAGRAPH EXTRACTION AND CAUSALITY DETECTION-BASED SIMILARITY FOR ANSWERING WHY-QUESTION</span></p>
<p><span class="font9" style="font-weight:bold;">A.A.I.N. Eka Karyawati</span></p>
<p><span class="font8">Computer Science/Informatics Program of Mathematics and Natural Sciences Faculty, Udayana University</span></p>
<p><a href="mailto:eka.karyawati@cs.unud.ac.id"><span class="font8" style="text-decoration:underline;">eka.karyawati@cs.unud.ac.id</span></a></p>
<h2><a name="bookmark2"></a><span class="font8" style="font-weight:bold;"><a name="bookmark3"></a>ABSTRACT</span></h2>
<p><span class="font8" style="font-style:italic;">Paragraph extraction is a main part of an automatic question answering system, especially in answering why-question. It is because the answer of a why-question usually contained in one paragraph instead of one or two sentences. There have been some researches on paragraph extraction approaches, but there are still few studies focusing on involving the domain ontology as a knowledge base. Most of the paragraph extraction studies used keywordbased method with small portion of semantic approaches. Thus, the question answering system faces a typical problem often occuring in keyword-based method that is word mismatches problem. The main contribution of this research is a paragraph scoring method that incorporates the TFIDF-based and causality-detection-based similarity. This research is a part of the ontology-based why-question answering method, where ontology is used as a knowledge base for each steps of the method including indexing, question analyzing, document retrieval, and paragraph extraction/selection. For measuring the method performance, the evaluations were conducted by comparing the proposed method over two baselines methods that did not use causality-detection-based similarity. The proposed method shown improvements over the baseline methods regarding MRR (95%, 0.82-0.42), P@1 (105%, 0.78-0.38), P@5(91%, 0.880.46), Precision (95%, 0.80-0.41), and Recall (66%, 0.88-0.53).</span></p>
<p><span class="font8" style="font-weight:bold;font-style:italic;">Keyword:</span><span class="font8"> Ontology-Based Question Answering, Paragraph Retrieval, Why-Question Answering,Why-Question, Causality Detection</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font8" style="font-weight:bold;"><a name="bookmark5"></a>1. &nbsp;&nbsp;&nbsp;INTRODUCTION</span></h2></li></ul>
<p><span class="font8">In the typical QA systems based on the document collection, the keyword-based approach are usually used to handle each step of the document retrieval process (Soricut &amp;&nbsp;Brill, 2006; Higashinaka &amp;&nbsp;Isozaki, 2008; Mori et al., 2008; Nakakura &amp;&nbsp;Fu-kumoto, 2008; Verberne et al., 2010; Verberne et al., 2011; Oh et al., 2012; Oh et</span></p>
<p><span class="font8">al., 2013). The keyword-based QA provides limited capabilities to capture the conceptualizations associated with user needs and document contents. Thus, the word mismatch often occurs because the query and the documents cannot represent the information correctly.</span></p>
<p><span class="font8">The word mismatch problem refers to the unsuitable use of words to describe the</span></p>
<p><span class="font8">similar concepts/relations in a question and in documents (i.e., paragraphs). The words used by a user to describe concepts/relations in a question are different from used by authors in documents to describe the same concepts/relation. For example, a question, &quot;Why is a VSM employed in text retrieval system?&quot;, and a document that contains multi-word term &quot;vector space model&quot; and relation &quot;in order to&quot;, where word &quot;VSM&quot; and multi-word &quot;vector space model&quot; describe the same concept (i.e., &quot;VectorSpaceModel&quot; concept), and question word &quot;why&quot; and relation &quot;in order to&quot; describe the same relation (i.e., &quot;causal&quot; relation). The document even contains concepts and relations asked by the user, is not retrieved as a relevant document. Thus, the word-mismatch problem causes the answers are not accurately extracted because most relevant documents that contain answers will not be retrieved. It will decrease the performance of the why-QA system.</span></p>
<p><span class="font8">The limitation to capture conceptualization of the user needs and the document contents can be solved by using an idea of semantic-based search that is a searching over the document collection based on meaning rather than a literal string (Fernandez et al., 2011). The previous semantic-based document retrieval methods (Castells et al., 2007; Fernandez et. al., 2011) that employed domain ontology are suitable for general questions, not for the why-questions because the approach does not consider the causality detection.</span></p>
<p><span class="font8">In order to solve the issue of the word mismatch problem in the keywordbased why-QA, and the issue of regardless of the causality detection in the ontologybased IR, the paragraph extraction method that incorporated the causality detection into the ontology-based IR is proposed.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8" style="font-weight:bold;">2. &nbsp;&nbsp;&nbsp;THE PROPOSED PARAGRAPH EXTRACTION METHOD FOR ANSWERING WHY-QUESTION</span></p></li></ul>
<p><span class="font8">The proposed ontology-based paragraph extraction method involves two components which are the paragraph filtering and indexing, and the paragraph extraction. The paragraph indexing uses semantic annotations based on the domain ontology underlying the question answering system. In the paragraph extraction, a paragraph scoring method involves two measures including the relevance and the appropriateness measure (Han et al. 2006). Figure 1 presents the graphical representation of the paragraph extraction method. The filled box represents the component that has a contribution. The main contribution is a paragraph scoring method. The proposed paragraph extraction method introduces a scoring formula that incorporates the causality-detection-based similarity (i.e., refers to as the appropriateness measure) into an ontologybased TFIDF model (i.e., refers to as the relevance measure).</span></p>
<p><span class="font8">As can be seen in Figure 1, a list of the top-10 relevant documents (from document retrieval) is used as input for the paragraph filtering and indexing phase. There are three outputs of this phase which are a list of filtered paragraphs, an inverted semantic index, and a TFIDF matrix. The outputs are used as inputs of the paragraph extraction. The output of the paragraph extraction is a list of extracted paragraphs. CA (a set of causality annotations), OSA (a set of original semantic annotations) and ASA (a set of additional semantic annotations) are semantic annotations of a question obtained from question analysis step (Karyawati et al., 2015).</span></p>
<div><img src="https://jurnal.harianregional.com/media/39731-1.jpg" alt="" style="width:366pt;height:360pt;">
<p><span class="font8" style="font-weight:bold;">Figure 1 </span><span class="font8">The Proposed Paragraph Extraction Method</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font7" style="font-weight:bold;"><a name="bookmark7"></a>2.1.</span><span class="font8" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Paragraph Filtering and Indexing</span></h2></li></ul>
<p><span class="font8">The paragraph filtering and indexing is performed offline. As can be seen in Figure 1, there are four main components of the paragraph filtering and indexing including the segmentation, the semantic annotations identification, the filtering, and the semantic index construction.</span></p>
<h2><a name="bookmark8"></a><span class="font8" style="font-weight:bold;"><a name="bookmark9"></a>Segmentation</span></h2>
<p><span class="font8">The first component of semantic index construction is segmentation, which has the goal to segment the documents into paragraphs. Verberne’s work (Verberne, 2006) resulted that a complete paragraph is a satisfactory answer for more than 80% of why-questions. Thus, the proposed method employs fixed-sized based segmentation by</span></p>
<p><span class="font8">segmenting the retrieved documents (the top-10 documents) into one-paragraph segments.</span></p>
<h2><a name="bookmark10"></a><span class="font8" style="font-weight:bold;"><a name="bookmark11"></a>Semantic annotations identification</span></h2>
<p><span class="font8">After segmentation, a list of semantic annotations of each paragraph is identified based on the </span><span class="font8" style="font-style:italic;">label</span><span class="font8">-</span><span class="font8" style="font-style:italic;">instanceName </span><span class="font8">pairs of the domain ontology-lexicon. Because the paragraphs are usually short texts, the semantic annotations identification do not involve </span><span class="font8" style="font-style:italic;">Lucene</span><span class="font8"> method. A heuristic algorithm is built to identify the semantic entities contained in a paragraph, its occurrences, as well as its positions.</span></p>
<p><span class="font8">The algorithm is based on n-gram model. Firstly, a paragraph is split into sentences. Then, the longest word sequence (e.g., with length=6) of a sentence is mapped</span></p>
<p><span class="font8">into the semantic entities (i.e., instances in ontology-lexicon), followed by the shorter one (e.g., with length=5), one by one, until no word or word sequence can be mapped. The process is continued until all sentences are handled. The positions of each semantic entity are also recorded.</span></p>
<h2><a name="bookmark12"></a><span class="font8" style="font-weight:bold;"><a name="bookmark13"></a>Filtering</span></h2>
<p><span class="font8">The paragraphs of all documents in the collection are filtered to keep only the paragraphs that contain causalities. This filtering process uses OR-Boolean search, where the Boolean query is given by,</span></p>
<p><span class="font8" style="font-style:italic;">Q</span><span class="font1"> = </span><span class="font8" style="font-style:italic;">X</span><span class="font8"><sub>1</sub>OR </span><span class="font8" style="font-style:italic;">X</span><span class="font6">2</span><span class="font8">OR...OR </span><span class="font8" style="font-style:italic;">Xn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;^</span></p>
<p><span class="font8">where </span><span class="font8" style="font-style:italic;">X</span><span class="font5" style="font-style:italic;">i</span><span class="font8"> is a causality annotation, </span><span class="font8" style="font-style:italic;">X</span><span class="font5" style="font-style:italic;">i </span><span class="font4" style="font-style:italic;">∈</span><span class="font0" style="font-style:italic;"> </span><span class="font8">{</span><span class="font8" style="font-style:italic;">Causality1</span><span class="font8">, &nbsp;&nbsp;&nbsp;</span><span class="font8" style="font-style:italic;">Causality2</span><span class="font8">, &nbsp;&nbsp;&nbsp;</span><span class="font8" style="font-style:italic;">Causality3,</span></p>
<p><span class="font8" style="font-style:italic;">Causality4</span><span class="font8">}.</span></p>
<p><span class="font8">The filtered paragraphs are recorded and saved in text-file. The meta-data of paragraphs includes information of paragraphs ID, paragraph content, a list of semantic annotations (i.e., referring to semantic entities) of the paragraphs, the title of a document where the paragraphs belong to, and semantic annotations of the title.</span></p>
<h2><a name="bookmark14"></a><span class="font8" style="font-weight:bold;"><a name="bookmark15"></a>Inverted Semantic Index Construction</span></h2>
<p><span class="font8">An inverted index of paragraphs collection is constructed for accelerating the paragraph extraction process. In indexing of the paragraphs, besides the occurrence of each term, the positions of the term within a document also store in the index. The reason is that the paragraph scoring also employs a combination of </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8">-based- and causalitydetection-based cosine similarity, where the causality detection is estimated by considering the proximity among the semantic annotations of a why-question (i.e., OSA, ASA, CA).</span></p>
<p><span class="font8">The &nbsp;&nbsp;&nbsp;&nbsp;semantic &nbsp;&nbsp;&nbsp;&nbsp;annotations,</span></p>
<p><span class="font8">calculation of the Semantic Entity Frequency (SEF), and the Semantic Entity Positions (SEP), is performed using a</span></p>
<p><span class="font8">heuristic algorithm. A semantic index of paragraphs collection is constructed after identifying the semantic annotations of each paragraph, and after identifying the semantic entity frequencies and positions within a paragraph. The semantic index is a four-column table. The first column is the instance name, the second column is the paragraph ID, the third column is the SEF, and the fourth column is a list of the SEP within the paragraph.</span></p>
<p><span class="font8">Inverted indexing process involves two main steps including inverting the semantic index into an inverted semantic index and then sorting the inverted index based on the paragraph ID. Thus, the inverted index is a four-column table. The first column is instance name, the second column is paragraph ID, the third column is SEF, and the fourth column is a list of the SEP within the paragraph. The semantic index construction returns not only an inverted semantic index of the paragraph collection but also a </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8"> matrix.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font7" style="font-weight:bold;"><a name="bookmark17"></a>2.2.</span><span class="font8" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;The proposed paragraph extraction</span></h2></li></ul>
<p><span class="font8">As can be seen in Figure 1, the paragraph extraction involves four main components which are the searching, the relevance scoring, the appropriateness scoring, and the paragraph ranking and selection.</span></p>
<h2><a name="bookmark18"></a><span class="font8" style="font-weight:bold;"><a name="bookmark19"></a>Searching</span></h2>
<p><span class="font8">Before ranking the paragraphs, the searching is performed to keep only the paragraphs containing all question focuses (got from OSA) and at least one ASA. The reason for using the only question focus not all elements of OSA is that paragraph is usually short. Thus, not all terms (i.e., semantic entities) contain in a question will occur in a relevant paragraph. Similar to the document search, the Boolean search model is applied in this paragraph search. The</span></p>
<p><span class="font8">searched query must satisfy the formula below,</span></p>
<div>
<p><span class="font7" style="font-style:italic;">Q</span><span class="font0"> = </span><span class="font2">(</span><span class="font7" style="font-style:italic;">X</span><span class="font7"><sub>1</sub> AND </span><span class="font7" style="font-style:italic;">X</span><span class="font5" style="font-style:italic;">2</span><span class="font7"> AND... AND </span><span class="font7" style="font-style:italic;">X</span><span class="font5" style="font-style:italic;">N<sub>i</sub></span><span class="font2"> )</span><span class="font7">AND</span><span class="font2">(</span><span class="font7" style="font-style:italic;">Y</span><span class="font7"><sub>1</sub> OR </span><span class="font7" style="font-style:italic;">Y</span><span class="font7"><sub>2</sub> OR... OR </span><span class="font7" style="font-style:italic;">Y</span><span class="font5" style="font-style:italic;">N^</span><span class="font2">)</span></p>
</div><br clear="all">
<p><span class="font8">where </span><span class="font8" style="font-style:italic;">X</span><span class="font5">1</span><span class="font8">, </span><span class="font8" style="font-style:italic;">X</span><span class="font5">2</span><span class="font8">, …, </span><span class="font8" style="font-style:italic;">X</span><span class="font5" style="font-style:italic;">N</span><span class="font5">1 </span><span class="font8">are elements of question focus, and </span><span class="font8" style="font-style:italic;">Y</span><span class="font5">1</span><span class="font8">, </span><span class="font8" style="font-style:italic;">Y</span><span class="font5">2</span><span class="font8">, …, </span><span class="font8" style="font-style:italic;">Y</span><span class="font5" style="font-style:italic;">N</span><span class="font5">2 </span><span class="font8">are elements of ASA. CA is not involved because the paragraphs have already been filtered using the CA.</span></p>
<h2><a name="bookmark20"></a><span class="font8" style="font-weight:bold;"><a name="bookmark21"></a>Relevance scoring</span></h2>
<p><span class="font8">The relevance score used in this research is the </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8">-based similarity. The </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8">-based ranking of paragraphs refers to cosine similarity score between a paragraph vector and a query vector. The paragraphs vectors are identified from </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8"> matrix, where the </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8"> matrix is constructed offline in the indexing process. The semantic entity weighting of a query defines TF as 0/1. The TF of a semantic entity is 1 if the semantic entity annotates the question, and otherwise, the value is 0.</span></p>
<p><span class="font8">A query is represented only by the presence of question focus, instead of the presence of both, OSA and ASA. The reason</span></p>
<p><span class="font8">(2)</span></p>
<p><span class="font8">is that the limitation of the semantic entities contained in a paragraph. The ASA involvement in the query representation will not affect the performance, even decrease it.</span></p>
<h2><a name="bookmark22"></a><span class="font8" style="font-weight:bold;"><a name="bookmark23"></a>The appropriateness scoring</span></h2>
<p><span class="font8">The appropriateness score is estimated by the causality-detection-based similarity value. The causality-detectionbased similarity for paragraph ranking is estimated by constructing causality vectors of a question and a paragraph. The causality vectors are constructed on the fly, after identifying semantic annotations of a question (i.e., OSA, ASA, and CA).</span></p>
<p><span class="font8">To get the most relevant document that contains a suitable causality patterns, the causality-detection based similarity is defined as a maximum value among the causality-based cosine similarity of the four type of causality patterns formulated as,</span></p><a name="caption2"></a>
<h1><a name="bookmark24"></a><span class="font9" style="font-style:italic;"><a name="bookmark25"></a>CausalityCosSim</span><span class="font9"> = </span><span class="font9" style="font-style:italic;">max(CausalityCosSim<sub>1</sub>, CausalityCosSim<sub>2</sub>, CausalityCosSim<sub>3</sub>, CausalityCosSim<sub>4</sub>)</span></h1>
<div>
<p><span class="font8">(3)</span></p>
</div><br clear="all">
<p><span class="font8">Where</span><span class="font8" style="font-style:italic;">CausalityCosSim</span><span class="font5" style="font-style:italic;">1</span><span class="font8">, </span><span class="font8" style="font-style:italic;">Causality-CosSim</span><span class="font5" style="font-style:italic;">2</span><span class="font8">, </span><span class="font8" style="font-style:italic;">CausalityCosSim</span><span class="font5" style="font-style:italic;">3</span><span class="font8">, and </span><span class="font8" style="font-style:italic;">CausalityCosSim</span><span class="font5" style="font-style:italic;">4</span><span class="font8"> are the relevance values of a documents (with respect to a query) estimated by calculating the causalitydetection-based-similarity corresponding to</span></p>
<p><span class="font8">the causality </span><span class="font8" style="font-style:italic;">Pattern1</span><span class="font8">, </span><span class="font8" style="font-style:italic;">Pattern2</span><span class="font8">, </span><span class="font8" style="font-style:italic;">Pattern3</span><span class="font8">, and </span><span class="font8" style="font-style:italic;">Pattern4.</span></p>
<p><span class="font8">Four types of causality patterns based on position/closeness (i.e., proximity) of the terms (i.e., the term proximity considers order of the terms) is (Khoo, 1995; Khoo et al., 2001):</span></p>
<p><span class="font7" style="font-style:italic;">Cause</span><span class="font0"> - </span><span class="font7" style="font-style:italic;">CausalityA</span><span class="font0"> - </span><span class="font7" style="font-style:italic;">EEffect</span><span class="font7"> (</span><span class="font7" style="font-style:italic;">Pattern!) Ejfect</span><span class="font0">- </span><span class="font7" style="font-style:italic;">CausalityB</span><span class="font0" style="font-style:italic;">- </span><span class="font7" style="font-style:italic;">Cause</span><span class="font7"> (</span><span class="font7" style="font-style:italic;">Pattern!) CausalityA </span><span class="font0" style="font-style:italic;">- </span><span class="font7" style="font-style:italic;">EEffect</span><span class="font0"> - </span><span class="font7" style="font-style:italic;">Cause</span><span class="font7"> (</span><span class="font7" style="font-style:italic;">Pattern!) CausalityB</span><span class="font0"> - </span><span class="font7" style="font-style:italic;">Cause</span><span class="font0"> - </span><span class="font7" style="font-style:italic;">EEffect</span><span class="font7"> (</span><span class="font7" style="font-style:italic;">Pattern4)</span></p>
<div>
<p><span class="font8">(4)</span></p>
</div><br clear="all">
<p><span class="font8">The causality vectors are the vectors that represent causality matching values (i.e., 0/1) between the causality patterns that present in the query and that present in the document. Because the causality-detectionbased similarity estimates how similar is the causality patterns in a document to the causality patterns in a query, the query causality vectors are set to be the vectors of ones (i.e., vectors whose all elements are 1). Moreover, elements of the document causality vectors are designed to represent the presence of the corresponding causality patterns (of the query) in the document.</span></p>
<p><span class="font8">The causality-detection-based similarity is a linear combination of OSA-co-occurrence-patterns-based and ASA-cooccurrence-pattern-based cosine similarity. The former are the similarities between the document and the query causality vectors that only consider the co-occurrence of the OSA (i.e., </span><span class="font8" style="font-style:italic;">CausalityCosSimA</span><span class="font8">). The latter is the similarity between the document and the query causality vector that considers the co-/occurrence of OSA and cooccurrence of ASA in the causality patterns (i.e., </span><span class="font8" style="font-style:italic;">CausalityCosSimB</span><span class="font8">). The causality cosine similarity of each pattern type (i = 1, 2, 3, 4) is formulated as,</span></p>
<h1><a name="bookmark26"></a><span class="font9" style="font-style:italic;"><a name="bookmark27"></a>CausalityCosSim</span><span class="font6" style="font-style:italic;">ι</span><span class="font9" style="font-style:italic;">(d, q)</span><span class="font9"> = </span><span class="font9" style="font-style:italic;">λCausalityCosSimA(d, q)</span><span class="font9"> + (1 — </span><span class="font9" style="font-style:italic;">λ~) Causality C osSim.B(d,q') </span><span class="font8">(5)</span></h1>
<p><span class="font8">where </span><span class="font0" style="font-style:italic;">λ </span><span class="font4" style="font-style:italic;">∈</span><span class="font8"> [0, 1].</span></p>
<h2><a name="bookmark28"></a><span class="font8" style="font-weight:bold;"><a name="bookmark29"></a>The paragraph scoring and selection</span></h2>
<p><span class="font8">The proposed scoring formula is defined as a linear combination of the relevance of the paragraph with respect to the query measure and the appropriateness of the writing style measure. The proposed</span></p>
<p><span class="font8">paragraph extraction introduces a scoring method that incorporates the proximitybased causality detection (referring to as an appropriateness measure) and the ontologybased </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8"> model (referring to as a relevance measure). The scoring formula is given by,</span></p>
<h1><a name="bookmark30"></a><span class="font9" style="font-style:italic;"><a name="bookmark31"></a>Score (</span><span class="font9" style="font-weight:bold;font-style:italic;">p, q</span><span class="font9" style="font-style:italic;">) = </span><span class="font0" style="font-style:italic;">λ</span><span class="font9" style="font-style:italic;">AppropriatenessScore(</span><span class="font9" style="font-weight:bold;font-style:italic;">p, q</span><span class="font9" style="font-style:italic;">) + (1-</span><span class="font0" style="font-style:italic;">λ</span><span class="font9" style="font-style:italic;">)RelevanceScore(</span><span class="font9" style="font-weight:bold;font-style:italic;">p, q</span><span class="font9" style="font-style:italic;">)</span><span class="font8"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6)</span></h1>
<p><span class="font8">where </span><span class="font0" style="font-style:italic;">λ</span><span class="font0"> </span><span class="font4">∈</span><span class="font0"> </span><span class="font8">[0,1], and </span><span class="font0" style="font-style:italic;">λ</span><span class="font8"> is set to be 0.6 because the value is found to work well, empirically. The term </span><span class="font8" style="font-weight:bold;">p </span><span class="font8">and </span><span class="font8" style="font-weight:bold;">q </span><span class="font8">stand for paragraph and question, respectively.</span></p>
<p><span class="font8">The proposed paragraph selection method uses a specific threshold value to determine whether a sentence is selected or not. The paragraph will be selected if the similarity score is greater than the threshold value. In this research, the threshold value is set to be 0.125, since that value makes the evaluation results seem good.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8" style="font-weight:bold;">3. &nbsp;&nbsp;&nbsp;EXPERIMENTS AND RESULTS</span></p>
<ul style="list-style:none;">
<li>
<h2><a name="bookmark32"></a><span class="font7" style="font-weight:bold;"><a name="bookmark33"></a>3.1.</span><span class="font8" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Experiments and Data</span></h2></li></ul></li></ul>
<p><span class="font8">The proposed paragraph extraction method is compared against two baseline methods. Both baseline methods are the paragraph extraction that employs a scoring method only based on the relevance measure. The first baseline method uses a relevance measure estimated by using the TFIDF-based similarity using question focuses, where the documents are retrieved using the ontology-based TFIDF method with Query Expansion (QE). The second</span></p>
<p><span class="font8">baseline method uses a relevance measure estimated also by using the TFIDF-based similarity using question focuses, but the documents are retrieved using the ontologybased TFIDF method without QE.</span></p>
<p><span class="font8">The evaluation is performed by conducting some experiments to measure the effectiveness and efficiency of the methods. The effectiveness of the methods is estimated by calculating the five standard evaluation measures, </span><span class="font8" style="font-style:italic;">MRR</span><span class="font8"> (Mean Reciprocal Rank), </span><span class="font8" style="font-style:italic;">P@1</span><span class="font8">, </span><span class="font8" style="font-style:italic;">P@5</span><span class="font8">, </span><span class="font8" style="font-style:italic;">Precision</span><span class="font8">, and </span><span class="font8" style="font-style:italic;">Recall</span><span class="font8"> of each method (Manning et al., 2008; Baeza-Yates &amp;&nbsp;Ribeiro-Neto, 1999; Thom &amp;&nbsp;Scholer, 2007). Moreover, the efficiency of the methods is estimated by calculating the runtime of the system when the method is executed.</span></p>
<p><span class="font8">The experiments are conducted by generating randomly 10, 20, 30, and 40 questions from the why-question collection in 10 iterations, where the total number of questions available is 5921 why-questions. The evaluation performances are the average values of each measure.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark34"></a><span class="font7" style="font-weight:bold;"><a name="bookmark35"></a>3.2.</span><span class="font8" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Results and Discussion</span></h2></li></ul>
<p><span class="font8">Table 1 shows the results of the evaluation of the proposed paragraph extraction against the three other methods. Values in bold correspond to the best results for the corresponding metrics. It is the surprising results because the proposed method that only involves </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8">-based similarity in estimating the relevance score outperforms the alternative method that involves the combination of </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8">-based and context-information-based similarity.</span></p>
<p><span class="font8">As shown in Table 1, the proposed method returns the improvement over the first baseline method in term of MRR (95,2%, 0.82-0.42), </span><span class="font8" style="font-style:italic;">P@1</span><span class="font8"> (105%, 0.780.38), </span><span class="font8" style="font-style:italic;">P@5</span><span class="font8"> (91%, 0.88-0.46), </span><span class="font8" style="font-style:italic;">Precision </span><span class="font8">(95%, 0.80-0.41), and </span><span class="font8" style="font-style:italic;">Recall</span><span class="font8"> (66%, 0.88-</span></p>
<p><span class="font8">0.53), and over the second baseline method in term of </span><span class="font8" style="font-style:italic;">MRR</span><span class="font8"> (173%, 0.82-0.30), </span><span class="font8" style="font-style:italic;">P@1 </span><span class="font8">(200%, 0.78-0.26), </span><span class="font8" style="font-style:italic;">P@5</span><span class="font8"> (144%, 0.88-0.36), </span><span class="font8" style="font-style:italic;">Precision</span><span class="font8"> (208%, 0.80-0.26), and </span><span class="font8" style="font-style:italic;">Recall </span><span class="font8">(109%, 0.88-0.42).</span></p>
<p><span class="font8">The second baseline method that is based on </span><span class="font8" style="font-style:italic;">TFIDF</span><span class="font8">-based similarity without QE becomes the worst method due to the use of only the question focuses without additional semantic annotation in query representation (in retrieving documents). On the other hand, the first baseline method even involves the QE by adding ASA to the original query, but still underperforms the proposed method. The reason is that the first baseline method does not consider the causality detection. Thus, it can be said that the causality detection is important to improve the performance of the paragraph extraction method.</span></p>
<p><span class="font8">The experiment results show the good values (&gt;=0.78) of all performance measures used in the evaluation of the proposed method because the questions used in the experiments are in well-ordered forms, the question patterns, and the concepts and relations contained in the questions are recognized by the system.</span></p>
<p><span class="font8">Besides estimating the effectiveness of the proposed methods by comparing the methods based on the five performances metric (i.e., </span><span class="font8" style="font-style:italic;">MRR</span><span class="font8">, </span><span class="font8" style="font-style:italic;">P@1</span><span class="font8">, </span><span class="font8" style="font-style:italic;">P@2</span><span class="font8">, </span><span class="font8" style="font-style:italic;">Precision</span><span class="font8">, and </span><span class="font8" style="font-style:italic;">Recall</span><span class="font8"> value) as explained above, another aspect also estimated is the efficiency of the methods by comparing the average values of runtimes among the four methods. As can be seen in Table 1, the efficiency of the proposed method is about 5 seconds, the first baseline method is about 2 seconds, and the second baseline method is about 1.5 seconds. It means that the proposed method consumes running time more than twice longer than both baseline methods.</span></p>
<div>
<p><span class="font8" style="font-weight:bold;">Table 7.3 </span><span class="font8">Comparison results of the proposed paragraph extraction methods against two baseline methods</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Metrics</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">The Proposed Method</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">The First Baseline Method</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">The Second Baseline Method</span></p></td></tr>
<tr><td rowspan="6" style="vertical-align:middle;">
<p><span class="font3">Data=10</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">MRR</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.835646</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.445972</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.325829</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.788167</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.413667</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.284917</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.914167</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.469333</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.369333</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.808244</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.439192</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.276661</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.914167</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.587083</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.465833</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">RunTime (s)</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">4.580740</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.878300</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.413460</span></p></td></tr>
<tr><td rowspan="6" style="vertical-align:middle;">
<p><span class="font3">Data=20</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">MRR</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.837527</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.429016</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.312137</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.787766</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.393163</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.271144</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.897035</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.475840</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.380936</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.824419</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.408427</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.256593</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.898958</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.528849</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.414186</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">RunTime (s)</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">4.867190</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.930070</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.457500</span></p></td></tr>
<tr><td rowspan="6" style="vertical-align:middle;">
<p><span class="font3">Data=30</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">MRR</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.829875</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.423656</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.299837</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.795167</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.382833</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.255611</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.880833</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.467333</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.363444</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.816830</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.438579</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.264742</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.880833</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.547167</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.427444</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">RunTime (s)</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">4.988889</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.989840</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.473040</span></p></td></tr>
<tr><td rowspan="6" style="vertical-align:middle;">
<p><span class="font3">Data=40</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">MRR</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.814366</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.416073</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.308477</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.779024</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.376138</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.265282</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">P@5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.866088</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.462277</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.376469</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.792932</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.412638</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.260749</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">0.869687</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.534220</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.422969</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">RunTime (s)</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">5.109870</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">2.039970</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.504400</span></p></td></tr>
</table>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark36"></a><span class="font8" style="font-weight:bold;"><a name="bookmark37"></a>4. &nbsp;&nbsp;&nbsp;CONCLUSION</span></h2></li></ul>
<p><span class="font8">By incorporating the causalitydetection-based similarity into TFIDF model can improve the performance of paragraph extraction in answering why-questions. The proposed ontology-based paragraph extraction showed the significant improvement over the ontology-based method without causality detection. The proposed method shows the improvements regarding MRR (95%, 0.82-0.42), P@1</span></p>
<p><span class="font8">(105%, 0.78-0.38), P@5(91%, 0.88-0.46), Precision (95%, 0.80-0.41), and Recall (66%, 0.88-0.53).</span></p>
<h2><a name="bookmark38"></a><span class="font8" style="font-weight:bold;"><a name="bookmark39"></a>REFERENCES</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font8">[1] &nbsp;&nbsp;&nbsp;Baeza-Yates, R. and Ribeiro-Neto, B., 1999, </span><span class="font8" style="font-style:italic;">Modern Information Retrieval</span><span class="font8">, ACM Press, New York.</span></p></li>
<li>
<p><span class="font8">[2] &nbsp;&nbsp;&nbsp;Castells, P., Fernández, M. and Vallet, D., 2007, An adaptation of the vector</span></p></li></ul>
<p><span class="font8">space model for ontology-based information &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;retrieval, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font8" style="font-style:italic;">IEEE</span></p>
<p><span class="font8" style="font-style:italic;">Transactions on Knowledge and Data Engineering</span><span class="font8">, Vol. 19, No. 2, pp. 261– 272.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[3] &nbsp;&nbsp;&nbsp;Fernández, M., Cantador, I., López, V., Vallet, D., Castells, P. and Motta, E., 2011, &nbsp;&nbsp;&nbsp;&nbsp;Semantically &nbsp;&nbsp;&nbsp;&nbsp;enhanced</span></p></li></ul>
<p><span class="font8">Information Retrieval: An ontologybased approach, Web Semantics: Science, Services and Agents on the World Wide Web, Vol. 9, pp. 434–452.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[4] &nbsp;&nbsp;&nbsp;Han, K.-S., Song, Y.-I., and Rim, H.-C., 2006, Probabilistic model for definitional question answering, </span><span class="font8" style="font-style:italic;">In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</span><span class="font8">, pp. 212–219.</span></p></li>
<li>
<p><span class="font8">[5] &nbsp;&nbsp;&nbsp;Higashinaka, R. and Isozaki, H., 2008, Corpus-based question answering for why-questions. </span><span class="font8" style="font-style:italic;">In Proceedings of IJCNLP</span><span class="font8">, Hyderabad.</span></p></li>
<li>
<p><span class="font8">[6] &nbsp;&nbsp;&nbsp;Karyawati, A.A.I.N.E., Winarko, E., Azhari, &amp;&nbsp;Harjoko, A., &nbsp;2015,</span></p></li></ul>
<p><span class="font8">Ontology-based &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Why-Question</span></p>
<p><span class="font8">Analysis Using Lexico-Syntactic Patterns, </span><span class="font8" style="font-style:italic;">International Journal of Electrical and Computer Engineering (IJECE)</span><span class="font8">, Vol. 5, No. 2, pp. 318-332.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[7] &nbsp;&nbsp;&nbsp;Khoo, C. S.-G., &nbsp;1995, Automatic</span></p></li></ul>
<p><span class="font8">Identification of Causal Relations in Text and their use for Improving Precision in Information Retrieval, </span><span class="font8" style="font-style:italic;">Ph.D. Dissertation</span><span class="font8">, Information Transfer in the School of Information Studies, Syracuse University, Syracuse, NY.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[8] &nbsp;&nbsp;&nbsp;Khoo, C. S. G., Myaeng, S. H. and Oddy, R. N., 2001, Using Cause-Effect Relations in Text to Improve Information Retrieval Precision, </span><span class="font8" style="font-style:italic;">Journal of Information Processing and Management</span><span class="font8">, Vol. 37, pp. 119-145.</span></p></li>
<li>
<p><span class="font8">[9] &nbsp;&nbsp;&nbsp;Manning, C. D., Raghavan, P. and Schutze, H., 2008, Introduction to</span></p></li></ul>
<p><span class="font8">Information Retrieval, Cambridge University Press, New York.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[10] &nbsp;&nbsp;&nbsp;Mori, T., Sato, M. and Ishioroshi, M., 2008, Answering any class of Japanese non-factoid question by using the Web and example Q&amp;A pairs from a social Q&amp;A website, </span><span class="font8" style="font-style:italic;">In IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology</span><span class="font8">, Sydney, Australia.</span></p></li>
<li>
<p><span class="font8">[11] &nbsp;&nbsp;&nbsp;Nakakura, S. and Fukumoto, J., 2008, Question Answering System beyond Factoid Type Questions, </span><span class="font8" style="font-style:italic;">In the 23<sup>rd </sup>International Technical Conference Circuits/Systems (ITC-CSCC 2008)</span><span class="font8">, Yamaguchi, Japan.</span></p></li>
<li>
<p><span class="font8">[12] &nbsp;&nbsp;&nbsp;Oh, J.-H., Torisawa, K., Hashimoto, C., Kawada, T., De Saeger, S., Kazama, J., and Wan, Y., 2012, Why Question Answering using Sentiment Analysis and Word Classes, </span><span class="font8" style="font-style:italic;">In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language &nbsp;&nbsp;&nbsp;&nbsp;Processing &nbsp;&nbsp;&nbsp;&nbsp;and</span></p></li></ul>
<p><span class="font8" style="font-style:italic;">Computational Natural language Learning</span><span class="font8">, Jeju Island, Korea.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[13] &nbsp;&nbsp;&nbsp;Oh J.-H, Torisawa K, Hashimoto C, Sano M, Saeger SD, Ohtake K., 2013, Why-Question Answering using Intra-and Inter-Sentential Causal Relations, </span><span class="font8" style="font-style:italic;">In Proceeding of the 51st Annual Meeting of the Association for Computational Linguistics</span><span class="font8">, Bulgaria.</span></p></li>
<li>
<p><span class="font8">[14] &nbsp;&nbsp;&nbsp;Soricut, R. and Brill, E., &nbsp;2006,</span></p></li></ul>
<p><span class="font8">Automatic Question Answering Using the Web: Beyond the Factoid. </span><span class="font8" style="font-style:italic;">Journal of Information Retrieval - Special Issue on Web Information Retrieval</span><span class="font8">, Vol. 9, No. 2, pp. 191–206.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[15] &nbsp;&nbsp;&nbsp;Thom, J.A. and Scholer, F., 2007, A Comparison of Evaluation Measures Given How Users Perform on Search Tasks, </span><span class="font8" style="font-style:italic;">In Proceedings of the 12<sup>th </sup>Australasian Document Computing Symposium</span><span class="font8">, Melbourne, Australia.</span></p></li>
<li>
<p><span class="font8">[16] &nbsp;&nbsp;&nbsp;Verberne, S., 2006, Developing an</span></p></li></ul>
<p><span class="font8">Approach for Why-question answering, </span><span class="font8" style="font-style:italic;">In Conference Companion of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2006)</span><span class="font8">, Trento.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font8">[17] &nbsp;&nbsp;&nbsp;Verberne, S., Boves, L., Oostdijk, N., and Coppen, P., 2010, What is not in the bag of words for why-QA?, </span><span class="font8" style="font-style:italic;">Computational Linguistics</span><span class="font8">, Vol. 32, No. 2, 229–245.</span></p></li>
<li>
<p><span class="font8">[18] &nbsp;&nbsp;&nbsp;Verberne, S., Boves, L, and Kraaij, W., 2011, Bringing Why-QA to Web Search. </span><span class="font8" style="font-style:italic;">In Proceedings of ECIR ’11</span><span class="font8">, Dublin, Ireland.</span></p></li></ul>