---
layout: full_article
title: "Efforts of Performance Optimization: The Experiment on Ten Accounting Datasets"
author: "Zico Karya Saputra Domas, M. Rizkiawan, Roby Rakhmadi"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-89695 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-89695"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-89695"  
comments: true
---

<p><span class="font3" style="font-weight:bold;">LONTAR KOMPUTER VOL. 13, NO. 3 DECEMBER 2022</span></p>
<p><span class="font3" style="font-weight:bold;">DOI : 10.24843/LKJITI.2022.v13.i03.p04</span></p>
<p><span class="font3" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 158/E/KPT/2021</span></p>
<p><span class="font3" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font3" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font4" style="font-weight:bold;"><a name="bookmark1"></a>Efforts of Performance Optimization: The Experiment on Ten Accounting Datasets</span></h1>
<p><span class="font3">Zico Karya Saputra Domas<sup>a1</sup></span><span class="font3" style="font-weight:bold;">, </span><span class="font3">M. Rizkiawan<sup>a2</sup></span><span class="font3" style="font-weight:bold;">, </span><span class="font3">Roby Rakhmadi<sup>b3</sup></span></p>
<p><span class="font3"><sup>a</sup>Directorate General of Taxes of Indonesia</span></p>
<p><span class="font3">Jakarta, Indonesia </span><a href="mailto:11401190046.zicoksd@gmail.com"><span class="font3"><sup>1</sup>1401190046.zicoksd@gmail.com</span></a><span class="font3"> </span><span class="font1" style="font-variant:small-caps;">(c</span><span class="font1">orresponding author) </span><a href="mailto:2rizkiawan.edu@gmail.com"><span class="font3"><sup>2</sup>rizkiawan.edu@gmail.com</span></a></p>
<p><span class="font3"><sup>b</sup>International Relations Department of Lampung University Lampung, Indonesia</span></p>
<p><a href="mailto:3roby.rakhmadi007@fisip.unila.ac.id"><span class="font3"><sup>3</sup>roby.rakhmadi007@fisip.unila.ac.id</span></a></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font3" style="font-style:italic;">In the big data and digitalization era, fast-accurate decision-making has become a basic need, so data mining has a crucial role. The decision tree algorithm is quite commonly applied for classification functions, but performance level must always be evaluated for optimizing accuracy rate. Several optimization methods to accommodate these objectives include GA-bagging, PSO-bagging, forward selection, backward elimination, SMOTE, under-sampling, GA-Adaboost, and ABSMOTE-WIGFS. The results of the decision tree experiment on ten types of accountingfinance datasets used in this study obtained results with an average accuracy of 83.46%, an average precision of 65.64%, and an average AUC of 71.9%, while the majority of various optimizations are proven in improving the performance of decision tree algorithm where the application of ABSMOTE-WIGFS method is proven in providing the best rate with an average accuracy 87.71%, an average precision 87.09%, and an average AUC 84.87%, so it can be concluded that various optimization efforts are worth to be applied in case of accounting-finance themes for increasing the performance rate. Furthermore, the next research can prove these methods in other fields outside of accounting cases.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font3" style="font-style:italic;">Classification, Optimization, Accuracy, AUC, Precision</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font3" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font3">Today, the data mining approach has developed rapidly. It has already been applied in more expansive fields [1], some of which are [2] and [3] used a data mining approach to the agriculture sector [4] in the health sector, [5], [6], and [7] in the biology sector, and [8] who applied a data mining approach in the financial industry. One of the data mining algorithms, which is often used, is a decision tree. The decision tree classification algorithm has advantages in visualizing decision trees that easily interpret and handle discrete and numeric type attributes. However, the decision tree is also at risk of having weaknesses in entropy and Gini, so accuracy calculations are prone to be less than optimal when the dataset has an unequal class imbalance [9]. The class imbalance pattern is characterized by a case label being more unequal than others. For example, a label is represented by an extensive sample, while others are represented by a much smaller sample [10] [11].</span></p>
<p><span class="font3">The class imbalance obstacle can be overcome with various efforts, one of which is the sampling method [12], where [13] and [14] conducted optimization experiments by applying under-sampling and over-sampling methods. The sampling approach is basically training data manipulated to neutralize the distribution tendency of a label or class [14], [15].</span></p>
<p><span class="font3">Then, [16] conducted optimization experiments by applying genetic algorithm (GA)-bagging and particle swarm optimization (PSO)-bagging. Feature selection through GA and PSO methods is a pre-processing data activity to select feature subsets that minimize classifier prediction errors. Testing all possible combinations of features can be almost impossible, so the feature selection</span></p>
<p><span class="font3">techniques, both GA and PSO methods, try to find solutions in the range between sub-optimal and near-optimal areas by means of local search (not global search) throughout the process. Moreover, [17] observed that the </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> method could be applied to improve classifier performance. In addition, [18], [19], and [20] also conducted experiments by assessing comparisons among SMOTE, </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3">, and bagging techniques to increase the accuracy level of a prediction.</span></p>
<p><span class="font3">Furthermore, [21] and [22] observe that the forward selection method is feasible to be applied as an optimization effort, whereas [23] also experimented with using the backward elimination method as an optimization effort. The feature selection method, both forward selection and backward elimination, is based on a large feature space reduction, for example, by eliminating irrelevant attributes [23] to increase accuracy [23]. In this study, researchers will apply various optimization efforts, namely the GA-bagging method, PSO-bagging, forward selection, GA-</span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3">, SMOTE, backward elimination, and under-sampling, to ABSMOTE-WIGFS on ten types of datasets in the financial-accounting sector.</span></p>
<p><span class="font3">Through this research, the researcher hopes to contribute adequate scientific references for opening the focus of further research on the financial-accounting theme with the data mining approaches.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font3" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span><br><br><span class="font3" style="font-weight:bold;"><a name="bookmark6"></a>2.1. &nbsp;&nbsp;&nbsp;Data Mining</span></h2></li></ul>
<p><span class="font3">In essence, data mining is analyzing hidden data in an extensive database by combining statistical science and artificial intelligence so that a pattern or information previously unknown is found to make it easier to understand and provide benefits in future decisions making [24].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark7"></a><span class="font3" style="font-weight:bold;"><a name="bookmark8"></a>2.2. &nbsp;&nbsp;&nbsp;Literature Review of Optimization Efforts</span></h2></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font3">[25] &nbsp;&nbsp;&nbsp;compared over-sampling, under-sampling, and synthetic minority over-sampling (SMOTE) techniques to improve prediction accuracy on minority labels. The results showed that the SMOTE optimization method achieved the best performance with an accuracy rate of 90.24%. Then, [17] observed that the ordinary version of the classification algorithm on 20 datasets obtained from the NASA Metrics Data Program and Predictor Models in Software Engineering Repository was proven that most of them experienced an increase in the AUC score after being optimized by applying the SMOTE method. Statistical tests prove that there is a significant difference between most of the ordinary version classifier models and the SMOTE model. Then, [14] observed that the ordinary version classifier on the telecommunications industry customer churn dataset obtained from </span><a href="https://bigml.com/dashboard/source/55c69eca200d5a25a0005180"><span class="font3">https://bigml.com/dashboard/source/55c69eca200d5a25a0005180</span></a><span class="font3">, it was proven that the AUC level had increased after being optimized by applying the over-sampling method that combined with </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> technique from 83.8% to 85.6%. Furthermore, [26] observed that the ordinary version classifier on the protein compound interaction prediction dataset [27] proved that the AUC level had increased after being optimized by the application of the Synthetic Minority Oversampling Technique (SMOTE) method from 50.3% to 64.9%. [28] also observed that the ordinary version classifier in the car evolution dataset taken from the UCI machine learning repository has proven that the average AUC level has increased by 9.97% after being optimized by the application of the SMOTE method.</span></p></li></ul>
<p><span class="font3">Meanwhile, [29] observed that the ordinary version of the classification algorithm on nine datasets obtained from the NASA metric data repository proved that most of them experienced an increase after being optimized by applying the GA-bagging method so that the AUC level that did not increase was only one of the nine datasets. Statistical tests prove that there is a significant difference between the ordinary classifier model and the GA-bagging model. Then, [30] observed that ten ordinary version classification algorithms on nine datasets obtained from the NASA metric data repository proved that most of them experienced an increase in the AUC score after being optimized by applying the GA-bagging method. Statistical tests prove that there is a significant difference between most of the ordinary version classifier models and the GA-bagging model. Statistical tests prove a significant difference between most ordinary classifier models and the GA-bagging and PSO-bagging models. In contrast, statistical tests prove no significant difference between eight out of ten GA-bagging and PSO-bagging models. Then, [31] observed that the</span></p>
<p><span class="font3">ordinary version classifier in the banking marketing dataset obtained from the UCI Machine Learning Repository proved that the AUC level had increased after being optimized by applying the GA (genetic algorithm) method from 66.7% to 83.46%. Then, [32] observed that the ordinary version classifier in the diabetes mellitus prediction dataset proved that the AUC level had increased after being optimized by applying the Particle Swarm Optimization (PSO) method from 75.8% to 76.5%. [33] Also, the ordinary version of the classifier in the high school selection dataset for students of SMP Islam Al-Hikmah Pondok Cabe proved that the accuracy rate had increased by 7.36% after the application's optimization of the GA (genetic algorithm) method. Furthermore, [34] and [35] also observed that the Particle Swarm Optimization (PSO) technique was proven to produce a better level of accuracy.</span></p>
<p><span class="font3">In addition, [22] observed that the ordinary version classifier in the heart disease diagnosis dataset proved that the level of accuracy had increased after being optimized with the application of the forward selection method from 73.44% to 78.66%. Then, [23] observed that the ordinary version classifier in the Movie Review Polarity V2.0 dataset [36], it was proven that the AUC level had increased after being optimized with the application of the forward selection method from 71.26% to 76.2 %. Likewise, with the implementation of backward elimination, it is proven that the accuracy rate has increased from 75.2% to 78.66%. Then, [37] observed that the ordinary version classifier in two datasets, churn [38] and telecom [39], it was proven that the AUC level had increased after being optimized by applying the forward selection-weighted information gain method, which is combined with bootstrapping technique. [21] also observed that the ordinary version of the classifier in the graduation dataset of the Faculty of Computer Science UNAKI Semarang students proved that the level of accuracy had increased after being optimized with the application of the forward selection method from 90.95% to 97.14%.</span></p>
<p><span class="font3">Regarding the </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> method, [40] observed that the classification algorithm produced an AUC level (area under curves) for predicting student graduation of 0.864, which was then optimized using </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> so that the AUC level increased to 0.951. Then, [41] observed that the ordinary version classifier in the restaurant review dataset located in New York, it was proven that the AUC level had increased after being optimized by applying the </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> method combined with the information gain feature selection technique from 50% to 88.7%. [42] Also, the classification algorithm resulted in an AUC level of for predicting heart disease is 0.957, which was then optimized using </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> to increase the AUC level to 0.982.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark9"></a><span class="font3" style="font-weight:bold;"><a name="bookmark10"></a>2.3. &nbsp;&nbsp;&nbsp;Dataset and Research Framework</span></h2></li></ul>
<p><span class="font3">In this study, the researchers applied a decision tree classification algorithm combined with various optimization methods to compare with the decision tree algorithm without the optimization method. The ten of accounting-finance datasets, which are the basis of the research, can be broken down into datasets that are publicly accessible and that cannot be publicly accessible, namely the credit card default dataset for banking customers [43], subscribing term deposits to prospective banking customers [44], lack of transparency in disclosing anti-corruption information on private sector corporations in Indonesia [45], indications of manipulation of financial statements using the Beneish Score on state-owned companies in Indonesia [46], credit approvals for banks [47], South German credit [48], banknote authentication [49], audit risk [50] for Indian companies, census of income [51], and bankruptcy dataset on Polish companies [52]. Thus, as presented in table I, the researcher utilized eight public access datasets and two non-public access datasets. After optimization, all datasets will undergo a data training model and then data testing to ensure whether.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 1. </span><span class="font3">Dataset Type</span></p>
<p><span class="font2">Dataset Name &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Access &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data Volume &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Label &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Label</span></p>
<p><span class="font2">Composition 0 &nbsp;&nbsp;&nbsp;Composition 1</span></p>
<p><span class="font2">Default of credit card &nbsp;&nbsp;Public (UCI machine &nbsp;5.000 rows &nbsp;&nbsp;&nbsp;77,88% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;22,12%</span></p>
<p><span class="font2">learning repository)</span></p>
<p><span class="font2">Subscribing term &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Public (UCI machine &nbsp;5.000 rows &nbsp;&nbsp;&nbsp;88% &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12%</span></p>
<p><span class="font2">deposit &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learning repository)</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font2">Dataset Name</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Access</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Data Volume</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Label</span></p>
<p><span class="font2">Composition 0</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Label</span></p>
<p><span class="font2">Composition 1</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Lack of anticorruption transparency</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Non-Public</span></p></td><td style="vertical-align:top;">
<p><span class="font2">141 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">52,48%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">47,52%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Beneish M-score fraud</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Non-Public</span></p></td><td style="vertical-align:top;">
<p><span class="font2">105 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">44,76%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">55,24%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Credit card approval</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Public (UCI machine learning repository)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">690 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">55,5%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">44,5%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">South German credit</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Public (UCI machine learning repository)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">1.000 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">70%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">30%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Banknotes authentication</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Public (UCI machine learning repository)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">1.372 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">55,54%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">44,46%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Audit Risk</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Public (UCI machine learning repository)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">776 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">60,7%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">39,3%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Census of income</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Public (UCI machine learning repository)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">5.000 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">75,92%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">24,08%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Bankruptcy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Public (UCI machine learning repository)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">5.000 rows</span></p></td><td style="vertical-align:top;">
<p><span class="font2">94,74%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">5,26%</span></p></td></tr>
</table>
<p><span class="font3">Based on table 1, datasets that have a data volume of more than 5,000 rows will be trimmed randomly to 5,000 rows while maintaining a proportional data structure, namely the percentage of majority labels and the percentage of minority labels so that the dataset used by the researcher remains as representative as the original version. This pruning was done because the rapidminer 9.9 application used by the researchers was an unpaid version, so it was constrained by the maximum number of limitations related to the volume of data that could be processed. The preprocessing stage, if the original dataset has a missing value, the researcher will apply the replacement with the average value. The replacement technique with the average value is carried out because the researcher believes that the replacement with the average value is still representative of the original version with the condition that the number of missing value attributes is not proportional to the total number of data attributes in a dataset. In practice, there are only three datasets out of 10 datasets that have missing values where the number of missing value attributes in a dataset is not proportional to the total number of data attributes, so the average value technique is feasible to apply. Then the data that has gone through the cleaning process is ready to be sorted into a dataset for training and testing purposes. After that, it is processed using nine types of classifiers, namely the usual version of the decision tree algorithm and eight optimization methods, namely GA-bagging, PSO-bagging, forward selection, GA-</span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3">, SMOTE, backward elimination, under-sampling, ABSMOTE-WIGFS. Then in the next step, a validation process is carried out using 10-fold cross-validation so that the performance aspects to be observed can be measured, namely the accuracy, precision, and AUC level.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark11"></a><span class="font3" style="font-weight:bold;"><a name="bookmark12"></a>2.4. &nbsp;&nbsp;&nbsp;Genetic Algorithm-Bagging (GA-B) and Genetic Algorithm-</span><span class="font3" style="font-weight:bold;font-style:italic;">Adaboost</span><span class="font3" style="font-weight:bold;"> (GA-A)</span></h2></li></ul>
<p><span class="font3">Genetic algorithm (GA) is an optimization technique analogous to the principles of genetics and natural selection based on Charles Darwin's theory of evolution [53]. The rule that the stronger individual is likely to be the winner in a competitive environment can be analogized as the optimal solution can be obtained or represented in the final winner of the genetic game [31]. GA works with a population of individuals, denoted by a fitness value, which will be used to find the best solution to the problem. In the end, the most appropriate solution will be obtained from the existing problems. Then, the bagging technique has the potential to be superior to the boosting technique when it comes to environments containing noise data because boosting is more about trying to build a model to classify noise data correctly [16].</span></p>
<p><span class="font3">Meanwhile, the </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> method gives different weights to the training data distribution in each iteration. Each boosting iteration adds weight to the wrong classification variety and decreases</span></p>
<p><span class="font3">the weight to the correct classification variety to effectively change training data distribution [54]. In other words, </span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3"> builds a robust classifier by combining several weak classifiers [14].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark13"></a><span class="font3" style="font-weight:bold;"><a name="bookmark14"></a>2.5. &nbsp;&nbsp;&nbsp;Particle Swarm Optimization-Bagging (PSO-B)</span></h2></li></ul>
<p><span class="font3">Particle swarm optimization (PSO) is a population search method that is analogous to the social behavior of animal colony organisms such as termites, bees, birds, or fish, using a population (swarm) of individuals (particles) that is updated from iterations [32], [55]. That is, the rule that if a termite finds a food source through the right (optimal) path, then the rest of the members of the other termite group will also take the same way even though the location of the termites in the group is not close to each other, can be analogized as an attempt to find the optimal solution then each -each particle moves towards the best individual experience position (</span><span class="font3" style="font-style:italic;">p-best</span><span class="font3">) and towards the best global position (</span><span class="font3" style="font-style:italic;">g-best</span><span class="font3">) [55], [56]. Then, the bagging technique has the potential to be superior to the boosting approach when it comes to environments containing noise data because boosting is more about trying to build a model to classify noise data correctly [29].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark15"></a><span class="font3" style="font-weight:bold;"><a name="bookmark16"></a>2.6. &nbsp;&nbsp;&nbsp;Forward Selection (FS)</span></h2></li></ul>
<p><span class="font3">Feature selection is a technique to determine the most relevant attribute in the dataset by selecting the correct subset of the original attributes because not all attributes may be relevant to the problem; even some of these attributes can interfere with the impact at reduced accuracy. In the forward selection method (FS), modeling starts with zero variables (empty model) then the variables are entered one by one until specific criteria are met [21], [22].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark17"></a><span class="font3" style="font-weight:bold;"><a name="bookmark18"></a>2.7. &nbsp;&nbsp;&nbsp;Backward Elimination (BE)</span></h2></li></ul>
<p><span class="font3">Feature selection is a technique to determine the most relevant attribute in the dataset by selecting the correct subset of the original attributes. Not all attributes may be relevant to the problem. Even some of these attributes can interfere with the impact at reduced accuracy. In the backward elimination method (BE), the modeling starts with the complete model (full model), and then the variables are reduced one by one until specific criteria are met.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark19"></a><span class="font3" style="font-weight:bold;"><a name="bookmark20"></a>2.8. &nbsp;&nbsp;&nbsp;Synthetic Minority Oversampling Technique (SMOTE)</span></h2></li></ul>
<p><span class="font3">The synthetic minority oversampling technique (SMOTE) selects data from minority labels synthetically. It then adds it to the training data so that the minority label data is equal to the majority label data [15].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font3" style="font-weight:bold;"><a name="bookmark22"></a>2.9. &nbsp;&nbsp;&nbsp;Under-sampling (US)</span></h2></li></ul>
<p><span class="font3">Under-sampling (US) selects the majority label data at random and removes it from training data so that the number of majority label data is the same as that of minority label data [57].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark23"></a><span class="font3" style="font-weight:bold;"><a name="bookmark24"></a>2.10. &nbsp;&nbsp;&nbsp;ABSMOTE-WIGFS</span></h2></li></ul>
<p><span class="font3">The ABSMOTE-WIGFS method is a combination method that refers to the substance of the experimental ideas of [37], [41] so that researchers experiment by combining data level</span></p>
<p><span class="font3">approaches (</span><span class="font3" style="font-style:italic;">Adaboost</span><span class="font3">, Bootstrap, SMOTE), filtering approaches (Weight Information Gain), to the wrapping approach (forward selection) in an integrated technique as shown in figure 1, 2, and Figure 3 meanwhile bootstrapping is a resampling method that has been widely applied and allows the creation of more realistic models [37]. That is, bootstrap resamples with a replacement where the data, which has been selected in an experiment, can still be chosen again in the next experiment [37].</span></p><img src="https://jurnal.harianregional.com/media/89695-1.jpg" alt="" style="width:401pt;height:63pt;">
<p><span class="font3" style="font-weight:bold;">Figure 1. </span><span class="font3">The process of the ABSMOTE-WIGFS method by version 9.9 of the </span><span class="font3" style="font-style:italic;">Rapidminer </span><span class="font3">application</span></p>
<div><img src="https://jurnal.harianregional.com/media/89695-2.jpg" alt="" style="width:250pt;height:206pt;">
<p><span class="font3" style="font-weight:bold;">Figure 2. </span><span class="font3">Bootstrap resampling parameters for small data volume dataset by the </span><span class="font3" style="font-style:italic;">Rapidminer</span><span class="font3"> 9.9</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/89695-3.jpg" alt="" style="width:207pt;height:116pt;">
<p><span class="font3" style="font-weight:bold;">Figure 3. </span><span class="font3">ABSMOTE-WIGFS method process for small data volume datasets by the </span><span class="font3" style="font-style:italic;">Rapidminer</span><span class="font3"> 9.9</span></p>
</div><br clear="all">
<p><span class="font3">Based on Figure 4, a resampling of 10,000 records was implemented because resampling with a ratio of 1.3 times the input records has the potential to exceed the data processing capacity of the </span><span class="font3" style="font-style:italic;">Rapidminer</span><span class="font3"> application, which has constraints on the maximum data volume limit. If the resampling selection on the bootstrap parameter exceeds the capacity, it will impact the potential for decreasing accuracy by up to 30%.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark25"></a><span class="font3" style="font-weight:bold;"><a name="bookmark26"></a>2.11. &nbsp;&nbsp;&nbsp;Ten-Folds Cross Validation and Model Evaluation</span></h2></li></ul>
<p><span class="font3">Cross-validation is a method that divides the dataset into two parts, where one part acts as training data while the other part acts as testing data. Some studies divide the data into ten parts, 90% is applied as training data, and the additional 10% is applied as testing data. This process is repeated up to 10 times, also known as ten-fold cross-validation. Researchers widely use this technique because it produces a more stable algorithm performance [24].</span></p>
<p><span class="font3">According to [58], the four fundamental matrices in evaluating the performance of the classification algorithm consist of True Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN). Then, the level of accuracy is defined as the ratio of the total number of correctly predicted observations, sensitivity is defined as the proportion of the positive observations correctly predicted as positive, and specificity is defined as how accurately the negative observations are correctly predicted as negative, so the Area Under Curve (AUC) representants the level of separability measurement that a model can distinguish among labels or classes.</span></p>
<p><span class="font3">Accuracy = (TP+TN)/ (TP+FP+TN+FN)</span></p>
<p><span class="font3">AUC &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= 1/2 * (Sensitivity + Specificity)</span></p>
<p><span class="font3">Furthermore, research [59] explains that the Area Under Curve (AUC) performance can be classified into five categories, namely:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">1. &nbsp;&nbsp;&nbsp;0.90 – 1.00 = </span><span class="font3" style="font-style:italic;">Excellent Classification</span></p></li>
<li>
<p><span class="font3">2. &nbsp;&nbsp;&nbsp;0.80 – 0.90 = </span><span class="font3" style="font-style:italic;">Good Classification</span></p></li>
<li>
<p><span class="font3">3. &nbsp;&nbsp;&nbsp;0.70 – 0.80 = </span><span class="font3" style="font-style:italic;">Fair Classification</span></p></li>
<li>
<p><span class="font3">4. &nbsp;&nbsp;&nbsp;0.60 – 0.70 = </span><span class="font3" style="font-style:italic;">Poor Classification</span></p></li>
<li>
<p><span class="font3">5. &nbsp;&nbsp;&nbsp;0.50 – 0.60 = </span><span class="font3" style="font-style:italic;">Failure</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark27"></a><span class="font3" style="font-weight:bold;"><a name="bookmark28"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span><br><br><span class="font3" style="font-weight:bold;"><a name="bookmark29"></a>3.1. &nbsp;&nbsp;&nbsp;Recapitulation of Comparison</span></h2></li></ul>
<p><span class="font3">The optimization variance experiment was applied to ten datasets using the rapidminer application version 9.9. Ten-fold stratified cross-validation is applied to validate the algorithm model, repeated ten times on the entire dataset, where each repetition uses different random data [60]. After the ten-fold stratified cross-validation is completed, the results of the ten-fold test for 90% of the training data are combined. The pattern of the training data results is automatically applied to 10% of the testing data so that the performance evaluation results of the eight optimization experiments can be measured objectively, as presented in table 2, table 3, and table 4.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 2. </span><span class="font3">Recapitulation of the evaluation of the comparison of the level of accuracy</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Dataset</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">D-tree</span></p></td><td colspan="8" style="vertical-align:bottom;">
<p><span class="font2">Optimization results of accuracy</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">GA-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">PSO-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">FS</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">GA-A</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">SMOTE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">BE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">US</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">ABSMOTE-WIGFS</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Default of credit card</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">77,98%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">80,60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">78,46%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">80,66%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">78,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50,69%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">78,44%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">49,82%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">71,52%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Subscribing term deposit Lack of anti-</span></p></td><td style="vertical-align:top;">
<p><span class="font2">92,36%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">97,16%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">97,16%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">97,10%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">97,18%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">93,93%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">94,84%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">89,92%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">96,23%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">corruption transparency</span></p></td><td style="vertical-align:top;">
<p><span class="font2">73,76%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">77,30%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">77,30%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">73,05%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">77,30%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">73,65%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">75,89%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">76,12%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">77,60%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Beneish M-score fraud</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">62,86%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">73,33%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">74,29%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">70,48%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">77,14%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">69,83%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">69,52%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">56,38%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">89,40%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Credit card approval</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">80,43%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">88,12%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">86,67%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">86,96%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">87,39%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">84,46%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">84,49%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">83,71%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">90,56%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">German credit</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">70,70%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">74,20%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">75,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">72,30%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">74,7%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">70,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">72,2%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">69,33%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">87,97%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Banknotes authentication</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,81%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">98,98%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,05%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">97,96%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,78%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,31%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">97,96%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,75%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Audit risk</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Census of income</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">83,96%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">85,46%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">85,26%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">85,38%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">85,36%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">81,64%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">85,26%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">79,98%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">86,53%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Bankruptcy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">94,72%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">94,76%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">94,76%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">94,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">94,76%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">62,11%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">94,74%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">59,89%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">77,56%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Average</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">83,46%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">86,99%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">86,85%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">85,87%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">87,21%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">78,41%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">85,33%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">76,20%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">87,71%</span></p></td></tr>
</table>
<p><span class="font2" style="font-weight:bold;">accuracy</span></p>
<p><span class="font3">Bold: improved over the regular version of the decision tree; *: best performance</span></p>
<p><span class="font3">Based on table 2, when viewed from the ten types of datasets, most optimization methods are proven to increase the average level of accuracy, which is better than the standard version of the decision tree algorithm. Only the SMOTE and under-sampling methods reduce the average accuracy level. Then, it can be concluded that the ABSMOTE-WIGFS method is proven to increase the average level of accuracy with the best performance among the seven other optimization methods, which is a score of 87.71%.</span></p>
<table border="1">
<tr><td colspan="10" style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">Table 3. </span><span class="font3">Recapitulation of the evaluation of the comparison of the level of precision</span></p></td></tr>
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Dataset</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">D-tree</span></p></td><td colspan="8" style="vertical-align:bottom;">
<p><span class="font2">Optimization results of precision</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">GA-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">PSO-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">FS</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">GA-A</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">SMOTE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">BE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">US</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">ABSMOTE-WIGFS</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Default of credit card</span></p></td><td style="vertical-align:top;">
<p><span class="font2">55,32%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">64,47%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">63,81%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">63,55%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">77,19%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">50,35%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">73,33%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">49,91%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">79,11%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Subscribing term deposit</span></p></td><td style="vertical-align:top;">
<p><span class="font2">63,1%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">99,14%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">99,35%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">98,92%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">99,14%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">91,3%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">77,06%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">88,82%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">95,2%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Lack of anti-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70,27%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">72,15%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">71,6%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">75,44%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">74,29%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">70,13%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">71,79%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">71,62%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">78,5%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">corruption transparency Beneish M-score fraud</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">64,62%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">78,95%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">74,55%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">76,36%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">81,82%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">64,06%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">75%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">92,11%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Credit card approval</span></p></td><td style="vertical-align:top;">
<p><span class="font2">86,42%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">93,2%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">90,28%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">91,27%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">90,81%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">90,71%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">85,98%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">87,95%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">95,53%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">German credit</span></p></td><td style="vertical-align:top;">
<p><span class="font2">50,96%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">60,61%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">60,4%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">59,74%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">60,5%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">63,47%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">64,63%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">60,25%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">83,13%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Banknotes authentication</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">98,49%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">98,53%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">98,69%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,7%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,51%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">98,03%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">98,19%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,87%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,69%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Audit risk</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Census of income</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">79,39%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">78,56%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">77,9%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">81,83%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">78,37%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">76,22%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">80,21%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">74,79%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">83,63%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Bankruptcy</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">66,67%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">80%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">66,67%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">56,98%</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">66,05%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">78,19%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Average</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">65,64%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">83,41%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">80,12%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">81,90%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">81,89%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">75,94%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">71,85%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">75,20%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">87,09%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">accuracy</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">*</span></p></td></tr>
<tr><td colspan="6" style="vertical-align:bottom;">
<p><span class="font3">Bold: improved over the regular version of the decision tree;</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font3">*: best performance</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td></tr>
</table>
<p><span class="font3">Based on table 3, when viewed from the ten types of datasets, all optimization methods are proven to increase the average level of precision, which is better than the standard version of the decision tree algorithm. Then, it can be concluded that the ABSMOTE-WIGFS method is proven to increase the average level of precision with the best performance among the seven other optimization methods, which is a score of 87.09%.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 4. </span><span class="font3">Recapitulation of the comparative evaluation of AUC level</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">Dataset</span></p></td><td rowspan="2" style="vertical-align:middle;">
<p><span class="font2">D-tree</span></p></td><td colspan="8" style="vertical-align:bottom;">
<p><span class="font2">Optimization results of AUC</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">GA-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">PSO-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">FS</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">GA-A</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">SMOTE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">BE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">US</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">ABSMOTE-WIGFS</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Default of credit card</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">51,70%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">71,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">66,30%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">69,70%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">51,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50,70%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">52,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">49,70%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">73,70%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Subscribing term deposit</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">92%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">89,90%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">89,70%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">90,30%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">88,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">97,40%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">93,30%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">90,90%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">98,10%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Lack of anticorruption transparency Beneish M-score fraud</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">77,20%</span></p>
<p><span class="font2">62,00%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">81,50%</span></p>
<p><span class="font2" style="font-weight:bold;">76,50%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">80,50%</span></p>
<p><span class="font2" style="font-weight:bold;">75,50%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">72,90%</span></p>
<p><span class="font2" style="font-weight:bold;">70,4%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">80,00%</span></p>
<p><span class="font2" style="font-weight:bold;">76%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">76,40%</span></p>
<p><span class="font2" style="font-weight:bold;">72,70%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">78,00%</span></p>
<p><span class="font2" style="font-weight:bold;">66,60%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">79,20%</span></p>
<p><span class="font2">49,80%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">78,70%</span></p>
<p><span class="font2" style="font-weight:bold;">92,40%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Credit card approval</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">85,60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">92,60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">91,40%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">91,30%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">90,40%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">88,20%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">87,20%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">86,10%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">93,50%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">German credit</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">70,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">75%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">75,20%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">62,6%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">67,7%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">73,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">60,20%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">72,10%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">93,80%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Banknotes authentication</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,40%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">98,4%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,90%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">97,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">98,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">99,90%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Audit risk</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Census of income</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">82,40%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">88,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">87,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">80,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">75,20%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">86%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">83,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">85,80%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">90,80%</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Bankruptcy</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">51,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">51,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">53,60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">50,50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">62,20%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">50%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">59,60%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">77,80%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Average accuracy</span></p></td><td style="vertical-align:top;">
<p><span class="font2">71,90%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">82,67%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">81,75%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">73,97%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">73,00%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">75,49%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">71,98%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">72,00%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">84,87%</span></p>
<p><span class="font2" style="font-weight:bold;">*</span></p></td></tr>
</table>
<p><span class="font3">Bold: improved over the regular version of the decision tree; *: best performance</span></p>
<p><span class="font3">Based on table 4, when viewed from the ten types of datasets, all optimization methods are proven to increase the average level of AUC (area under the curve), which is better than the standard version of the decision tree algorithm. Then, it can be concluded that the ABSMOTE-WIGFS method can increase the average AUC level with the best performance among the seven other optimization methods, which is a score of 84.87%.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark30"></a><span class="font3" style="font-weight:bold;"><a name="bookmark31"></a>3.2. &nbsp;&nbsp;&nbsp;Results of </span><span class="font3" style="font-weight:bold;font-style:italic;">T-test</span></h2></li></ul>
<p><span class="font3">Statistically, the standard version of the decision tree cannot be concluded as a different cluster from the decision tree algorithm based on the optimization method. However, the various optimization efforts briefly appear to have a better level of performance based on experiments on these ten datasets. Thus, it is necessary to carry out a different T-test to know the statistical differences level as presented in table 5.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 5. </span><span class="font3">Test results of the </span><span class="font3" style="font-style:italic;">T-test</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font2">Dataset</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">GA-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">PSO-B</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">FS</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">GA-A</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">SMOTE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">BE</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">US</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">ABSMOTE</span></p>
<p><span class="font2">-WIGFS</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Default of credit card</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,257</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,423</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,497</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Subscribing deposit</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,044</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,034</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,035</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Lack of anticorruption transparency</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,608</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,552</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,927</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,546</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,994</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,716</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,68</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,52</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Beneish M-score fraud</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,071</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,081</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,262</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,022</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,235</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,298</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,309</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Credit &nbsp;&nbsp;&nbsp;&nbsp;card</span></p>
<p><span class="font2">approval</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,004</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,004</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,028</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,04</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,108</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">South &nbsp;German</span></p>
<p><span class="font2">credit</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,106</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,012</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,265</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,012</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,897</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,356</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,507</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Banknotes authentication</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,031</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,02</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,782</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,481</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,808</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,132</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Audit risk</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">--</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">--</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">--</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">--</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">--</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">--</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">--</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">--</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Census &nbsp;&nbsp;&nbsp;&nbsp;of</span></p>
<p><span class="font2">income</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,045</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,022</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,034</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,036</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,002</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,072</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Bankruptcy</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,449</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,591</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,255</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,449</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2">0,714</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0,000</span></p></td></tr>
</table>
<p><span class="font3">Bold: statistically significant; --: can be interpreted as insignificant</span></p>
<p><span class="font3">Based on table 5 for four datasets with a large data volume pattern, namely 5,000 records, the majority of the alpha values are less than 0.05, so it can be concluded that statistically, there is a significant difference between the default of the decision tree algorithm and the majority of various optimization efforts. However, for the six datasets with a small data volume pattern which is below 1,372 records, the majority of the alpha values are above 0.05, so it can be concluded that statistically, there is no significant difference between the default of the decision tree and the majority of the various optimization efforts. This means that the majority of optimization methods can increase the performance level of the decision tree from the perspective of predictive accuracy of the classification function in finance research. Still, statistically, the various optimization methods sometimes provide significant differences with the decision tree for datasets with extensive data volume input (in this study, it means 5,000 rows and above) and sometimes do not provide significant differences with the decision tree for datasets with small data volume input (in this study it means 1,372 rows and below). This is understandable because a dataset with a small input data volume will affect the quality of the training data representation and data testing. Then, the results of the performance evaluation on the average AUC level for the ABSMOTE-WIGFS method of 84.87% so that it can be concluded that it is in the good classifier category [59]. However, in one out of ten datasets, namely the audit risk dataset with the data volume of 776 records, an anomaly occurs casuistically that the ABSMOTE-WIGFS method fails to improve the AUC performance on the decision tree classification while ABSMOTE-WIGFS on the other nine datasets always proves successful in improving the AUC performance.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark32"></a><span class="font3" style="font-weight:bold;"><a name="bookmark33"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font3">Based on the experiment results, this study concludes that most optimization efforts for the classification function algorithm can improve the performance level of experiments on ten types of datasets as a whole. However, based on a statistical perspective, the majority of optimization efforts have no significant difference for the classification function algorithm on datasets with low data volume (1,372 records and below), while the majority of optimization efforts have a considerable difference for classification function algorithms on datasets with large data volumes (5,000 records and above). Thus, if the decision tree performance is still unsatisfactory, then the various optimization methods, especially the ABSMOTE-WIGFS method, are worth applying to the financial-accounting problem because the ABSMOTE-WIGFS is proven to improve the best performance compared to the other seven optimization methods.</span></p>
<p><span class="font3">Then, the researcher also stated two main limitations of this study. First, the research dataset does not use all of the original versions of public data records because the unpaid version of the </span><span class="font3" style="font-style:italic;">Rapidminer</span><span class="font3"> 9.9 application has a maximum limit on data processing. Consequently, it is necessary to trim the number of data records so that this limitation of trimming can add hidden and random loads to each test item. Second, this study also uses two datasets that are not accessible publicly, so the quality of these characteristics datasets has the limitation that they still have not been validated publicly.</span></p>
<p><span class="font3">The author also recommends suggestions for further research related to data mining. Further research can apply various optimization efforts to the classification function algorithm, limited to decision tree algorithms and logistic regression algorithms, k-nn, naive Bayes, and other classification algorithms. Finally, the author tries to provide input to stakeholders in the fields of management, economics, finance, accounting, and business, to apply various optimization efforts as one of the considerations in the decision-making process to be more accurate based on scientifically proven data. Besides, further research can also prove these methods in many other fields outside accounting cases.</span></p>
<h2><a name="bookmark34"></a><span class="font3" style="font-weight:bold;"><a name="bookmark35"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;J. Liu </span><span class="font3" style="font-style:italic;">et al.</span><span class="font3">, &quot;Artificial intelligence in the 21st century,&quot; </span><span class="font3" style="font-style:italic;">IEEE Access</span><span class="font3">, vol. 6, pp. 34403–34421, 2018, doi: 10.1109/ACCESS.2018.2819688.</span></p></li>
<li>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;S. Tangwannawit and P. Tangwannawit, &quot;An optimization clustering and classification based on artificial intelligence approach for internet of things in agriculture,&quot; </span><span class="font3" style="font-style:italic;">IAES International Journal of Artificial Intelligence &nbsp;(IJ-AI)</span><span class="font3">, vol. 11, no. &nbsp;1, p. 201, &nbsp;March 2022, doi:</span></p></li></ul>
<p><span class="font3">10.11591/ijai.v11.i1.pp201-209.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;A. A. J. V. Priyangka and I. M. S. Kumara, &quot;Classification Of Rice Plant Diseases Using the Convolutional Neural Network Method,&quot; </span><span class="font3" style="font-style:italic;">Lontar Komputer: Jurnal Ilmiah Teknologi Informasi</span><span class="font3">, vol. 12, no. 2, p. 123, August 2021, doi: 10.24843/LKJITI.2021.v12.i02.p06.</span></p></li>
<li>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;M. Panda, D. P. Mishra, S. M. Patro, and S. R. Salkuti, &quot;Prediction of diabetes disease using machine learning algorithms,&quot; </span><span class="font3" style="font-style:italic;">IAES International Journal of Artificial Intelligence (IJ-AI)</span><span class="font3">, vol. 11, no. 1, p. 284, March 2022, doi: 10.11591/ijai.v11.i1.pp284-290.</span></p></li>
<li>
<p><span class="font3">[5] &nbsp;&nbsp;&nbsp;Z. E. Fitri, L. N. Sahenda, P. S. D. Puspitasari, P. Destarianto, D. L. Rukmi, and A. M. N. Imron, “The The Classification of Acute Respiratory Infection (ARI) Bacteria Based on K-Nearest Neighbor,” </span><span class="font3" style="font-style:italic;">Lontar Komputer: Jurnal Ilmiah Teknologi Informasi</span><span class="font3">, vol. 12, no. 2, p. 91, 2021, doi: 10.24843/lkjiti.2021.v12.i02.p03.</span></p></li>
<li>
<p><span class="font3">[6] &nbsp;&nbsp;&nbsp;I. M. A. S. Widiatmika, I. N. Piarsa, and A. F. Syafiandini, “Recognition of The Baby Footprint Characteristics Using Wavelet Method and K-Nearest Neighbor (K-NN),” </span><span class="font3" style="font-style:italic;">Lontar Komputer: Jurnal Ilmiah Teknologi &nbsp;&nbsp;Informasi</span><span class="font3">, &nbsp;&nbsp;vol. 12, no. 1, p. 41, &nbsp;&nbsp;2021, doi:</span></p></li></ul>
<p><span class="font3">10.24843/lkjiti.2021.v12.i01.p05.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[7] &nbsp;&nbsp;&nbsp;P. A. W. Santiary, I. K. Swardika, I. B. I. Purnama, I. W. R. Ardana, I. N. K. Wardana, and D. A. I. C. Dewi, &quot;Labeling of an intra-class variation object in deep learning classification,&quot; </span><span class="font3" style="font-style:italic;">IAES International Journal of Artificial Intelligence (IJ-AI)</span><span class="font3">, vol. 11, no. 1, p. 179, March 2022, doi: 10.11591/ijai.v11.i1.pp179-188.</span></p></li>
<li>
<p><span class="font3">[8] &nbsp;&nbsp;&nbsp;M. Sánchez, V. Olmedo, C. Narvaez, M. Hernández, and L. Urquiza-Aguiar, &quot;Generation of a Synthetic Dataset for the Study of Fraud through Deep Learning Techniques,&quot; </span><span class="font3" style="font-style:italic;">International Journal on Advanced Science, Engineering and Information Technology</span><span class="font3">, vol. 11, no. 6, p.</span></p></li></ul>
<p><span class="font3">2534, December 2021, doi: 10.18517/ijaseit.11.6.14345.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[9] &nbsp;&nbsp;&nbsp;D. A. Cieslak, T. R. Hoens, N. V. Chawla, and W. P. Kegelmeyer, &quot;Hellinger distance decision trees are robust and skew-insensitive,&quot; </span><span class="font3" style="font-style:italic;">Data Mining and Knowledge Discovery</span><span class="font3">, vol. 24, no. 1, pp. 136–158, January 2012, doi: 10.1007/s10618-011-0222-1.</span></p></li>
<li>
<p><span class="font3">[10] &nbsp;&nbsp;&nbsp;Y. Sun, M. S. Kamel, A. K. C. Wong, and Y. Wang, &quot;Cost-sensitive boosting for classification of imbalanced data,&quot; </span><span class="font3" style="font-style:italic;">Pattern Recognition</span><span class="font3">, vol. 40, no. 12, pp. 3358–3378, December 2007, doi: 10.1016/j.patcog.2007.04.009.</span></p></li>
<li>
<p><span class="font3">[11] &nbsp;&nbsp;&nbsp;A. Fernández, S. García, M. Galar, R. C. Prati, B. Krawczyk, and F. Herrera, </span><span class="font3" style="font-style:italic;">Learning from Imbalanced Data Sets</span><span class="font3">, 10th ed. Berlin: Springer, 2018.</span></p></li>
<li>
<p><span class="font3">[12] &nbsp;&nbsp;&nbsp;J. Van Hulse and T. Khoshgoftaar, &quot;Knowledge discovery from imbalanced and noisy data,&quot; </span><span class="font3" style="font-style:italic;">Data &amp;&nbsp;Knowledge Engineering.</span><span class="font3">, vol. 68, no. 12, pp. 1513–1542, December 2009, doi: 10.1016/j.datak.2009.08.005.</span></p></li>
<li>
<p><span class="font3">[13] &nbsp;&nbsp;&nbsp;A. Ilham, “Komparasi Algoritma Kasifikasi dengan Pendekatan Level Data Untuk Menangani Data Kelas Tidak Seimbang,” </span><span class="font3" style="font-style:italic;">Jurnal Ilmiah Ilmu Komputer, vol. 3, no. 1, 1 April 2017, pp. 16</span><span class="font3">, doi: 10.35329/jiik.v3i1.60.</span></p></li>
<li>
<p><span class="font3">[14] &nbsp;&nbsp;&nbsp;S. Mulyati, Y. Yulianti, and A. Saifudin, “Penerapan Resampling dan Adaboost untuk Penanganan Masalah Ketidakseimbangan Kelas Berbasis Naϊve Bayes pada Prediksi Churn Pelanggan,” </span><span class="font3" style="font-style:italic;">Jurnal Informatika Universitas Pamulang</span><span class="font3">, vol. 2, no. 4, p. 190, Desember 2017, doi: 10.32493/informatika.v2i4.1440.</span></p></li>
<li>
<p><span class="font3">[15] &nbsp;&nbsp;&nbsp;N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, &quot;SMOTE: Synthetic Minority Over-sampling Technique,&quot; </span><span class="font3" style="font-style:italic;">Journal Of Artificial Intelligence Research</span><span class="font3">, vol. 16, no. 2, pp. 321–357, June 2002, doi: 10.1613/jair.953.</span></p></li>
<li>
<p><span class="font3">[16] &nbsp;&nbsp;&nbsp;R. S. Wahono, N. S. Herman, and S. Ahmad, &quot;Neural network parameter optimization based on genetic algorithm for software defect prediction,&quot; </span><span class="font3" style="font-style:italic;">Advanced Science Letters</span><span class="font3">, vol. 20, no. 10–12, pp. 1951–1955, 2014, doi: 10.1166/asl.2014.5641.</span></p></li>
<li>
<p><span class="font3">[17] &nbsp;&nbsp;&nbsp;A. Saifudin and R. S. Wahono, “Pendekatan Level Data untuk Menangani Ketidakseimbangan Kelas pada Prediksi Cacat Software,” </span><span class="font3" style="font-style:italic;">IlmuKomputer.com Journal of Software Engineering</span><span class="font3">, vol. 1, no. 2, pp. 76–85, 2015.</span></p></li>
<li>
<p><span class="font3">[18] &nbsp;&nbsp;&nbsp;J. Sun, J. Lang, H. Fujita, and H. Li, &quot;Imbalanced enterprise credit evaluation with DTE-SBD: Decision tree ensemble based on SMOTE and bagging with differentiated sampling rates, &quot;&nbsp;</span><span class="font3" style="font-style:italic;">Information Sciences</span><span class="font3">, vol. 425, pp. 76–91, Jan. 2018, doi: 10.1016/j.ins.2017.10.017.</span></p></li>
<li>
<p><span class="font3">[19] &nbsp;&nbsp;&nbsp;J. Shin, S. Yoon, Y. W. Kim, T. Kim, B. G. Go, and Y. K. Cha, &quot;Effects of class imbalance on resampling and ensemble learning for improved prediction of cyanobacteria blooms,&quot; </span><span class="font3" style="font-style:italic;">Ecological Informatics</span><span class="font3">, vol. 61, p. 101202, 2021, doi: 10.1016/j.ecoinf.2020.101202.</span></p></li>
<li>
<p><span class="font3">[20] &nbsp;&nbsp;&nbsp;Y. E. Kurniawati and Y. D. Prabowo, &quot;Model optimization of class imbalanced learning using ensemble classifier on over-sampling data,&quot; </span><span class="font3" style="font-style:italic;">IAES International Journal of Artificial Intelligence (IJ-AI)</span><span class="font3">, vol. 11, no. 1, p. 276, March 2022, doi: 10.11591/ijai.v11.i1.pp276-283.</span></p></li>
<li>
<p><span class="font3">[21] &nbsp;&nbsp;&nbsp;M. F. Nugroho and S. Wibowo, “Fitur Seleksi Forward Selection Untuk Menetukan Atribut Yang Berpengaruh Pada Klasifikasi Kelulusan Mahasiswa Fakultas Ilmu Komputer UNAKI Semarang Menggunakan Algoritma Naive Bayes,” </span><span class="font3" style="font-style:italic;">Jurnal Informatika Upgris</span><span class="font3">, vol. 3, no. 1, pp. 63–70, September 2017, doi: 10.26877/jiu.v3i1.1669.</span></p></li>
<li>
<p><span class="font3">[22] &nbsp;&nbsp;&nbsp;J. Zeniarja, A. Ukhifahdhina, and A. Salam, &quot;Diagnosis Of Heart Disease Using K-Nearest Neighbor Method Based On Forward Selection,&quot; </span><span class="font3" style="font-style:italic;">Journal of Applied Intelligent System (JAIS)</span><span class="font3">, vol. 4, no. 2, pp. 39–47, March 2020, doi: 10.33633/jais.v4i2.2749.</span></p></li>
<li>
<p><span class="font3">[23] &nbsp;&nbsp;&nbsp;V. Chandani and R. S. Wahono, “Komparasi Algoritma Klasifikasi Machine Learning Dan Feature Selection pada Analisis Sentimen Review Film,” </span><span class="font3" style="font-style:italic;">Journal of Intelligent Systems</span><span class="font3">, vol. 1, no. 1, pp. 55–59, 2015.</span></p></li>
<li>
<p><span class="font3">[24] &nbsp;&nbsp;&nbsp;E. Pradana, “Analisis Penerapan Adaptive Boosting ( Adaboost ) Dalam Meningkatkan Performasi Algoritma C4.5,” Skripsi, Program Studi Teknik Informatika Universitas Pelita Bangsa, 2018.</span></p></li>
<li>
<p><span class="font3">[25] &nbsp;&nbsp;&nbsp;D. Thammasiri, D. Delen, P. Meesad, and N. Kasap, &quot;A critical assessment of imbalanced class distribution problem: The case of predicting freshmen student attrition,&quot; </span><span class="font3" style="font-style:italic;">Expert Systems with Applications</span><span class="font3">, vol. 41, no. 2, pp. 321–330, February 2014, doi: 10.1016/j.eswa.2013.07.046.</span></p></li>
<li>
<p><span class="font3">[26] &nbsp;&nbsp;&nbsp;N. S. Ramadhanti, W. A. Kusuma, and A. Annisa, “Optimasi Data Tidak Seimbang pada Interaksi Drug Target dengan Sampling dan Ensemble Support Vector Machine,” </span><span class="font3" style="font-style:italic;">Jurnal Teknologi Informasi dan Ilmu Komputer (JTIIK)</span><span class="font3">, vol. 7, no. 6, p. 1221, Desember 2020, doi: 10.25126/jtiik.2020762857.</span></p></li>
<li>
<p><span class="font3">[27] &nbsp;&nbsp;&nbsp;Y. Yamanishi, M. Araki, A. Gutteridge, W. Honda, and M. Kanehisa, &quot;Prediction of drugtarget interaction networks from the integration of chemical and genomic spaces,&quot; </span><span class="font3" style="font-style:italic;">Bioinformatics</span><span class="font3">, vol. 24, no. 13, pp. i232–i240, July 2008, doi: 10.1093/bioinformatics/btn162.</span></p></li>
<li>
<p><span class="font3">[28] &nbsp;&nbsp;&nbsp;F. D. Astuti and F. N. Lenti, “Implementasi SMOTE untuk mengatasi Imbalance Class pada Klasifikasi Car Evolution menggunakan K-NN,” </span><span class="font3" style="font-style:italic;">JUPITER (Jurnal Penelitian Ilmu dan Teknologi Komputer)</span><span class="font3">, vol. 13, no. 1, pp. 89–98, 2021.</span></p></li>
<li>
<p><span class="font3">[29] &nbsp;&nbsp;&nbsp;R. S. Wahono, N. Suryana, and S. Ahmad, &quot;Metaheuristic Optimization based Feature Selection for Software Defect Prediction,&quot; </span><span class="font3" style="font-style:italic;">Journal of Software</span><span class="font3">, vol. 9, no. 5, pp. 1324–1333, May 2014, doi: 10.4304/jsw.9.5.1324-1333.</span></p></li>
<li>
<p><span class="font3">[30] &nbsp;&nbsp;&nbsp;R. S. Wahono and N. S. Herman, &quot;Genetic Feature Selection for Software Defect Prediction,&quot; </span><span class="font3" style="font-style:italic;">Advanced Science Letters</span><span class="font3">, vol. 20, no. 1, pp. 239–244, Jan. 2014, doi: 10.1166/asl.2014.5283.</span></p></li>
<li>
<p><span class="font3">[31] &nbsp;&nbsp;&nbsp;I. Ispandi and R. S. Wahono, “Penerapan Algoritma Genetika untuk Optimasi Parameter pada Support Vector Machine untuk Meningkatkan Prediksi Pemasaran Langsung,” </span><span class="font3" style="font-style:italic;">Journal of Intelligent Systems</span><span class="font3">, vol. 1, no. 2, pp. 115–119, &nbsp;2015, [Online]. Available:</span></p></li></ul>
<p><a href="http://journal.ilmukomputer.org/index.php/jis/article/view/53"><span class="font3">http://journal.ilmukomputer.org/index.php/jis/article/view/53</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[32] &nbsp;&nbsp;&nbsp;F. Handayanna, “Prediksi Penyakit Diabetes Mellitus Dengan Metode Support Vector Machine Berbasis Particle Swarm Optimization,” </span><span class="font3" style="font-style:italic;">Jurnal Teknik Informatika (JTI)</span><span class="font3">, vol. 2, no. 1, pp. 30–37, 2016, [Online]. Available: </span><a href="https://ejournal.antarbangsa.ac.id/jti/article/view/5"><span class="font3">https://ejournal.antarbangsa.ac.id/jti/article/view/5</span></a></p></li>
<li>
<p><span class="font3">[33] &nbsp;&nbsp;&nbsp;A. A. Saraswati, “Optimasi Algoritma C4.5 dalam Prediksi Sekolah Lanjutan Tingkat Atas Menggunakan Seleksi Fitur Algoritma Genetika di SMP Islam Al-hikmah Pondok Cabe,” Skripsi, Program Studi Teknik Informatika Universitas Pelita Bangsa, Bekasi, 2019.</span></p></li>
<li>
<p><span class="font3">[34] &nbsp;&nbsp;&nbsp;Y. Aufar, I. S. Sitanggang, and - Annisa, &quot;Parameter Optimization of Rainfall-runoff Model GR4J using Particle Swarm Optimization on Planting Calendar,&quot; </span><span class="font3" style="font-style:italic;">International Journal on Advanced Science, Engineering and Information Technology</span><span class="font3">, vol. 10, no. 6, p. 2575, December 2020, doi: 10.18517/ijaseit.10.6.9110.</span></p></li>
<li>
<p><span class="font3">[35] &nbsp;&nbsp;&nbsp;H. A. Younis, D. S. Hammadi, and A. N. Younis, &quot;Identify tooth cone beam computed tomography based on contourlet particle swarm optimization,&quot; </span><span class="font3" style="font-style:italic;">IAES International Journal of Artificial Intelligence (IJ-AI)</span><span class="font3">, vol. 11, no. 1, p. 397, &nbsp;&nbsp;March &nbsp;&nbsp;2022, doi:</span></p></li></ul>
<p><span class="font3">10.11591/ijai.v11.i1.pp397-404.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[36] &nbsp;&nbsp;&nbsp;B. Pang and L. Lee, &quot;A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts,&quot; </span><span class="font3" style="font-style:italic;">ACL '04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</span><span class="font3">, vol. 42, pp. 271--278, 2004, [Online]. Available: </span><a href="http://arxiv.org/abs/cs/0409058"><span class="font3">http://arxiv.org/abs/cs/0409058</span></a></p></li>
<li>
<p><span class="font3">[37] &nbsp;&nbsp;&nbsp;A. R. Naufal, R. Satria, and A. Syukur, “Penerapan Bootstrapping untuk Ketidakseimbangan Kelas dan Weighted Information Gain untuk Feature Selection pada Algoritma Support Vector Machine untuk Prediksi Loyalitas Pelanggan,” </span><span class="font3" style="font-style:italic;">Journal of Intelligent Systems</span><span class="font3">, vol. 1, no. 2, pp. 98–108, 2015.</span></p></li>
<li>
<p><span class="font3">[38] &nbsp;&nbsp;&nbsp;G. Xia and W. Jin, &quot;Model of Customer Churn Prediction on Support Vector Machine,&quot; </span><span class="font3" style="font-style:italic;">Systems Engineering - Theory &amp;&nbsp;Practice</span><span class="font3">, vol. 28, no. 1, pp. 71–77, January 2008, doi: 10.1016/S1874-8651(09)60003-X.</span></p></li>
<li>
<p><span class="font3">[39] &nbsp;&nbsp;&nbsp;Z.-Y. Chen, Z.-P. Fan, and M. Sun, &quot;A hierarchical multiple kernel support vector machine for customer churn prediction using longitudinal behavioral data,&quot; </span><span class="font3" style="font-style:italic;">European Journal of Operational &nbsp;Research</span><span class="font3">, &nbsp;vol. 223, no. 2, pp. 461–472, December 2012, doi:</span></p></li></ul>
<p><span class="font3">10.1016/j.ejor.2012.06.040.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[40] &nbsp;&nbsp;&nbsp;A. Bisri and R. S. Wahono, “Penerapan Adaboost untuk Penyelesaian Ketidakseimbangan Kelas pada Penentuan Kelulusan Mahasiswa dengan Metode Decision Tree,” </span><span class="font3" style="font-style:italic;">Journal of Intelligent Systems</span><span class="font3">, vol. 1, no. 1, pp. 27–32, 2015.</span></p></li>
<li>
<p><span class="font3">[41] &nbsp;&nbsp;&nbsp;L. D. Utami and R. S. Wahono, “Integrasi Metode Information Gain Untuk Seleksi Fitur dan Adaboost Untuk Mengurangi Bias Pada Analisis Sentimen Review Restoran Menggunakan Algoritma Naïve Bayes,” </span><span class="font3" style="font-style:italic;">Journal of Intelligent Systems</span><span class="font3">, vol. 1, no. 2, pp. 120–126, 2015.</span></p></li>
<li>
<p><span class="font3">[42] &nbsp;&nbsp;&nbsp;A. Rohman, V. Suhartono, and C. Supriyanto, “Penerapan Agoritma C4.5 Berbasis Adaboost Untuk Prediksi Penyakit Jantung,” </span><span class="font3" style="font-style:italic;">Jurnal Teknologi Informasi</span><span class="font3">, vol. 13, no. 1, pp. 13–19, 2017.</span></p></li>
<li>
<p><span class="font3">[43] &nbsp;&nbsp;&nbsp;I.-C. Yeh and C. Lien, &quot;The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients, &quot;&nbsp;</span><span class="font3" style="font-style:italic;">Expert Systems with Applications</span><span class="font3">, vol. 36, no. 2, pp. 2473–2480, March 2009, doi: 10.1016/j.eswa.2007.12.020.</span></p></li>
<li>
<p><span class="font3">[44] &nbsp;&nbsp;&nbsp;S. Moro, R. M. S. Laureano, and P. Cortez, &quot;Using data mining for bank direct marketing: An</span></p></li></ul>
<p><span class="font3">application of the CRISP-DM methodology,&quot; </span><span class="font3" style="font-style:italic;">European Simulation and Modelling Conference 2011</span><span class="font3">, no. 1, pp. 117–121, 2011.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[45] &nbsp;&nbsp;&nbsp;Z. K. S. Domas, “Pengaruh Tekanan, Kesempatan, Rasionalitas, Kompetensi, Arogansi, serta Kolusi terhadap Ketidakbersediaan Transparansi Pengungkapan Anti-korupsi: Analisis Model Heksagon,” Skripsi. Program Studi Diploma IV Akuntansi Politeknik Keuangan Negara STAN, Tangerang Selatan, 2021.</span></p></li>
<li>
<p><span class="font3">[46] &nbsp;&nbsp;&nbsp;M. Rizkiawan, “Analisis Fraud Hexagon dan Tata Kelola Perusahaan Atas Adanya Kecurangan Dalam Laporan Keuangan,” Skripsi, Program Studi Diploma IV Akuntansi Politeknik Keuangan Negara STAN, 2021.</span></p></li>
<li>
<p><span class="font3">[47] &nbsp;&nbsp;&nbsp;UCI Machine Learning Repository, &quot;Credit Approval Data Set,&quot; 1998. </span><a href="https://archive.ics.uci.edu/ml/datasets/credit+approval"><span class="font3">https://archive.ics.uci.edu/ml/datasets/credit+approval</span></a></p></li>
<li>
<p><span class="font3">[48] &nbsp;&nbsp;&nbsp;UCI Machine Learning Repository, &quot;South German Credit (UPDATE) Data Set,&quot; 2019. </span><a href="https://archive.ics.uci.edu/ml/datasets/South+German+Credit+%28UPDATE%29"><span class="font3">https://archive.ics.uci.edu/ml/datasets/South+German+Credit+%28UPDATE%29</span></a></p></li>
<li>
<p><span class="font3">[49] &nbsp;&nbsp;&nbsp;V. Lohweg, &quot;banknote authentication Data Set,&quot; </span><span class="font3" style="font-style:italic;">UCI Machine Learning Repository</span><span class="font3">, 2012. </span><a href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication"><span class="font3">https://archive.ics.uci.edu/ml/datasets/banknote+authentication</span></a></p></li>
<li>
<p><span class="font3">[50] &nbsp;&nbsp;&nbsp;N. Hooda, CSED, TIET, and Patiala, &quot;Audit Data Data Set,&quot; </span><span class="font3" style="font-style:italic;">UCI Machine Learning Repository</span><span class="font3">, 2018. </span><a href="https://archive.ics.uci.edu/ml/datasets/Audit+Data"><span class="font3">https://archive.ics.uci.edu/ml/datasets/Audit+Data</span></a></p></li>
<li>
<p><span class="font3">[51] &nbsp;&nbsp;&nbsp;R. Kohavi and B. Becker, &quot;Census Income Data Set,&quot; </span><span class="font3" style="font-style:italic;">UCI Machine Learning Repository</span><span class="font3">, 1994. </span><a href="https://archive.ics.uci.edu/ml/datasets/Census+Income"><span class="font3">https://archive.ics.uci.edu/ml/datasets/Census+Income</span></a></p></li>
<li>
<p><span class="font3">[52] &nbsp;&nbsp;&nbsp;S. Tomczak, &quot;Polish companies bankruptcy data Data Set,&quot; </span><span class="font3" style="font-style:italic;">UCI Machine Learning Repository</span><span class="font3">, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016.</span></p></li></ul>
<p><a href="https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data"><span class="font3">https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[53] &nbsp;&nbsp;&nbsp;Adiyanto, “Prediksi Harga Crude Palm Oil Menggunakan Metode Support Vector Machine dengan Optimasi Parameter Menggunakan Algoritma Genetika,” </span><span class="font3" style="font-style:italic;">Jurnal IPSIKOM</span><span class="font3">, vol. 1, no. 1, 2013.</span></p></li>
<li>
<p><span class="font3">[54] &nbsp;&nbsp;&nbsp;D. Kanellopoulos, S. Kotsiantis, and P. Pintelas, &quot;Handling imbalanced datasets: A review Cite this paper Related papers Handling imbalanced datasets: A review,&quot; </span><span class="font3" style="font-style:italic;">GESTS International Transaction on Computer Science and Engineering</span><span class="font3">, vol. 30, no. 1, pp. 25–36, 2006.</span></p></li>
<li>
<p><span class="font3">[55] &nbsp;&nbsp;&nbsp;J. S. D. Raharjo, “Model Artificial Neural Network Berbasis Particle Swarm Optimization Untuk Prediksi Laju Inflasi,” </span><span class="font3" style="font-style:italic;">Jurnal Sistem Komputer</span><span class="font3">, vol. 3, no. 1, pp. 10–21, 2013.</span></p></li>
<li>
<p><span class="font3">[56] &nbsp;&nbsp;&nbsp;R. S. Wahono and N. Suryana, &quot;Combining Particle Swarm Optimization based Feature Selection and Bagging Technique for Software Defect Prediction,&quot; </span><span class="font3" style="font-style:italic;">International Journal of Software Engineering and Its Applications</span><span class="font3">, vol. 7, no. 5, pp. 153–166, September 2013, doi: 10.14257/ijseia.2013.7.5.16.</span></p></li>
<li>
<p><span class="font3">[57] &nbsp;&nbsp;&nbsp;C. Shabrina, “Metode Hibrida Oversampling Dan Undersampling Untuk Menangani Ketidakseimbangan Data Kegagalan Akademik Pada Universitas XYZ,” Desertasi, Institut Teknologi Sepuluh Nopember, 2019.</span></p></li>
<li>
<p><span class="font3">[58] &nbsp;&nbsp;&nbsp;F. Itoo, Meenakshi, and S. Singh, &quot;Comparison and analysis of logistic regression, Naïve Bayes and KNN machine learning algorithms for credit card fraud detection, &quot;&nbsp;</span><span class="font3" style="font-style:italic;">International Journal of Information Technology</span><span class="font3">, vol. 13, no. 4, pp. 1503–1511, August 2021, doi: 10.1007/s41870-020-00430-y.</span></p></li>
<li>
<p><span class="font3">[59] &nbsp;&nbsp;&nbsp;F. Gorunescu, </span><span class="font3" style="font-style:italic;">Data Mining</span><span class="font3">, 12th ed., vol. 12. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011. doi: 10.1007/978-3-642-19721-5.</span></p></li>
<li>
<p><span class="font3">[60] &nbsp;&nbsp;&nbsp;J. Perols, &quot;Financial Statement Fraud Detection: An Analysis of Statistical and Machine Learning Algorithms,&quot; </span><span class="font3" style="font-style:italic;">Auditing: A Journal of Practice &amp;&nbsp;Theory</span><span class="font3">, vol. 30, no. 2, pp. 19–50, May 2011, doi: 10.2308/ajpt-50009.</span></p></li></ul>
<p><span class="font3">184</span></p>