---
layout: full_article
title: "Ships Detection on Aerial Imagery using Transfer Learning and Selective Search"
author: "Desak Ayu Sista Dewi, Dewa Made Sri Arsa, Anak Agung Ngurah Hary Susila, I Made Oka Widyantara"
categories: merpati
canonical_url: https://jurnal.harianregional.com/merpati/full-109397 
citation_abstract_html_url: "https://jurnal.harianregional.com/merpati/id-109397"
citation_pdf_url: "https://jurnal.harianregional.com/merpati/full-109397"  
comments: true
---

<p><span class="font0">JURNAL ILMIAH MERPATI VOL. 11, NO. 3 DECEMBER 2023 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p-ISSN: 2252-3006</span></p>
<p><span class="font0">e-ISSN: 2685-2411</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font1" style="font-weight:bold;"><a name="bookmark1"></a>Ships Detection on Aerial Imagery using Transfer Learning and Selective Search</span></h1>
<h2><a name="bookmark2"></a><span class="font0" style="font-weight:bold;"><a name="bookmark3"></a>Desak Ayu Sista Dewi<sup>a1</sup>, Dewa Made Sri Arsa<sup>b2</sup>, Anak Agung Ngurah Hary Susila<sup>b3</sup>, I Made Oka Widyantara<sup>c4</sup></span></h2>
<p><span class="font0"><sup>a</sup>Department of Industrial Engineering, Universitas Udayana <sup>b</sup>Department of Information Technology, Universitas Udayana <sup>c</sup>Department of Electronic Engineering, Universitas Udayana</span></p>
<p><span class="font0">e-mail: <sup>1</sup></span><a href="mailto:sistadasd@unud.ac.id"><span class="font0" style="text-decoration:underline;">sistadasd@unud.ac.id</span></a><span class="font0">, <sup>2</sup></span><a href="mailto:dewamsa@unud.ac.id"><span class="font0" style="text-decoration:underline;">dewamsa@unud.ac.id</span></a><span class="font0">, <sup>3</sup></span><a href="mailto:harysusila@unud.ac.id"><span class="font0" style="text-decoration:underline;">harysusila@unud.ac.id</span></a><span class="font0">,</span></p>
<p><a href="mailto:4oka.widyantara@unud.ac.id"><span class="font0" style="text-decoration:underline;"><sup>4</sup>oka.widyantara@unud.ac.id</span></a></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Abstrak</span></p>
<p><span class="font0" style="font-style:italic;">Lalu lintas di perairan air seperti pelabuhan dan laut sangat penting untuk dilakukan pengamatan karena dapat membantu meminimalisir kecelakaan kapal laut yang tidak diinginkan. Oleh karena itu, sebuah metode pendeteksian otomatis kapal laut diusulkan dalam penelitian ini. Beberapa metode deep learning dikaji dalam penelitian ini, seperti MobileNetV2, DenseNet121, VGG16, dan ResNet50. Kemudian, metode yang sudah dilatih digunakan untuk mendeteksi kapal laut. Untuk mempercepat pendeteksian, kami menggunakan metode selective search daripada sliding window untuk mengambil sampel kandidat objek dari gambar perairan yang diberikan. Eksperiment dilakukan dengan menggunakan data Shipsnet dan diuji pada data satelit. Pada penelitian ini, evaluasi lintas domain juga dilakukan untuk dataset yang diambil menggunakan Google Earth. Hasil eksperiment mengindikasikan bahwa MobileNetV2 memiliki performa klasifikasi dan deteksi terbaik dengan akurasi sebesar 99.07</span><span class="font0">%</span><span class="font0" style="font-style:italic;">. Metode MobileNetV2 juga dapat mendeteksi kapal laut pada skenario eksperiment lintas domain.</span></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Kata kunci: </span><span class="font0" style="font-style:italic;">convolutional neural network, deep learning, selective search, pendeteksian kapal laut</span></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font0" style="font-style:italic;">The traffic in the water area such as harbor and sea strait is highly important to be monitored because it helps to minimize unwanted ships accident. As a result, we proposed an automatic detection method to localize ships contained in sattelite image. We examine several deep learning methods as the classification backbone, namely MobileNetV2, DenseNet121, VGG16, and ResNet50. Afterwards, we employed the trained model for detecting the ships. To make the detection faster, inspite of using a sliding window, we use selective search to sample the object candidates from the given scene. The experiments were done using Shipsnet dataset and tested on aerial images. We also conducted a cross domain evaluation where the images were taken using Google Earth. The results indicate that MobileNetV2 has the best performance on classification and detection tasks with 99.07</span><span class="font0">% </span><span class="font0" style="font-style:italic;">of accuracy. The MobileNetV2 is also able to detect the ships on cross-domain scenarios.</span></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font0" style="font-style:italic;">convolutional neural network, deep learning, selective search, ship detection</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font0" style="font-weight:bold;"><a name="bookmark5"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font0">The development of technology for ship has been advanced rapidly in order to improve the navigation control, navigation security, and reduce manpower cost. The breakthrough in artificial intelligence, especially deep learning, brings a fresh breath in developing robust autonomous ship technology through ships recognition, classification, and ships detection. For example, automatic ships detection can be used for mapping the movement of ships in ocean area, so the unwanted event like collision can be prevented.</span></p>
<p><span class="font0">Ships detection has important role in monitoring the ocean and becomes significant in remote sensing. Various studies have been done to detect ship automatically, however two challenges still affecting the method performance, such as complex background and the small object scales [1]. Deep learning becomes more popular since AlexNet won the ImageNet challenge in 2012 [2] while previously the approaches dominated by the used of scale-invariant feature transform (SIFT), histogram of oriented gradients (HOG) along with support vector machine (SVM) and Adaboost classifiers [3].</span></p>
<p><span class="font0">The approach in object detection using deep learning can be divided into three approaches [4]. The first approach is a two-stage method. In two-stage method, the phase is began with finding object candidates, for example selective search, in given scene. Then, the score, label, and coordinate offset are computed for each object candidates. R-CNN is one of the example using the two-stage method [5] while later improved in faster and optimised method [6]. The second approach is one-stage method which aiming to decrease the processing time in the first method. The phase for selecting the candidate proposal is removed, so the objects are detected directly from the feature map. YOLOs [7]–[9], SSD [10], and RetinaNet [11] are the example of the best method which adopting the second approach. The third method is an attention-based mechanisms. This mechanism is inspired by the method proposed in [12] for allowing content-based summarization of information given a variable length source sentence using an encoder-decoder in a neural sequence.</span></p>
<p><span class="font0">Beside of object detection approaches, the detection of ships become harder causes by the remote sensing image covering a large sea area which make the ship is in small size [13], [14]. Therefore, it becomes computationally expensive when applying directly a dense feature extraction. Consequently, the two-stage schema was preferred here. (E.g.Refs.11,12,19,21,31)</span></p>
<p><span class="font0">This study examines extensively the robust deep learning for detecting ships in aerial imagery through a transfer learning mechanism using a two-stage method. The pretrained deep learning was chosen as a feature extraction and the top of that we built a classification layer. It used on the first stage. On the detection phase, which is the second stage, we utilized selective search algorithm to detect objects contained on the given image. Then, the detected object will be filtered by the model outputted from the first stage and the objects classified as not ship will be deleted. Four deep learning architectures were used on the first stage</span></p>
<p><span class="font0">This paper is written as follows. The proposed method is introduced in section 2. Then,</span></p>
<p><span class="font0">we provide the experiment setup in section 3. In section 4, the experiment results are presented</span></p>
<p><span class="font0">with extensive discussion. Finally, we conclude our findings in section 5.</span></p><img src="https://jurnal.harianregional.com/media/109397-1.jpg" alt="" style="width:383pt;height:204pt;">
<p><span class="font0">Figure 1. The deep learning architecture used in this study</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font0" style="font-weight:bold;"><a name="bookmark7"></a>2. &nbsp;&nbsp;&nbsp;&nbsp;Research Method</span><br><br><span class="font0" style="font-weight:bold;"><a name="bookmark8"></a>2.1 &nbsp;&nbsp;&nbsp;Proposed workflow</span></h2></li></ul>
<p><span class="font0">This study used a two-stage approach. The first stage purpose is to build a model which can classify an image rather ship or not ship. In this study, the classification model was built using a transfer learning where the pretrained model is the based method. The deep learning</span></p>
<p><span class="font0">architecture built on this study can be seen in Fig. 1. The pretrained model was used as a representation layer. Before the images feed to this layer, we resize the image to accommodate the variation of input size and to fit the required input size for the pretrained model. After that, we applied a global average pooling for the feature map from the last layer of the pretrained model. The pooling operation was important to get one dimensional structure of features to be connected to fully connected layer. Then we insert a dropout layer to prevent the vanishing gradient problem and overfitting. Finally, a dense layer was placed at the last as a classification layer and contained a softmax activation layer. This layer will produce a probability of the input image belong to certain classes.</span></p>
<p><span class="font0">In the second stage, we firstly detect the object candidates using selective search algorithm. Then, all candidates are filtered by classifying each of them using the classification model. The object candidates, which classified as not ship, will be excluded on merging process. In the merging process, we combine the overlap region of objects into one object.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark9"></a><span class="font0" style="font-weight:bold;"><a name="bookmark10"></a>2.2. &nbsp;&nbsp;&nbsp;Pretrained Deep Learning</span></h2></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font0" style="font-style:italic;">1) &nbsp;&nbsp;&nbsp;MobileNetV2</span><span class="font0">: MobileNet is a model which use depth-wise separable convolutions, a combination of several convolutional layers which consist as deptwise convolution and pointwise convolution [15]. Each input channel on MobileNet applied depthwise convolution on one layer, then followed by pointise convolution to merge the output from input layer. Most known type of MobileNet currently are MobileNet and MobileNetv2. MobileNetV2 have 1 additional layer on MobileNet’s residual block. This additional layer used for data channel expansion before the data is sent into depthwise convolutional layer.</span></p></li>
<li>
<p><span class="font0" style="font-style:italic;">2) &nbsp;&nbsp;&nbsp;DenseNet</span><span class="font0">: DenseNet is one of pretrained neural network method which utilize repeated feature usage, that makes DenseNet is an easily trained model and has high efficiency parameter [16]. Combination of feature maps that come out from different layers increased the input variation and the efficiency of DenseNet model. Size of used layers on DenseNet is very narrow, with few additions of feature maps to collect knowledge from networks but maintained other feature map left.</span></p></li></ul>
<p><span class="font0">DenseNet starts with a basic convolution and pooling layer, followed by a dense block, then continued into transition layer. After that, add another dense block followed by a transition layer. This process can be repeated several times, then add a dense block followed by a classification layer. DenseNet121 consist of 1 7x7 convolution layer, 58 3x3 convolution layers, 61 1x1 convolutional layers, 4 average pooling, and 1 fully connected layer. In short, DenseNet121 has 120 convolutions and 4 average pooling.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0" style="font-style:italic;">3) &nbsp;&nbsp;&nbsp;ResNet</span><span class="font0">: ResNet stands for Residual Network. ResNet was made in 2016 by Researcher team from Microsoft [17]. ResNet inspired by VGG and have its own convolutional layer with 3x3 filter size. Similar to VGG, ResNet also have some variant depending on the amount of used layer, i.e., ResNet50. ResNet50 used 50 neural networks layers. ResNet50 use 3-layer bottleneck block which replaced 2-layer block in ResNet34. ResNet creation must follow 2 main principles, such as (i) output which we got from the same size feature map must have the same filter size on each layer and (ii) if the size of feature map is only half from the start, then the amount of filter needs to be multiplied by 2 to maintain the complexity of each layer.</span></p></li>
<li>
<p><span class="font0" style="font-style:italic;">4) &nbsp;&nbsp;&nbsp;VGG16</span><span class="font0">: VGG is an abbreviation of Visual Geometry Group which developed by Oxford University. VGG architecture has already tested on ILSVR2014 and come out as first runner up on classification task [18]. There are 2 most used VGG type, such as VGG16 and VGG19. Number that appeared after the VGG is the amounts of layers that used in the model. VGG16 used 16 layers on its creation, which consist of 5 convolutional block that connected into multilayer perceptron classifier. This multilayer perceptron is made by 2 hidden layers and 1 output layer.</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark11"></a><span class="font0" style="font-weight:bold;"><a name="bookmark12"></a>2.3. &nbsp;&nbsp;&nbsp;Selective search</span></h2></li></ul>
<p><span class="font0">We used the selective search proposed in [19] as the initial region of interest. This method was developed to rapidly capture all objects in any scales contained on the scenes using various strategies. Selective search was developed to capture multiple scales of objects, improve diversification on detecting varying types of objects, and decrease the detection time. Multiple scales detection was handles by hierarchical grouping algorithm. This approach surpasses the efficiency of the sliding window method, as it eliminates the need to run multiple window sizes across all pixels in the image. The process involves initially segmenting the image</span></p>
<p><span class="font0">based on color, texture, intensity, and other low-level features. Secondly, a greedy algorithm is applied to iteratively merge segments into larger regions. In the third step, a set of region proposals is generated, considering various scales and shapes to capture potential object boundaries. The fourth step involves measuring objectness scores and ranking the region proposals. The final set of region proposals is obtained by selecting the top-ranked proposals.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark13"></a><span class="font0" style="font-weight:bold;"><a name="bookmark14"></a>3. &nbsp;&nbsp;&nbsp;&nbsp;Experiment Setup</span></h2></li></ul>
<p><span class="font0">The experiment was conducted in such environment and scenarios. The methods were developed using Tensorflow and ran on GPU RTX2060 6GB, Intel core i5, and RAM 16 GB. For the dataset, we used Shipsnet dataset which can be found on Kaggle. In the resizing layer, we resize the image into 96x96. The proposed method was trained using Adam optimizer and 0.001 of learning rate. The loss function used in this study is cross entropy. Given C number of classes, the prediction </span><span class="font6">̂</span><span class="font0">, and target </span><span class="font0" style="font-style:italic;">y</span><span class="font0">, then the loss can be computed using the following equation.</span></p>
<p><span class="font4" style="font-style:italic;">C</span></p>
<p><span class="font6">ℒ<sub>c</sub>ℯ=∑</span><span class="font0" style="font-style:italic;">y</span><span class="font6">log( ̂) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font0">(1)</span></p>
<p><span class="font0">Furthermore, the evaluation was done in two phases. In the first phases, the evaluation was done by examining the performance of the methods on classifying ships images and not ships images. The Shipsnet dataset is not balanced where it provides 1,000 images for ships and 3000 images as not ships. Then we augmented the ships images to make the data in balanced condition. Moreover, the k-Fold cross validation was chosen with k is equal to 10. For each fold, we measure the classification accuracy and the loss value.</span></p>
<p><span class="font0">The accuracy was computed using equation 2 where TP, TN, FP, and FN are true positive, true negative, false positive, and false negative respectively. Equation 3 show the categorical cross entropy loss function used in this study. N is the size of output, </span><span class="font0" style="font-style:italic;">y<sub>i</sub></span><span class="font0"> correspond to the actual target value, and ̂<sub>i</sub> is the predicted value from the model.</span></p>
<div>
<p><span class="font0" style="font-style:italic;">TP</span><span class="font6"> + </span><span class="font0" style="font-style:italic;">TN</span></p>
</div><br clear="all">
<div>
<p><span class="font0" style="font-style:italic;">Acc =----------------</span></p>
<p><span class="font0" style="font-style:italic;">TP</span><span class="font6"> + </span><span class="font0" style="font-style:italic;">TN</span><span class="font6"> + </span><span class="font0" style="font-style:italic;">FP</span><span class="font6"> + </span><span class="font0" style="font-style:italic;">FN</span></p>
</div><br clear="all">
<div>
<p><span class="font0">(2)</span></p>
</div><br clear="all">
<p><span class="font4" style="font-style:italic;">N</span></p>
<div>
<p><span class="font6">=</span></p>
</div><br clear="all">
<p><span class="font6">-∑</span><span class="font0" style="font-style:italic;">yt</span><span class="font6"> </span><span class="font2">⋅</span><span class="font6"> </span><span class="font0" style="font-style:italic;">log</span><span class="font6"> ̂</span></p>
<div>
<p><span class="font0">(3)</span></p>
</div><br clear="all">
<p><span class="font6">1 = 1</span></p>
<p><span class="font0">In the second phases, the examination was done qualitatively. The images’ contained ships were extracted from scene given from Shipsnet dataset. Moreover, we conduct a cross domain evaluation where the images taken from Google Earth in Bali’s local harbour to show the generalization performance of the trained deep learning.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark15"></a><span class="font0" style="font-weight:bold;"><a name="bookmark16"></a>4. &nbsp;&nbsp;&nbsp;Results and discussion</span></h2></li></ul>
<p><span class="font0">The classification results can be seen in Fig. 2. The accuracy presents the performance of the given trained model to distinguish ship and not ship while the loss value, which is given in Fig. 3, provides the information how far the predicted value to the original value. In the accuracy, the higher its value, the more data are classified as the original class. The lower the loss value means that the predicted value is closer to the true value. From Fig. 2, we found that the best model is MobileNetV2 with average accuracy about 99.07% and standard deviation about 0.38%. MobileNetV2 provides the highest accuracy compared to other. Moreover, the second-best method is DenseNet121 with 98.28% and 0.49 of accuracy and standard deviation, followed by VGG16 and ResNet50 where their accuracies are 94.70%±0.92 and 80.53%±4.25 respectively. This result demonstrates that MobileNetV2 has better feature representation than alternative methods for ship classification.</span></p>
<div><img src="https://jurnal.harianregional.com/media/109397-2.jpg" alt="" style="width:218pt;height:176pt;">
<p><span class="font0">Figure 2. The accuracies for all k-folds at k = 10 for all methods. Note: higher is better</span></p>
</div><br clear="all">
<p><span class="font5" style="font-weight:bold;">DcnscNctiSI MobileNelVS ResNet50</span></p>
<p><span class="font5" style="font-weight:bold;">VGG16</span></p>
<p><span class="font0">Similar results were also figured out in the loss value for all folds as shown in Fig. 3. The lowest loss value is produced from MobileNetV2, and the highest loss value is computed by VGG16. This loss value results show that MobileNetV2 prediction is evenly matched the actual value. VGG16 shows has higher loss than other methods. DenseNet121 has closer performance to MobileNetV2 since its loss value has no significant differences. Moreover, Table 1 shows the number of parameters for each deep learning models. MobilenetV2 has lowest number of parameters, indicating less resource utilization for inference. On the other hand, VGG16 has the highest number parameters among others.</span></p>
<p><span class="font5" style="font-weight:bold;">1</span></p>
<p><span class="font5" style="font-weight:bold;">0.8</span></p>
<p><span class="font5" style="font-weight:bold;">0.6</span></p>
<p><span class="font5" style="font-weight:bold;">C _i</span></p>
<p><span class="font5" style="font-weight:bold;">0.4</span></p>
<div>
<p><span class="font5" style="font-weight:bold;">→- DenseNetlSl • MobileNetVS • ResNctfO</span></p>
<p><span class="font5" style="font-weight:bold;">→- VGG16</span></p><img src="https://jurnal.harianregional.com/media/109397-3.jpg" alt="" style="width:167pt;height:27pt;">
</div><br clear="all">
<div>
<p><span class="font5" style="font-weight:bold;">0.2</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/109397-4.jpg" alt="" style="width:183pt;height:29pt;">
<p><span class="font0">0123456789 10 k-Folds</span></p>
<p><span class="font0">Figure 3. The loss for all k-folds at k = 10. Note: lower is better</span></p>
</div><br clear="all">
<p><span class="font0">Furthermore, Fig. 4 depicts the detection results of all methods. In the top row, where only one ship is appeared in the scene, MobileNetV2 detects the ship accurately while VGG16 is failed to detect the ship. ResNet50 detects the ship as two objects and DenseNet121 has false positive detections. When the scence become more complex (row 2 to 4), MobileNetV2 accurately detect the ships. DenseNet121 produces a lot of false positives, which means it detects the background as the ships. ResNet50 shows good detection on the the complex scene. VGG16 contains falls negatives in the second and fourth row.</span></p>
<p><span class="font3" style="font-weight:bold;">DenseNetl21 MobileNetV2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ResNet50 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VGG16</span></p><img src="https://jurnal.harianregional.com/media/109397-5.jpg" alt="" style="width:426pt;height:405pt;">
<p><span class="font0">Figure 4. Detection results on satellite imagery image for all methods. The green box indicates the detected ship.</span></p>
<p><span class="font0">Table 1. Number </span><span class="font0" style="text-decoration:underline;">of parameters of all deep learning models u</span><span class="font0">sed in this study.</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font0" style="font-weight:bold;">Method</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0" style="font-weight:bold;">Number of parameters (millions)</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font0">DenseNet121</span></p></td><td style="vertical-align:top;">
<p><span class="font0">6,9</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">MobileNetV2</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">2,2</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">ResNet50</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">23,5</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">VGG16</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">134,2</span></p></td></tr>
</table>
<p><span class="font0">Figure 5 shows detection results on cross domain of MobileNetV2 and VGG16. The cross-domain evaluation was conducted to see the generalization performance of the best and the poorest methods based on the results in Fig. 4. As depicted in the top row of Fig. 5, MobileNetV2 is able to detecth the ships eventhough produces come false positive, while VGG16 misses some ships. The first-row images show similar properties to the images in Fig. 4, where the sea has green colour. When we use different scene (second row of Fig. 5),</span></p>
<p><span class="font0">MobileNetV2 is hardly locating the ships and VGG16 detects no ships. This observation means that MobileNetV2 and VGG16 overfit to the training data. Therefore, a special treatment might be needed to improve the performance on cross domain dataset.</span></p>
<p><span class="font3" style="font-weight:bold;">MobileNetV2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VGG16</span></p><img src="https://jurnal.harianregional.com/media/109397-6.jpg" alt="" style="width:232pt;height:223pt;">
<p><span class="font0">Figure 5. Cross-domain deteection of MobileNetV2 and VGG16. The images are taken using Google Earth</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark17"></a><span class="font0" style="font-weight:bold;"><a name="bookmark18"></a>5. &nbsp;&nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font0">In this study, we proposed a workflow for ship detection by combining deep learning and selective search. We examine several deep learning methods, namely DenseNet121, MobileNetV2, ResNet50, and VGG16. The classification accuracy and loss show that MobileNetV2 has better performance than others with the accuracy of 99.07% ± 0.38. Beside of classification results, we also conducted a qualitative evaluation. The qualitative evaluation shows that MobileNetV2 has better accuracy, while DenseNet121 produces high false positive. For generalization performance, we compared MobileNetV2 and VGG16 on cross-domain evaluation where the images are taken using Google Earth on Bali’s local harbour. The results indicate that MobileNetV2 is ablet to detect the ships, but still needs improvement when the scene characteristic is different. As future research direction, we are going to develop a method which has good generalization performance, so the method can be performed well in varying environments.</span></p>
<h2><a name="bookmark19"></a><span class="font0" style="font-weight:bold;"><a name="bookmark20"></a>Acknowledgement</span></h2>
<p><span class="font0">This study is funded by Penelitian Unggulan Program Studi 2021 entitled by “Deteksi kapal laut secara otomatis untuk monitoring laut berbasis convolutional neural network.”</span></p>
<h2><a name="bookmark21"></a><span class="font0" style="font-weight:bold;"><a name="bookmark22"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font0">[1] H. Guo, X. Yang, N. Wang, and X. Gao, “A centernet++ model for ship detection in sar</span></p></li></ul>
<p><span class="font0">images,” Pattern Recognition, vol. 112, p. 107787, 2021.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep</span></p></li></ul>
<p><span class="font0">convolutional neural networks,” Advances in neural information processing systems, vol. 25, pp. 1097–1105, 2012.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[3] &nbsp;&nbsp;&nbsp;Y.-L.Chang,A.Anagaw,L.Chang,Y.C.Wang,C.-Y.Hsiao,andW.-H. Lee, “Ship detection based on yolov2 for sar imagery,” Remote Sensing, vol. 11, no. 7, p. 786, 2019.</span></p></li>
<li>
<p><span class="font0">[4] &nbsp;&nbsp;&nbsp;H. Perreault, G.-A. Bilodeau, N. Saunier, and M. He ́ritier, “Spotnet: Self-attention multi-task network for object detection,” in 2020 17th Conference on Computer and Robot Vision (CRV). IEEE, 2020, pp. 230–237.</span></p></li>
<li>
<p><span class="font0">[5] &nbsp;&nbsp;&nbsp;R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580–587.</span></p></li>
<li>
<p><span class="font0">[6] &nbsp;&nbsp;&nbsp;S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” Advances in neural information processing systems, vol. 28, pp. 91–99, 2015.</span></p></li>
<li>
<p><span class="font0">[7] &nbsp;&nbsp;&nbsp;J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779– 788.</span></p></li>
<li>
<p><span class="font0">[8] &nbsp;&nbsp;&nbsp;J. Redmon and A. Farhadi, “Yolo9000: better, faster, stronger,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 7263–7271.</span></p></li>
<li>
<p><span class="font0">[9] &nbsp;&nbsp;&nbsp;A. Farhadi and J. Redmon, “Yolov3: An incremental improvement,” in Computer Vision and Pattern Recognition, 2018, pp. 1804–02 767.</span></p></li>
<li>
<p><span class="font0">[10] &nbsp;&nbsp;&nbsp;Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. -Y. Fu, and A. C. Berg, “Ssd: Single shot multibox detector,” in European conference on computer vision. Springer, 2016, pp. 21–37.</span></p></li>
<li>
<p><span class="font0">[11] &nbsp;&nbsp;&nbsp;T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dolla ́r, “Focal loss for dense object detection,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980–2988.</span></p></li>
<li>
<p><span class="font0">[12] &nbsp;&nbsp;&nbsp;D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014.</span></p></li>
<li>
<p><span class="font0">[13] &nbsp;&nbsp;&nbsp;Y. Tan, H. Liang, Z. Guan, and A. Sun, “Visual saliency based ship extraction using improved bing,” in IGARSS 2019-2019 IEEE Interna- tional Geoscience and Remote Sensing Symposium. IEEE, 2019, pp. 1292–1295.</span></p></li>
<li>
<p><span class="font0">[14] &nbsp;&nbsp;&nbsp;F. Yang, Q. Xu, F. Gao, and L. Hu,“ Ship detection from optical satellite images based on visual search mechanism,” in 2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS). IEEE, 2015, pp. 3679–3682.</span></p></li>
<li>
<p><span class="font0">[15] &nbsp;&nbsp;&nbsp;M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4510–4520.</span></p></li>
<li>
<p><span class="font0">[16] &nbsp;&nbsp;&nbsp;G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the IEEE confer- ence on computer vision and pattern recognition, 2017, pp. 4700–4708.</span></p></li>
<li>
<p><span class="font0">[17] &nbsp;&nbsp;&nbsp;K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.</span></p></li>
<li>
<p><span class="font0">[18] &nbsp;&nbsp;&nbsp;K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.</span></p></li>
<li>
<p><span class="font0">[19] &nbsp;&nbsp;&nbsp;J. R. Uijlings, K. E. Van De Sande, T. Gevers, and A. W. Smeulders, “Selective search for object recognition,” International journal of computer vision, vol. 104, no. 2, pp. 154–171, 2013.</span></p></li></ul>
<p><span class="font0" style="font-style:italic;">Ships Detection on Aerial Imagery using Transfer Learning and Selective Search (Desak Ayu</span><span class="font0"> 187</span></p>
<p><span class="font0" style="font-style:italic;">Sista Dewi)</span></p>