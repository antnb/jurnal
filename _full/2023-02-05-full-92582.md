---
layout: full_article
title: "Implementasi Decision Tree berbasis Forward Selection untuk Klasifikasi Penyakit Ginjal Kronis"
author: "Jeremi Herodian Abednigo, Made Agung Raharja"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-92582 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-92582"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-92582"  
comments: true
---

<p><span class="font2">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font2">Volume 12, No 2. November 2023</span></p>
<p><span class="font2">p-ISSN: 2301-5373</span></p>
<p><span class="font2">e-ISSN: 2654-5101</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>Implementasi Decision Tree berbasis Forward Selection untuk Klasifikasi Penyakit Ginjal Kronis</span></h1>
<p><span class="font2">Jeremi Herodian Abednigo<sup>a1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Made Agung Raharja<sup>a2</sup></span></p>
<p><span class="font2"><sup>a</sup>Program Studi Informatika, Universitas Udayana Kuta Selatan, Badung, Bali, Indonesia </span><a href="mailto:1jeremi.herodian.a43@gmail.com"><span class="font2"><sup>1</sup>jeremi.herodian.a43@gmail.com</span></a><span class="font2"> </span><a href="mailto:2made.agung@unud.ac.id"><span class="font2"><sup>2</sup>made.agung@unud.ac.id</span></a></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font2">Penyakit Ginjal Kronis adalah penyakit yang umum di masyarakat dengan jumlah penderita yang terus meningkat. Penyakit ini menyerang ginjal yang mengakibatkan ginjal tidak bisa berfungsi dengan dengan baik. Data mining adalah proses untuk mengekstrak data dengan tujuan mendapatkan informasi yang berharga, salah satunya metode data mining adalah klasifikasi. Algoritma Decision Tree adalah salah satu algoritma klaifikasi yang bisa digunakan untuk melakukan klasifikasi penyakit ginjal kronis. Pada penelitian ini, klasifikasi diakukan dengan Decision Tree yang digabungkan dengan seleksi fitur menggunakan Forward Selection. Forward Selection digunakan untuk mengurangi fitur yang tidak relevan terhadap target klasifikasi. Dataset yang digunakan adalah Chronic Kidney Disease dataset dari Kaggle. Pada hasil pengujian Decision Tree dengan bantuan library sklearn dari python dengan cross validation sebanyak 10 fold didapatkan bahwa seleksi fitur dengan forward selection berhasil meningkatkan hasil akurasi, presisi, recall, dan f1 score secara berurutan adalah 99.5%, 98,75%, 100%, 99,35%</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font2" style="font-style:italic;">Klasifikasi, Penyakit Ginjal Kronis, Decision Tree, Feature Selection, Forward Selection</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Pendahuluan</span></h3></li></ul>
<p><span class="font2">Organ ginjal merupakan salah satu organ yang penting dalam tubuh manusia yang berfungsi untuk menjaga kadar darah dengan mencegah menumpuknya limbah dan mengatur keseimbangan carian tubuh. Penyakit ginjal adalah gangguan yang mengenai organ ginjal yang banyak disebabkan oleh infeksi, tumor, kelainan bawaan, gangguan metabolic, atau degeneratif dan lain-lain [1].</span></p>
<p><span class="font2">Penyakit ginjal kronis merupakan salah satu penyakit yang tingkat penderitanya cukup tinggi di dunia. Menurut The United States Renal Data System (USRDS), jumlah penderita penyakit ginjal kronis diperkirakan mencapai 2.020 kasus perjuta penduduknya pada tahun 2012 dengan tingkat pertumbuhan mencapai 7% dan untuk di Amerika Serikat, hampir setiap tahunnya sekitar 70 orang meninggal dunia akibat menderita penyakit ginjal kronis[4]. Dengan laju pertumbuhan penduduk yang semakin pesat, Hal ini juga meningkatkan jumlah penderita penyakit CKD. Data yang didapatkan dari Global Burden of Disease, dilaporkan bahwa penyakit CKD menempati rangking ke-27 pada tahun 1990 dan rangking ke-18 pada tahun 2010 [3]. Menurut data Kementerian Kesehatan RI, 2 dari setiap 1.000 orang di Indonesia atau 499.800 orang menderita penyakit ginjal kronis pada tahun 2013. Prevalensi penyakit ginjal kronis meningkat seiring bertambahnya usia [2].</span></p>
<p><span class="font2">Data mining dapat diaplikasikan di bidang kesehatan misalnya mendiagnosis penyakit kanker payudara, penyakit jantung, penyakit diabetes dan lain-lain [5]. Klasifikasi adalah teknik data mining yang dapat digunakan dalam mendiagnosis penyakit ginjal kronis. Dimana data mining merupakan suatu metode yang digunakan untuk menemukan pola dari data yang digunakan untuk mencari solusi dari suatu masalah berdasarkan berbagai aturan proses [6].</span></p>
<p><span class="font2">Decision Tree Learning (DTL) merupakan salah satu teknik pembelajaran mesin (Machine Learning) yang menggunakan aturan klasifikasi berstruktur sekuensial hirarki dengan cara mempartisi himpunan data latih secara rekursif [7]. Pada peneliti terdahulu menggunakan Decision Tree sebagai algoritma</span></p>
<p><span class="font2">untuk prediksi metode penyakit kutil, didapatkan hasil akurasi sebesar 90% [8]. Juga pada penelitian sebelumnya mengenai perbandingan metode Decision Tree dengan Naïve Bayes untuk klasifikasi tumor otak, didapatkan akurasi algoritma Decision Tree lebih unggul dibandingkan Naïve Bayes yaitu 96% untuk Decision Tree dan 91% untuk Naïve Bayes [9].</span></p>
<p><span class="font2">Banyak hal yang bisa dilakukan untuk meningkatkan akurasi pada algoritma Decision Tree,salah satunya adalah dengan cara melakukan seleksi fitur. Seleksi fitur adalah cara menyeleksi subset fitur yang berlebihan dan kurang informatif pada dataset [2]. Pada penelitian terdahulu dengan menggunakan algoritma Decision Tree C4.5 dengan metode seleksi fitur Binary Particle Swarm Optimization (BPSO) berhasil meningkatkan hasil akurasi dengan hasil akhir akurasi 96,869% [2]. Penelitian yang lain menggunakan metode wrapper backward selection untuk klasifikasi penyakit diabetes, didapatkan hasil akurasi yang baik sebesar 96,7% [10].</span></p>
<p><span class="font2">Berdasarkan hasil uraian penelitian di atas, penelitian ini akan membahas sejauh apa hasil dari metode klasifikasi Decision Tree dan Forward Selection sebagai seleksi fitur untuk mendapatkan subset yang relevan sehingga mendapatkan kinerja yang baik. Ukuran pengujian menggunakan k-fold cross validation sebanyak 10-fold.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font2" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Metode Penelitian</span></h3></li></ul>
<p><span class="font2">Pada studi kasus ini, alur pemrosesan meliputi pengumpulan data, preprocessing data, seleksi fitur menggunakan Forward Selection, klasifikasi Decision Tree, dan evaluasi menggunakan k-fold validation.</span></p><img src="https://jurnal.harianregional.com/media/92582-1.jpg" alt="" style="width:257pt;height:372pt;">
<p><span class="font2">Gambar 1. Alur penelitian</span></p>
<p><span class="font2">Pada langkah awal data yang didapatkan akan melalui tahap preprocessing. Tahap preprocessing yang dilakukan meliputi pengisian </span><span class="font2" style="font-style:italic;">missing values</span><span class="font2"> dan encoding data categorical. Metode missing value dibedakan berdasarkan jenis fitur, fitur numerical menggunakan mean dan fitur kategorikal menggunakan modus/frekuensi kemunculan data terbanyak. Data yang sudah diolah, siap untuk masuk seleksi fitur forward selection. Subset data yang terbaik kemudian masuk ke dalam model klasifikasi Decision Tree sekali lagi untuk dibandingan dengan hasil sebelum dilakukan seleksi fitur. Terakhir divalidasi menggunakan k-fold validation sebanyak 10-fold untuk divalidasi apakah model yang dihasilkan tidak terjadi overfitting. Hasil akhirnya adalah data evaluasi model Decision Tree berupa akurasi, presisi, recall, dan f1 score.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark6"></a><span class="font2" style="font-weight:bold;"><a name="bookmark7"></a>2.1 &nbsp;&nbsp;&nbsp;Preprocessing Data</span></h3></li></ul>
<p><span class="font2">Seperti yang dijelaskan sebelumnya, pada tahapan preprocessing data ini dilakukan penanganan pada missing value. Proses pengisian missing value berdasarkan pada jenis data. Untuk data numerikal menggunakan metode mean dan untuk kategorikal menggunakan metode frekuensi kemunculan nilai terbanyak.Berikut implementasi untuk pengisian missing value.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. &nbsp;&nbsp;&nbsp;Mengisi Missing Value Data Kategorikal</span></p></li></ul>
<p><span class="font2">Pengisian data yang hilang dengan tipe data kategorikal bisa menggunakan rumus modus sebagai berikut</span></p>
<p><span class="font10">dl</span></p>
<p><span class="font10" style="font-style:italic;"><sup>M</sup>° <sup>= Tb + (</sup>d1+d<sup>c</sup></span></p>
<p><span class="font2">Untuk implementasi di sini mengguanakn bantuan dari library sklean yaitu SimpleImputer dengan memberikan parameter “most_frequent” sehingga data kategorikal yang hilang akan terisi sesuai dengan kemunculan data terbanyak.</span></p>
<p><span class="font1" style="font-style:italic;">from</span><span class="font1"> sklearn.impute </span><span class="font1" style="font-style:italic;">import</span><span class="font1"> SimpleImputer</span></p>
<p><span class="font1" style="text-decoration:underline;">impu<sub>ι</sub>te</span><span class="font1">r = SimpleImputerCstrategy='most frequent’) </span><span class="font1" style="text-decoration:underline;">imputer</span><span class="font1"> imputer fit(data[cat cols])</span></p>
<p><span class="font1">data cat<sub>-</sub>cols] = imputer transform:data[cat<sub>-</sub>cols])</span></p>
<p><span class="font2">Gambar 2. Penerapan missing value kategorikal dengan modus</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">b. &nbsp;&nbsp;&nbsp;Encoding</span></p></li></ul>
<p><span class="font2">Pada tahapan encoding, data yang bersifat kategorikal string diubah menjadi data numerik yang mewakili data kategorikal tersebut.</span></p><img src="https://jurnal.harianregional.com/media/92582-2.jpg" alt="" style="width:168pt;height:133pt;">
<p><span class="font2">Gambar 3. Sebelum encoding data</span></p><img src="https://jurnal.harianregional.com/media/92582-3.jpg" alt="" style="width:247pt;height:126pt;">
<p><span class="font2">Gambar 4. Penerapan missing value data numerikal</span></p><img src="https://jurnal.harianregional.com/media/92582-4.jpg" alt="" style="width:157pt;height:140pt;">
<p><span class="font2">Gambar 5. Setelah encoding data</span></p>
<p><span class="font2">Seperti terlihat pada gambar di atas, data telah berubah dari kategorikal menjadi numerikal. Data encoding sangat penting karna model Decision Tree hanya mengenali data yang numerik.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">c. &nbsp;&nbsp;&nbsp;Mengisi Missing Value Data Numerikal</span></p></li></ul>
<p><span class="font2">Data yang hilang menggunakan rumus mean.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font10">„ &nbsp;. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;. &nbsp;&nbsp;&nbsp;&nbsp;∑</span><span class="font7">Γ=</span><span class="font10"><sub>1</sub>Xi</span></p></li></ul>
<p><span class="font10" style="font-style:italic;">Xrata</span><span class="font10"> - </span><span class="font10" style="font-style:italic;">rata</span><span class="font7"> = ———</span></p>
<p><span class="font2">Berikut ini adalah implementasi missing value menggunakan bantuan dari library sklearn.</span></p>
<p><span class="font1" style="font-style:italic;">from</span><span class="font1"> sklearn.impute </span><span class="font1" style="font-style:italic;">import</span><span class="font1"> SimpleImputer </span><span class="font1" style="text-decoration:underline;">imputer</span><span class="font1"> = SimpleImputerC </span><span class="font1" style="font-style:italic;">strategy='</span><span class="font1">mean') </span><span class="font1" style="text-decoration:underline;">imputer</span><span class="font1"> = imputer.fit(data) all_col = data.columns</span></p>
<p><span class="font1">data = pd.DataFrameCdata=Imputer.transform(data), coZumns=allcol) df = data</span></p>
<p><span class="font2">Gambar 6. Penerapan missing value data numerikal</span></p>
<p><span class="font2">Dengan SimpleImputer, penerapan mean untuk missing value bisa digunakan dengan memberikan parameter ‘mean’ sehingga data-data yang hilang pada semua kolom yang bertipe numerikal akan terisi.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark8"></a><span class="font2" style="font-weight:bold;"><a name="bookmark9"></a>2.2 &nbsp;&nbsp;&nbsp;Decision Tree</span></h3></li></ul>
<p><span class="font2">Algortima Decision Tree merupakan suatu metode pengklasifikasian yang menggunakan contoh pohon, menyatakan node yang menggambarkan tiap atribut, yang mana daun menggambarkan tiap kelas, juga setiap cabangnya menggambarkan nilai dari tiap kelas. Node akar menyatakan node yang berada paling atas dari pohon. Setiap node ini menggambarkan node pembagi, yang mana tiap node ini merupakan satu masukan dan memiliki sedikitnya dua keluaran [11]. Leaf node adalah node terakhir, hanya mempunyai satu masukan, dan tidak mempunyai keluaran. Pohon keputusan pada tiap leaf node menyatakan label tiap kelas. Pohon keputusan pada tiap cabangnya menyatakan keadaan yang harus diisi dan tiap puncak pohonnya menggambarkan nilai kelas data [12].</span></p>
<p><span class="font2">Pada kebanyakan kasus, decision tree bisa melakukan klasifikasi yang targetnya bersiifat biner atau disebut klasifikasi biner, contoh untuk memprefiksi jawaban ya atau tidak.</span></p>
<p><span class="font2">Berikut adalah beberapa jenis klasifikasi biner:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. &nbsp;&nbsp;&nbsp;Classification Trees (</span><span class="font2" style="font-style:italic;">Yes/No types</span><span class="font2">)</span></p></li></ul>
<p><span class="font2">Jenis klasifikasi ini adalah contoh klasifikasi untuk memberikan prediksi jawaban ya atau tidak, hujan atau tidak, terinfeksi atau tidak, dan lain sebagainya.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">b. &nbsp;&nbsp;&nbsp;Regression Tree (</span><span class="font2" style="font-style:italic;">Continuous data types</span><span class="font2">)</span></p></li></ul>
<p><span class="font2">Jenis klasifikasi ini adalah klasifikasi untuk tipe data kontiyu seperti angka bilangan riil.</span></p>
<p><span class="font2">Dari sekian banyak pohon keputusan yang dibuat, salah satu algoritma pohon keputusan yang populer adalah Algoritma ID3. ID3 adalah singkatan dari Iterative Dichotomiser 3. Ada beberapa definisi yang membangun algoritma ID3.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. Entropy</span></p></li></ul>
<p><span class="font2">Entropy atau juga disebut Shannon Entropy dilambangkan dengan H(S) untuk himpunan S yang terbatas adalah ukuran jumlah ketidakpastian atau suatu nilai acak dalam data.</span></p>
<p><span class="font10" style="font-style:italic;">H(S)=</span><span class="font10"> ∑ </span><span class="font10" style="font-style:italic;">p(x)log</span><span class="font7"> i</span></p>
<p><span class="font8" style="font-style:italic;font-variant:small-caps;">⅛<sup>j</sup>x</span><span class="font10" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>p(x</sup></span><span class="font9" style="font-style:italic;">)</span></p>
<div>
<p><span class="font2">b.</span></p>
</div><br clear="all">
<p><span class="font2">Information Gain</span></p>
<p><span class="font2">Nama lain dari Information Gain adalah Kullback-Leibler divergence dilambangkan IG(S,A) untuk semua himpunan S adalah peubah efektif dalam entropi setelah memutuskan atribut A tertentu. Pengukuran perubahan ini relative dalam entropi sehubung dengan variable independent.</span></p>
<p><span class="font9" style="font-style:italic;">n</span></p>
<p><span class="font10" style="font-style:italic;">IG(S,A') = H(S) - ∑P(x)*H(x) </span><span class="font9" style="font-style:italic;">i=0</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark10"></a><span class="font2" style="font-weight:bold;"><a name="bookmark11"></a>2.3 Feature Selection</span></h3></li></ul>
<p><span class="font2">Feature Selection atau yang sering disebut juga sebagai attribute selection merupakan proses menemukan subset hasil seleksi fitur dari suatu dataset. Feature selection dipakai pada bidang statistika, dan data mining[13].</span></p>
<p><span class="font2">Forward selection adalah salah satu metode dalam Feature Selection. Feature selection adalah metode yang penting untuk menghasilkan klasifikasi yang baik. Tanpa feature selection proses komputasi dan performansi model menjadi buruk. Tujuan dari feature selection adalah membuang atribut yang tidak relevan[13].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. Forward Selection</span></p></li></ul>
<p><span class="font2">Metode Forward Selection mengadopsi prinsip regresi Linear. Forward Selection adalah salah satu model wrapper yang digunakan mereduksi atribut dataset [13].</span></p>
<p><span class="font2">Proses pencarian attribute dengan forward selection diawali dengan empty model, selanjutnya tiap variabel dimasukan hingga kriteria kombinasi model attribute terpenuhi dengan baik. Berikut adalah pseudo code dari forward selection:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">1. &nbsp;&nbsp;&nbsp;Membuat empty set: </span><span class="font10" style="font-style:italic;">Y<sub>k</sub> =</span><span class="font10"> {0}, </span><span class="font10" style="font-style:italic;">k = 0</span></p></li>
<li>
<p><span class="font2">2. &nbsp;&nbsp;&nbsp;Memilih feature terbaik: </span><span class="font10" style="font-style:italic;">X<sup>+</sup> = argmax<sub>x+eγk</sub>[J(Y<sub>k</sub> + X<sup>+</sup>)]</span></p></li>
<li>
<p><span class="font2">3. &nbsp;&nbsp;&nbsp;Jika </span><span class="font10" style="font-style:italic;">[J(Y</span><span class="font9" style="font-style:italic;">k </span><span class="font10" style="font-style:italic;">+ X<sup>+</sup>)] &gt;&nbsp;J(Y<sub>k</sub>):</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font2" style="font-style:italic;">•</span><span class="font2"> &nbsp;&nbsp;&nbsp;Update </span><span class="font10" style="font-style:italic;">Y<sub>k</sub> + 1=</span><span class="font10"> Y</span><span class="font7"><sub>k</sub> </span><span class="font10">+ X</span><span class="font7"><sup>+</sup></span></p></li>
<li>
<p><span class="font2">•</span><span class="font10" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;k = k + 1</span></p>
<div>
<p><span class="font2">• Kembali ke step-2</span></p>
</div><br clear="all">
<div>
<h3><a name="bookmark12"></a><span class="font2" style="font-weight:bold;"><a name="bookmark13"></a>2.4 K-Fold Cross Validation</span></h3>
<p><span class="font2">Cross validation atau disebut validasi silang adalah salah satu metrik untuk mengukur hasil dari algoritma klasifikasi. Sedangkan K-fold validation adalah salah satu cara untuk mengetahui rata-rata keberhasilan sebuah sistem klasifikasi. K-fold validation akan mengacak sebuah dataset secara silang sehingga sistem diuji untuk beberapa dataset yang sudah diacak. Tujuanya adalah untuk menghindari dominasi data pada pembelajaran model klasifikasi. Validasi k-fold akan dimulai dengan membagi beberapa data menjadi n-fold yang diinginkan. Demikian jika data dibagi menjadi 5 akan menghasilkan 5 partisi data dengan ukuran yang sama D1, D2, D3. Kemudian dilakukan proses test dan training sebanyak jumlah fold. Setiap iterasi ke-i, data partisi n akan menjadi dataset uji dan sisanya menjadi dataset pelatihan[15].</span></p>
<p><span class="font2">Setiap iterasi dihitung Accuracy, Presisi, Recall, dan F1 Scorenya menggunakan rumus berikut:</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">Accuracy =</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">∑ ccorrect classification test data ∑ total test data</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">Precision =</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">∑ ccorrect classification test data ∑ total data predicted</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">Recall =</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">∑ ccorrect classification test data ∑ total test data on a specifi c class</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">F1 Score</span><span class="font10"> = 2 *</span></p>
</div><br clear="all">
<div>
<p><span class="font10" style="font-style:italic;">Recall * Precision</span></p>
<p><span class="font10" style="font-style:italic;">Recall + Precision</span></p>
</div><br clear="all">
<div>
<h3><a name="bookmark14"></a><span class="font2" style="font-weight:bold;"><a name="bookmark15"></a>3. Hasil dan Pembahasan</span></h3>
<p><span class="font2">Pada hasil pengujian, data yang digunakan berjumlah 400 entitas dengan 24 fitur dan 1 label dengan 2 class yaitu CKD (Chronic Kidney Disease) dan Not CKD (Not Chronic Kidney Disease). Pada proses seleksi fitur forward selection yang dibantu dengan library mlxtend dari python, setiap fitur yang ditambahkan kepada subset dilakukan perhitungan cross validation sebanyak 10-Fold. Seleksi melakukan iterasi sebanyak 24 kali sesuai dengan jumlah fiturnya. Pada setiap iterasi, kombinasi fitur yang menghasilkan akurasi terbaik dari cross validation akan dipilih hingga pada akhirnya semua fitur berhasil dikombinasikan. Tahap akhir dari semua iterasi akan menghasilkan nilai rata-rata cross validation yang kemudian akan di seleksi dengan mengambil nilai rata-rata cross validation tertinggi. Hasil fitur-fitur yang terpilih menggunakan forward selection seperti pada Tabel 1.</span></p>
</div><br clear="all">
<div>
<p><span class="font2" style="font-weight:bold;">Table 1. </span><span class="font2">Fitur terpilih</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Algoritma</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Index Features</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Decision Tree</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">0, 2, 3, 5, 11, 13, 14, 15, 18</span></p></td></tr>
</table>
</div><br clear="all"></li></ul>
<p><span class="font2" style="font-weight:bold;">Table 2. </span><span class="font2">Keterangan Fitur</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Index</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Deskripsi</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">0</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">age</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">2</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Specific gravity</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">3</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Albumin</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">5</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Red blood cells</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">11</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Serum creatinine</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">13</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Potassium</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">14</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Haemogllobin</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">15</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Packed cell volume</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">18</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Hypertension</span></p></td></tr>
</table>
<p><span class="font2">Untuk pengujian Decision Tree tanpa menggunakan seleksi fitur, didapatkan nilai akurasi, Presisi, Recall, F1 Score tanpa seleksi fitur, hasil cross validation seperti tertera pada Tabel 3.</span></p>
<p><span class="font6">Hasil mean akurasi cross validation :97.25 %</span></p>
<p><span class="font6">Hasil mean Presisi cross validation :96.16666666666666 %</span></p>
<p><span class="font6">Hasil mean Recall cross validation :96.66666666666666 S</span></p>
<p><span class="font6">Hasil mean Fl Score cross validation :96.31723078552889 %</span></p>
<p><span class="font6">0.00298953, 0.00299048, 0.00199318, 0.00199366, 0.00299001]),</span></p>
<p><span class="font6">'test_accuracy<sup>,</sup>: array( [0.975, 0.975, 0.975, 1. &nbsp;&nbsp;&nbsp;, 0.95 , 0.95 , 1. &nbsp;&nbsp;&nbsp;, 0.95 , 0.975,</span></p>
<p><span class="font6">0.975]),</span></p>
<p><a href="#bookmark16"><span class="font6">'test_precision<sup>,</sup>: array([0.9375 &nbsp;&nbsp;&nbsp;, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.9375 &nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,1.,</span></a></p>
<p><a href="#bookmark17"><span class="font6">0.93333333, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.93333333, 0.9375 &nbsp;&nbsp;&nbsp;, 0.9375]),</span></a></p>
<p><span class="font6">'test,recall': array([l. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.93333333, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.86666667,</span></p>
<p><a href="#bookmark18"><span class="font6">0.93333333, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.93333333, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 1.]),</span></a></p>
<p><span class="font6">'test_fl_score': array( [0.96774194, 0.96551724, 0.96774194, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.92857143,</span></p>
<p><span class="font6">0.93333333, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.93333333, 0.96774194, 0.96774194])}</span></p>
<p><span class="font2">Gambar 7. Output 10-Fold Validation </span><span class="font2" style="font-weight:bold;">sebelum </span><span class="font2">seleksi fitur</span></p>
<p><span class="font2" style="font-weight:bold;">Table 3. </span><span class="font2">Akurasi Decision Tree </span><span class="font2" style="font-weight:bold;">sebelum </span><span class="font2">seleksi fitur</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:top;">
<p><span class="font2">Fold</span></p></td><td colspan="4" style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Ukuran evaluasi (Rata-Rata Fold)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Akurasi</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Presisi</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">F1 Score</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,75</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,77</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,33</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,55</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,75</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,77</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">95</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">86,67</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">92,85</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">6</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">95</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,33</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,33</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,33</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">95</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,33</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">09,33</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,33</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">9</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,75</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,77</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,75</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,77</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Rata-Rata</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">97,25%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">96,16%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">96.66%</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">96.31%</span></p></td></tr>
</table>
<p><span class="font2">Pada hasil keseluruhan fitur sebelum diseleksi mendapatkan hasil yang lumayan baik, akurasi rata-rata yang diperoleh dari setiap fold adalah 97,25 %. Kemudian pada Tabel 4 di bawah ini adalah hasil dari Decision Tree dengan seleksi fitur forward selection.</span></p>
<p><span class="font7">Hasil mean akurasi cross validation :99.5 %</span></p>
<p><span class="font7">Hasil mean Presisi cross validation :98.75 %</span></p>
<p><span class="font7">Hasil mean Recall cross validation :100.0 %</span></p>
<p><span class="font7">Hasil mean Fl Score cross validation :99.35483870967741 %</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7"><sup>,</sup>test_accuracy<sup>,</sup>: array([l. , 1. &nbsp;&nbsp;&nbsp;, 1. &nbsp;&nbsp;&nbsp;, 1. &nbsp;&nbsp;&nbsp;, 1. &nbsp;&nbsp;&nbsp;, 1. &nbsp;&nbsp;&nbsp;, 0.975, 0.975, 1.</span></p></li></ul>
<p><span class="font7">1. ]),</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7"><sup>,</sup>test_precision<sup>,</sup>: array([l. , 1. &nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;, 0.9375,</span></p></li></ul>
<p><span class="font7">0.9375, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, I- ]),</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7"><sup>,</sup>test_recall<sup>,</sup>: array([l., 1., 1., 1.<sub>1</sub> 1., 1., l.<sub>j</sub> 1., 1., 1.]),</span></p></li></ul>
<p><span class="font7">'test_fl_score': array([l. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sub>r</sub> 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,1.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7">1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;, 0.96774194, 0.96774194, 1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;,1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;])}</span></p></li></ul>
<p><span class="font2">Gambar 8. Output 10-Fold Validation </span><span class="font2" style="font-weight:bold;">setelah </span><span class="font2">seleksi fitur</span></p>
<p><span class="font2" style="font-weight:bold;">Table 4. </span><span class="font2">Akurasi Decision Tree </span><span class="font2" style="font-weight:bold;">sesudah </span><span class="font2">seleksi fitur</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:top;">
<p><span class="font2">Fold</span></p></td><td colspan="4" style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Ukuran evaluasi (Rata-Rata Fold)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Akurasi</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Presisi</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font2">F1 Score</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">1</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">2</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">3</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">4</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">5</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">6</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,75</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,77</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">97,5</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">93,75</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">100</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">96,77</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">9</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">10</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td><td style="vertical-align:top;">
<p><span class="font2">100</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Rata-Rata</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">99.5%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">98,75%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">100%</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">99,35%</span></p></td></tr>
</table>
<p><span class="font2">Untuk melihat hasil perbandingan antara Decision Tree sebelum dan sesudah dilakukan seleksi fitur, bisa dilihat pada histogram pada Gambar 7.</span></p>
<h2><a name="bookmark19"></a><span class="font5"><a name="bookmark20"></a>Perbandingan Decision Tree</span></h2>
<p><span class="font4">102</span></p><img src="https://jurnal.harianregional.com/media/92582-5.jpg" alt="" style="width:259pt;height:71pt;">
<p><span class="font0">■</span><span class="font4"> Decision Tree </span><span class="font0">■</span><span class="font4"> Decision Tree + Forward Selection</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">Gambar 7. Perbandingan Decision Tree</span></p></li></ul>
<p><span class="font2">Pada gambar histogram di atas bisa terllihat bahwa metode seleksi fitur bisa meningkatkan akurasi Decision Tree lebih baik daripada hanya menggunakan Decision Tree. Jika dilihat dari perbandingan akurasi, Decision Tree tanpa seleksi fitur mendapatkan nilai sebesar 97,25%, sedangkan setelah dilakukan seleksi fitur hasilnya meningkat sebsar 2,25% menjadi 99,5% di mana nilai akurasi ini termasuk ke dalam </span><span class="font2" style="font-style:italic;">excellent accuracy</span><span class="font2">. Terbukti metode seleksi fitur forward selection berhasil meningkatkan akurasi Decision Tree pada kasus klasifikasi penyakit ginjal kronis.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark21"></a><span class="font2" style="font-weight:bold;"><a name="bookmark22"></a>4. &nbsp;&nbsp;&nbsp;Kesimpulan</span></h3></li></ul>
<p><span class="font2">Hasil kesimpulan pada penelitian ini bahwa metode seleksi fitur forward selection telah berhasil meningkatkan akurasi dari algoritma Decision Tree dengan hasil akurasi, presisi, recall, dan f1 score secara berurutan adalah 99.5%, 98,75%, 100%, 99,35% dengan kombinasi subset fitur yang terbaik sebanyak 9 fitur dari 24. Dari penelitian ini, peneliti juga menyimpulkan bahwa peran dari preprocessing data berpengaruh dalam mendapatkan hasil akurasi karna terdapat missing value. Jadi peneliti menyimpulkan bahwa pada studi kasus ini metode preprocessing data untuk missing value sangat penting untuk dilakukan.</span></p>
<p><span class="font2">Saran untuk penelitian selanjutnya yaitu bisa menerapkan metode missing value lainnya dan membandingkan hasilnya. Juga bisa menerapkan metode seleksi fitur lainnya seperti ekstraksi fitur (PCA) atau Seleksi fitur berdasarkan korelasi.</span></p>
<h3><a name="bookmark23"></a><span class="font2" style="font-weight:bold;"><a name="bookmark24"></a>References</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;I.W. Gamadarenda, and I.Waspada, &quot;IMPLEMENTASI DATA MINING UNTUK DETEKSI PENYAKIT GINJAL KRONIS (PGK) MENGGUNAKAN K-NEAREST NEIGHBOR (KNN) DENGAN BACKWARD ELIMINATION&quot; </span><span class="font2" style="font-style:italic;">Jurnal Teknologi Informasi dan Ilmu Komputer (JTIIK),</span><span class="font2"> Vol. 7, No. 2. 417-426, 2020.</span></p></li>
<li>
<p><span class="font2">[2] &nbsp;&nbsp;&nbsp;I.G.A.Mahardika Pratama, dkk, “Diagnosis Penyakit Ginjal Kronis dengan Algoritma C4.5, K-Means dan BPSO” Jurnal-Elektronik-Ilmu-Komputer-Udayana, Volume 10, No 4, 2022.</span></p></li>
<li>
<p><span class="font2">[3] &nbsp;&nbsp;&nbsp;I. Fadilla, P. P. Adikara, and R. Setya Perdana, “Klasifikasi Penyakit Chronic Kidney Disease (CKD) Dengan Menggunakan Metode Extreme Learning Machine (ELM),” J. Pengemb. Teknol. Inf. dan Ilmu Komput., vol. 2, no. 10, pp. 3397–3405, &nbsp;&nbsp;2018, [Online]. Available:</span></p></li></ul>
<p><a href="https://www.researchgate.net/publication/323365845"><span class="font2">https://www.researchgate.net/publication/323365845</span></a><span class="font2">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;E.A.Kurnianto, dkk, “Klasifikasi Penderita Penyakit Ginjal Kronis Menggunakan Algoritme Support Vector Machine (SVM)” Jurnal Pengembangan Teknologi Informasi dan Ilmu Komputer, Vol. 2, No. 12, 2018.</span></p></li>
<li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;D. T. Larose, Discovering Knowledge in Data: An Introduction to Data Mining. United States of America: John Wiley &amp;&nbsp;Sons, Inc, 2005.</span></p></li>
<li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;I. Handayani, “Penyakit Disk Hernia Dan Spondylolisthesis Dalam Kolumna Vertebralis,” vol. 1, no. 2, pp. 83–88, 2019, doi: 10.12928/JASIEK.v13i2.xxxx.</span></p></li>
<li>
<p><span class="font2">[7] &nbsp;&nbsp;&nbsp;Suyanto, Machine Learning Tingkat Dasar dan Lanjut, 1st ed. Bandung: Informatika Bandung, 2018.</span></p></li>
<li>
<p><span class="font2">[8] &nbsp;&nbsp;&nbsp;Fitriyani, and T.Arifin, “IMPLEMENTASI GREEDY FORWARD SELECTION UNTUK PREDIKSI METODE PENYAKIT KUTIL MENGGUNAKAN DECISION TREE”, Jurnal Sains dan Teknologi, Vol.9 No. 1, 2020.</span></p></li>
<li>
<p><span class="font2">[9] &nbsp;&nbsp;&nbsp;S.D.Kamil, “PERBANDINGAN METODE DECISION TREE DENGAN NAIVE BAYES DALAM KLASIFIKASI TUMOR OTAK CITRA MRI”, UPV Veteran Jakarta, 2022.</span></p></li>
<li>
<p><span class="font2">[10]</span></p></li></ul>
<p><span class="font2">iratama, M.Abid, “OPTIMASI ALGORITMA DATA MINING MENGGUNAKAN BACKWARD ELIMINATION UNTUK KLASIFIKASI PENYAKIT DIABETES”, Universitas Amikom, 2022.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[11]</span></p></li></ul>
<p><span class="font2">H.Ferdian, and H.Seng. (2017). Penerapan Algoritma C4.5 untuk Memprediksi Penerimaan Calon</span></p>
<p><span class="font2">Pegawai Baru di PT WISE. Palembang: Jatisi, Vol. 3, No.2.H</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[12]</span></p></li></ul>
<p><span class="font2">Robianto, dkk, “PENERAPAN METODE DECISION TREE UNTUK MENGKLASIFIKASIKAN MUTU BUAH JERUK BERDASARKAN FITUR WARNA DAN UKURAN”, Coding:Jurnal Komputer dan Aplikasi, Volume 09, No. 01, 2021</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[13]</span></p></li></ul>
<p><span class="font2">H.B.Sasongko , and O.Arifin, “IMPLEMENTASI METODE FORWARD SELECTION PADA ALGORITMA SUPPORT VECTOR MACHINE (SVM) DAN NAIVE BAYES CLASSIFIER KERNEL DENSITY (STUDI KASUS KLASIFIKASI JALUR MINAT SMA)”, Jurnal Teknologi Informasi dan Ilmu Komputer (JTIIK), Vol. 6, No. 4, 2019</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[14]</span></p></li></ul>
<p><span class="font2">Pedregosa </span><span class="font2" style="font-style:italic;">et al, “Scikit-learn: Machine Learning in Python”,</span><span class="font2"> JMLR 12, pp. 2825-2830, 2011.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[15]</span></p></li></ul>
<p><span class="font2">I K.S Putri Rahayua1, and I K.A. Mogi, “Implementation of K-Nearest Neighbor Algorithm in Heart Disease Classification”, Jurnal Elektronik Ilmu Komputer Udayana, Volume 10 No. 1, 2021.</span></p>
<p><span class="font2">286</span></p>