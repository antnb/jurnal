---
layout: full_article
title: "SVM Optimization Based on PSO and AdaBoost to Increasing Accuracy of CKD Diagnosis"
author: "Amanah Febrian Indriani, Much Aziz Muslim"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-48406 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-48406"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-48406"  
comments: true
---

<p><span class="font2" style="font-weight:bold;">LONTAR KOMPUTER VOL. 10, NO. 2 AUGUST 2019</span></p>
<p><span class="font2" style="font-weight:bold;">DOI : 10.24843/LKJITI.2019.v10.i02.p06</span></p>
<p><span class="font2" style="font-weight:bold;">Accredited B by RISTEKDIKTI Decree No. 51/E/KPT/2017</span></p>
<p><span class="font2" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font2" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>SVM Optimization Based on PSO and AdaBoost to Increasing Accuracy of CKD Diagnosis</span></h1>
<p><span class="font2">Amanah Febrian Indriani<sup>a1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Much Aziz Muslim<sup>a2</sup></span></p>
<p><span class="font2"><sup>a</sup>Department of Computer Science, Universitas Negeri Semarang Semarang, Indonesia</span></p>
<p><a href="mailto:1amanahfebrian@students.unnes.ac.id"><span class="font2"><sup>1</sup>amanahfebrian@students.unnes.ac.id</span></a></p>
<p><a href="mailto:2a212muslim@yahoo.com"><span class="font2"><sup>2</sup>a212muslim@yahoo.com</span></a></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font2" style="font-style:italic;">Classification is data mining techniques which used for the purposes of diagnosis in the medical field as measured by the high accuracy produced. The accuracy of classification algorithm is influenced by the use of features and dimensions in dataset. In this study, Chronic Kidney Disease (CKD) dataset was used where the data is one of the high dimension datasets. Support Vector Machine (SVM) algorithm is used because its ability to handle high-dimensional data. In the dataset, it consists of 24 attributes and 1 class which if all are used results accuracy of classification will be diminished. Method for selecting features with Particle Swarm Optimization (PSO) is applied to reduce redundant features and produce optimal features. In addition, ensemble AdaBoost also applied in this research to increase performance of entirety classification algorithm. The results showed that the optimization of SVM algorithm by using PSO as a selection and ensemble feature of AdaBoost with an average of selected features of 18 features could increase the accuracy of 36.20% to 99.50% in the diagnosis of CKD compared to the SVM algorithm without optimization only resulting in accuracy 63.30%. This research can be used as a reference for further research in focusing on the preprocessing stage.</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font2" style="font-style:italic;">Data Mining, Support Vector Machine, Particle Swarm Optimization, AdaBoost, Chronic Kidney Disease</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font2">Currently, from various sources data can be collected and become very large. If this data is not utilized, it will only become a pile of useless data. Large and hidden databases can be extracted into useful knowledge with data mining techniques [1] [2]. Therefore, data mining can be considered as a tool to obtain knowledge from raw data, and the data that does not mean in the medical field. There are 3 stages of data mining, namely: data processing, data modeling, and processing of data posts. In data modeling, data mining tasks are divided into two, namely: predictive/classification algorithms and regression algorithms that are learned through a supervised learning process [3]. From a patient’s medical record, data mining can be used to predict disease with classification [4].</span></p>
<p><span class="font2">In the medical field, there are two types of kidney failure, namely Chronic and Acute Kidney Disease which occurs when the kidneys cannot filter waste from the blood [5]. Patients with CKD are increasing as the population grows rapidly throughout the world, even within 10 years, the Global Burden of Disease notes that Chronic Kidney Disease (CKD) disease rises 9 ranks from the initial rank 27 to 18th place [6]. Medical examinations performed on patients produce very large data. However, in a very large volume of data, there are still some missing data, therefore good classification techniques are needed and produce high accuracy for detecting chronic kidney disease based on datasets [7].</span></p>
<p><span class="font2">There are several classification techniques in data mining include Neural Network (NN), Decision Tree (DT), Logistic Regression (LR), Naïve Bayesian (NB), and Support Vector Machine (SVM) [8]. SVM is a learning machine that utilizes the space of linear function hypotheses in high dimensional feature space, based on optimization theory obtained from statistic learning theory [9]. SVM has the concept of looking for hyperplane based on the best vector support and margins that function as the boundary of two classes and have been successfully applied to many classification cases with high accuracy [10].</span></p>
<p><span class="font2">To increase the accuracy of the classification algorithm, an ensemble technique used to combining several weak classifiers using the AdaBoost algorithm [11]. One of the most promising algorithms with convergence fast and easy to implement is Adaboost, because AdaBoost does not require knowledge from the weak learner and can be easily combined with other methods such as SVM [12].</span></p>
<p><span class="font2">Another way to increase accuracy is by selecting features at the preprocessing stage. Feature selection is a data preprocessing step that is used to delete some features in a data set so that the process runs faster, and data visualization is easier [13]. Feature selection methods usually implicate heuristic or random search strategies to avoid complexity [14]. PSO is a heuristic algorithm that has been proven to provide optimization of value [15]. In some cases, it has been proven that PSO is more competitive when compared to genetic algorithms to overcome the feature selection problem [16].</span></p>
<p><span class="font2">The goals of this study were to improve the accuracy of the SVM algorithm that had been optimized using the PSO algorithm as an Adaboost selection and ensemble feature in the diagnosis of CKD.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font2" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Reseach Methods</span></h2></li></ul>
<p><span class="font2">The combination of several proposed algorithms aims to improve the accuracy of the diagnosis of Chronic Kidney Disease. Steps that will be carried out in this experiment include preprocessing, feature selection using PSO, and SVM classification with AdaBoost ensemble.</span></p><img src="https://jurnal.harianregional.com/media/48406-1.jpg" alt="" style="width:203pt;height:109pt;">
<p><span class="font2" style="font-weight:bold;">Figure 1. </span><span class="font2">Research Method</span></p>
<p><span class="font2">The work step in this study began by inputting the CKD dataset. Then the data will be processed in the data preprocessing stage, was by cleaning data. Data cleaning is done by removing the missing value in the CKD dataset. Still, in the preprocessing stage, the data that has been filled in with the missing value will then be feature selection using the PSO algorithm. Based on the selected features, the classification process will be carried out with the SVM algorithm combined with AdaBoost. Then the classification model is tested using data testing and evaluated using a confusion matrix to produce accuracy values. The flowchart of the research method carried out in Figure 2.</span></p><img src="https://jurnal.harianregional.com/media/48406-2.jpg" alt="" style="width:318pt;height:511pt;">
<p><span class="font2" style="font-weight:bold;">Figure 2. </span><span class="font2">Flowchart Support Vector Machine Algorithm with PSO and Adaboost</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font2" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Preprocessing</span></h2></li></ul>
<p><span class="font2">The dataset used in this study is a CKD dataset that was collected and uploaded by the Apollo hospital, India in 2015 at the UCI Machine Learning Repository. The collected data amounts to 400 instances and 25 attributes consist of 11 numeric attributes and 14 nominal attributes. The attribute description of the CSD dataset can be seen in Table 1.</span></p>
<table border="1">
<tr><td colspan="2" style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Tabel 1. </span><span class="font2">Description of Chronic Kidney Disease Dataset</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Features</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">Type</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Age (Age)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Blood pressure (Bp)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Appetite (appet)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Specific gravity (Sg)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Albumin (Al)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Sugar (Su)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Red blood cells (Rbc)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Pus cell (Pc)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Pus cell clumps (Pcc)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Bacteria (Ba)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Hypertension (Htn)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Haemoglobin (Hemo)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Serum creatinine (Sc)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Blood glucoses (Bgr)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Blood urea (Bu)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Coronary artery disease (Cad)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font2">Sodium (Sod)</span></p></td><td style="vertical-align:top;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Potassium (Pot)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Pedal edema (Pe)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Packed cell volume (Pcv)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">White blood cell (Wbcc)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Red blood cell count (Rc)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Numeric</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Diabetes mellitus (Dm)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Anemia (Ane)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Nominal</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Class (class)</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">Nominal</span></p></td></tr>
</table>
<p><span class="font2">In the preprocessing stage, nominal type attributes will be transformed into numeric. There are a number of 14 nominal attributes that will be transformed into numeric type attributes. Then, of the 400 instances in the CKD dataset, 250 of them are labeled with the </span><span class="font2" style="font-style:italic;">ckd</span><span class="font2"> class while the other 150 are labeled the </span><span class="font2" style="font-style:italic;">notckd</span><span class="font2"> class. In the CKD dataset, there are more than 50% of the missing value, so it is necessary to handle missing values to produce higher accuracy. Filling in the missing value is done by the mode method, namely by replacing the empty value with the most frequency of each attribute.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font2" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;PSO for Feature Selection</span></h2></li></ul>
<p><span class="font2">The PSO algorithm in the selection of features tries to get the best composition of features in a problem space. PSO has the ability to get the optimal subset by finding the best position around the local position and global position [17].</span></p>
<p><span class="font2">Although PSO was initially introduced to optimize real number problems, now PSO can also display discrete or qualitative differences between variables, it is called Binary Particle Swarm Optimization (BPSO). In BPSO, each particle will be represented in binary variables 0 or 1. Then, velocity is transformed into a change in probability, that is, the probability of a binary variable takes a value of 1. However, the velocity must be limited to the range [0,1] [18].</span></p>
<p><span class="font2">There are stages in the BSPO algorithm as feature selection by initializing random positions and velocities of particles. Then, the fitness value of each particle in the population will be evaluated. After that, unite if the fitness value of particle i is less than </span><span class="font2" style="font-style:italic;">pBest</span><span class="font2"> value, then </span><span class="font2" style="font-style:italic;">pBest</span><span class="font2"> from particle i to particle position i, but if </span><span class="font2" style="font-style:italic;">pBes</span><span class="font2">t is updated and fitness value is less than current </span><span class="font2" style="font-style:italic;">gBest</span><span class="font2"> value, set </span><span class="font2" style="font-style:italic;">gBest</span><span class="font2"> to </span><span class="font2" style="font-style:italic;">pBest</span><span class="font2"> at this time from particle i. Then, update the speed and position of the</span></p>
<p><span class="font2">particle. If the best fitness value or iteration is fulfilled if it has not returned to the fitness calculation stage then stop iteration [19].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font2" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Adaboost Ensemble</span></h2></li></ul>
<p><span class="font2">Ensemble learning usually consists of several basic learning algorithms that are usually generated from training data. Ensemble methods are widely used because they can improve the basic learning algorithm and make highly accurate predictions [20]. The Ensemble method can be used to improve overall accuracy by studying and combining a series of individual classifier models [21].</span></p>
<p><span class="font2">Adaptive boosting (AdaBoost) is one of several variants in the boosting algorithm. AdaBoost is an Ensemble of learning that is often used in boosting algorithms [22]. Adaboost and its variants have been successfully applied in several fields because of its strong theoretical basis and great simplicity. The steps of the Adaboost algorithm are [23]:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. &nbsp;&nbsp;&nbsp;Input: A collection of training samples with labels </span><span class="font2" style="font-style:italic;">{(x</span><span class="font6" style="font-style:italic;">∣</span><span class="font2" style="font-style:italic;">,y</span><span class="font0" style="font-style:italic;">i</span><span class="font2" style="font-style:italic;">),...,(x</span><span class="font0" style="font-style:italic;">N</span><span class="font2" style="font-style:italic;">, </span><span class="font2" style="font-style:italic;font-variant:small-caps;">y</span><span class="font1" style="font-style:italic;font-variant:small-caps;">n</span><span class="font2" style="font-style:italic;font-variant:small-caps;">)}</span><span class="font2">, a basic learning algorithm, the number of T turns.</span></p></li>
<li>
<p><span class="font2">b. &nbsp;&nbsp;&nbsp;Initialize: Weight of a training sample w = 1 / N, for i = 1, ..., N.</span></p></li>
<li>
<p><span class="font2">c. &nbsp;&nbsp;&nbsp;Do for t = 1, ..., T.</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font2">1) &nbsp;&nbsp;&nbsp;Use the basic learning algorithm to train a classification component, ht, on the training weight sample</span></p></li>
<li>
<p><span class="font2">2) &nbsp;Calculate the training error at </span><span class="font5" style="font-style:italic;">h<sub>t</sub>: ε<sub>t</sub>= ∑</span><span class="font4" style="font-style:italic;">t=</span><span class="font5" style="font-style:italic;"><sub>1</sub>w</span><span class="font4" style="font-style:italic;">t </span><span class="font5" style="font-style:italic;">,y<sub>l</sub> ≠ h<sub>t</sub>(x<sub>l</sub>)</span></p></li>
<li>
<p><span class="font2">3) &nbsp;Set the weight for component classifier </span><span class="font2" style="font-style:italic;">h</span><span class="font0" style="font-style:italic;">t </span><span class="font2" style="font-style:italic;">= a</span><span class="font0" style="font-style:italic;">t</span><span class="font2"> = </span><span class="font4" style="font-style:italic;">-</span><span class="font5" style="font-style:italic;">ln</span><span class="font5"> (</span><span class="font4"><sup>1-</sup>¾</span></p></li></ul>
<p><span class="font4" style="font-style:italic;">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;εt</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">4) &nbsp;&nbsp;&nbsp;Update the training sample weight </span><span class="font5">w</span><span class="font4">t<sup>+1</sup> </span><span class="font5">= </span><span class="font4" style="font-style:italic;">dexpl-W</span><span class="font0" style="font-style:italic;">i^</span><span class="font4" style="font-style:italic;">l</span><span class="font2">, <sub>i</sub> = 1, ..., N </span><span class="font2" style="font-style:italic;">C</span><span class="font0" style="font-style:italic;">t</span><span class="font2"> is a</span></p></li></ul>
<p><span class="font4" style="font-style:italic;">C</span><span class="font8" style="font-style:italic;">t</span></p>
<p><span class="font2">normalization constant.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">d. &nbsp;&nbsp;&nbsp;Output</span><span class="font5">/(x) = </span><span class="font5" style="font-style:italic;">sign(∑l</span><span class="font4" style="font-style:italic;">=</span><span class="font5" style="font-style:italic;"><sub>1</sub>a<sub>t</sub>h<sub>t</sub>(x))</span><span class="font2"> to make predictions using the last model.</span></p></li></ul>
<p><span class="font2">Therefore, the core of the iterative AdaBoost process is iteratively AdaBoost by in circles, updating the sample to find the best weak classifier distribution at the moment, and then calculate the error rate of each weak classifier, and finally build a weak classifier into a strong classifier several times [24].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font2" style="font-weight:bold;"><a name="bookmark13"></a>2.4. &nbsp;&nbsp;&nbsp;Support Vector Machine Classification</span></h2></li></ul>
<p><span class="font2">Classification is the process of classifying a collection of objects, data or ideas into groups, where each member has one of the same characteristics. In classification, classes cannot be contested before examining data so that it is often called supervised learning [25].</span></p>
<p><span class="font2">SVM is used for linear and nonlinear data classifications. SVM with nonlinear mapping functions to convert the original training data into higher dimensions, this is done when the data is not linearly separated. Data from 2 classes separated by hyperplane found by SVM uses margin and support vector [26].</span></p>
<p><span class="font2">SVM uses kernel tricks to connect training sample input space to high dimensional feature space and identify optimal separator hyperplane. The RBF (Radial Basis Function) kernel with gamma parameters is used. To control the complexity of the model and training errors, regulatory parameter C is used. Choosing the right gamma and C values, solving the problem of overfitting. A low parameter C value makes a smooth decision, while a high C goals to classify all training samples correctly. The function of the SVM decision for binary classification problems is defined as follows [27].</span></p>
<p><span class="font5" style="font-style:italic;">f<sup>(</sup>x) = [w,φ <sup>(</sup>x<sup>)]</sup> + b</span><span class="font2"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)</span></p>
<p><span class="font2">Mapping sample x from input space to high dimensional feature space is represented by φ (x). Dot product in the feature space is displayed as [..., ...]. The ideal value w and b are achieved by doing the following optimization.</span></p>
<p><a href="#bookmark14"><span class="font2">Minimize : </span><span class="font5" style="font-style:italic;">g(w, ε)</span><span class="font5"> = </span><span class="font4">1 </span><span class="font7">∣∣</span><span class="font5">w</span><span class="font7">∣∣</span><span class="font4"><sup>2</sup> </span><span class="font5">+ </span><span class="font5" style="font-style:italic;">C</span><span class="font5"> ∑</span><span class="font4">∙=</span><span class="font5"><sub>1</sub> </span><span class="font5" style="font-style:italic;">ε<sub>l</sub></span><span class="font2">(2)</span></a></p>
<p><a href="#bookmark15"><span class="font2">Subject to : </span><span class="font5" style="font-style:italic;">y</span><span class="font4" style="font-style:italic;">i</span><span class="font5" style="font-style:italic;">([w,φ(x<sub>i</sub>)]</span><span class="font5"> + 6)≥1-ε<sub>i</sub>,ε<sub>i</sub>≥0</span></a></p>
<p><span class="font2">Where ε_i is a variable slack.</span></p>
<p><a href="#bookmark16"><span class="font5" style="font-style:italic;">k(x<sub>i</sub>,x<sub>j</sub>) = [φ(x<sub>i</sub>),φ(x</span><span class="font4" style="font-style:italic;">j</span><span class="font5" style="font-style:italic;">)]</span><span class="font2">(4)</span></a></p>
<p><span class="font2">The kernel function k (xi, xj) is used to map input vectors non-linearly to the appropriate feature space using the RBF function.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2" style="font-weight:bold;">3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></p></li></ul>
<p><span class="font2">In this study, the proposed algorithm was tested using the Python programming language by utilizing a little-known library and the Pyswarms library. This experiment was carried out 5 times with the provisions of the PSO parameters shown in Table 2 as follows.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 2. </span><span class="font2">PSO Parameter setting </span><span class="font2" style="font-weight:bold;">Parameter &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Values</span></p>
<p><a href="#bookmark17"><span class="font2">Swarm size30</span></a></p>
<p><a href="#bookmark18"><span class="font2">Cognitive parameters (</span><span class="font2" style="font-style:italic;">c</span><span class="font0" style="font-style:italic;">1</span><span class="font2">)2</span></a></p>
<p><a href="#bookmark19"><span class="font2">Social parameters (</span><span class="font2" style="font-style:italic;">c</span><span class="font0" style="font-style:italic;">2</span><span class="font2">)2</span></a></p>
<p><a href="#bookmark20"><span class="font2">Inertia weight1</span></a></p>
<p><a href="#bookmark21"><span class="font2">Number of iteration100</span></a></p>
<p><span class="font2">The parameters of SVM in this study are arranged as follows. Parameters C = 0.1 and gamma parameters = 1 / number of features. While for AdaBoost in this study, 10 iterations will be conducted. Data collection is done randomly with ratio 3:7 for each data testing: training data. The comparison is taken because it can produce high accuracy.</span></p>
<p><span class="font2">The application of the PSO algorithm as a feature selection in this study after performing 5 times the execution produces a feature that is not always the same for each execution. Because in each execution there are several differences in the features selected, it produces different accuracy. After feature selection, classification process was carried out using the SVM algorithm. In this research, ensemble Adaboost was applied to improve the accuracy of the SVM classification algorithm. The results of the feature selection process with PSO and the accuracy of each SVM execution with the application of PSO as feature selection without ensemble adaboost and with the application of ensemble adaboost can be seen in Table 3 as follows.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 3. </span><span class="font2">The Result of Feature Selection PSO</span></p>
<p><span class="font2" style="font-weight:bold;">Accuracy</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Execution</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Feature Set</span></p></td><td style="vertical-align:top;">
<p><span class="font2" style="font-weight:bold;">Total</span></p>
<p><span class="font2" style="font-weight:bold;">Feature</span></p></td><td style="vertical-align:middle;">
<p><span class="font2" style="font-weight:bold;">PSO + SVM</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">PSO +</span></p>
<p><span class="font2" style="font-weight:bold;">SVM + </span><span class="font2" style="font-weight:bold;font-style:italic;">Adaboost</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">18</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">97,25%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">100%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">2</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">18</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">98,75%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99,16%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">18</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">97,25%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">100%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">4</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">18</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">98,75%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99,16%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">5</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">18</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">98,75%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">99,16%</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">Average</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">98,15%</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2" style="font-weight:bold;">99,50%</span></p></td></tr>
</table>
<p><span class="font2">Information:</span></p>
<p><span class="font2">Selected features &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: 1</span></p>
<p><span class="font2">Unselected feature &nbsp;&nbsp;&nbsp;&nbsp;: 0</span></p>
<p><span class="font2">From the experimental data, it can be seen that the application of PSO as a feature selection can produce a high level of accuracy, besides that adaboost ensemble application can also increase the accuracy of the SVM + PSO classification, which is increased by 1.35% compared to before added Adaboost. So that it can be seen the accuracy comparison of the SVM classification method without optimization and SVM after being optimized using PSO as a feature selection and Adaboost ensemble. Comparison of the results of accuracy can be seen in Table 4 as follows.</span></p>
<p><span class="font2" style="font-weight:bold;">Ta</span><span class="font2" style="font-weight:bold;text-decoration:underline;">ble 4. </span><span class="font2" style="text-decoration:underline;">Accuracy Result of Feature Selection P</span><span class="font2">SO</span></p>
<p><span class="font2" style="font-weight:bold;">Algorithm &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Accuracy</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font2">SVM</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">63,30%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">PSO + SVM</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">98,15%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">PSO + SVM + Adaboost</span></p></td><td style="vertical-align:middle;">
<p><span class="font2">99,50%</span></p></td></tr>
</table>
<p><span class="font2">From this study, the classification with the SVM algorithm obtained an accuracy of 63.30% while the classification with the SVM algorithm optimized by PSO and ensemble Adaboost algorithms produced an average accuracy of 99.50%. By applying the algorithm PSO and ensemble Adaboost can increase accuracy by 36.20%. Significant accuracy increases due to the application of the PSO algorithm to select the optimal feature set in the classification algorithm SVM performs an optimal solution based on the swarm intelligence concept where each particle in the search area represents a classification process. In addition, the determination of the parameter values used in the application of the PSO algorithm also affects the selection of optimal features so that it can provide high accuracy results. Besides that, ensemble Adaboost in this combination can also improve the performance of the SVM algorithm classification for the CKD dataset or which has the same characteristics. From this study, it is known that by applying the PSO and ensemble Adaboost algorithms on the SVM algorithm it can improve the accuracy of the diagnosis of CKD so that it can be used by researchers as a reference in conducting research into the diagnosis of CKD.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark22"></a><span class="font2" style="font-weight:bold;"><a name="bookmark23"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font2">Based on this research, the application of PSO and ensemble AdaBoost algorithms to optimize the SVM classification algorithm in the CKD dataset taken from the UCI Machine Learning Repository. PSO algorithm is used to get the best combination of features for the classification process, while AdaBoost is used as an ensemble method to improve SVM accuracy results as weak classifiers to become strong classifiers. The results of this study, obtained the accuracy of the application of the SVM algorithm without optimization of 63.30% while after being optimized using the PSO + AdaBoost feature selection the average accuracy increased by 36.20% to 99.50% with the selected feature average numbering 18 features. Thus, it can be concluded that the application of the PSO and ensemble AdaBoost algorithms can get optimal features and can improve the accuracy of the SVM algorithm. In future works, the spilted reduction feature can be applied to many types of the dataset with the same characteristics as the CKD dataset. It also compares with other feature algorithms to determine the impact of the model in increasing the accuracy of classifiers.</span></p>
<h2><a name="bookmark24"></a><span class="font2" style="font-weight:bold;"><a name="bookmark25"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;M. H. Elhebir and A. Abraham, &quot;A Novel Ensemble Approach to Enhance the Performance of Web Server Logs Classification,&quot; </span><span class="font2" style="font-style:italic;">International Journal of Computer Information Systems and Industrial Management Applications,</span><span class="font2"> vol. 7, pp. 189-195, 2015.</span></p></li>
<li>
<p><span class="font2">[2] &nbsp;&nbsp;&nbsp;G. A. Afzali and S. Mohammadi, &quot;Privacy Preserving Big Data Mining: Association Rule Hiding,&quot; </span><span class="font2" style="font-style:italic;">Journal of Information System and Telecomunication,</span><span class="font2"> vol. 4, no. 2, pp. 70-77,</span></p></li></ul>
<p><span class="font2">2016.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[3] &nbsp;&nbsp;&nbsp;H. Hamidi and A. Daraei, &quot;Analysis and Evaluation of Techniques for Myocardial Infraction Based on Genetic Algorithm and Weight by SVM,&quot; </span><span class="font2" style="font-style:italic;">Journal of Information System and Telecommunication,</span><span class="font2"> vol. 4, no. 2, pp. 85-91, 2016.</span></p></li>
<li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;M. A. Muslim, E. Sugiharti, B. Prasetiyo and S. Alimah, &quot;Penerapan Dizcrretization dan Teknik Bagging UNtuk Meningkatkan Akurasi Klasifikasi Berbasis Enseble pada Algoritma C4.5 dalam Mendiagnosa Diabetes,&quot; </span><span class="font2" style="font-style:italic;">LONTAR KOMPUTER: Jurnal Ilmiah Teknologi Informasi,</span><span class="font2"> vol. 8, no. 2, pp. 135-143, 2017.</span></p></li>
<li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;L. J. Rubini and P. Eswaran, &quot;Generating Comparative Analysis of Early Stage Prediction of Chronic Kidney Disease,&quot; </span><span class="font2" style="font-style:italic;">International Journal of Modern Engineering Research (IJMER), </span><span class="font2">vol. 5, no. 7, pp. 49-55, 2015.</span></p></li>
<li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;I. Fadilla, P. P. Adikara and R. S. Perdana, &quot;Klasifikasi Penyakit Chronic Kidney Disease (CKD) Dengan Menggunakan Metode Extreme Learning Machine (ELM),&quot; </span><span class="font2" style="font-style:italic;">Jurnal Pengembangan Teknologi Informasi dan Ilmu Komputer e-ISSN 2548:964X,</span><span class="font2"> vol. 2, no. 10, pp. 3397-3405, 2018.</span></p></li>
<li>
<p><span class="font2">[7] &nbsp;&nbsp;&nbsp;W. Abedalkhader and N. Abdulrahman, &quot;Missing Data Classification of Chronic Kidney Disease,&quot; </span><span class="font2" style="font-style:italic;">International Journal of Data Mining &amp;&nbsp;Knowledge Management Process (IJDKP), </span><span class="font2">vol. 7, no. 5, pp. 55-61, 2017.</span></p></li>
<li>
<p><span class="font2">[8] &nbsp;&nbsp;&nbsp;A. Widodo and S. Handoyo, &quot;The Classification Performance using Logistic regression and Support Vector Machine (SVM),&quot; </span><span class="font2" style="font-style:italic;">Journal of Theoritical and Applied Information Technology,</span><span class="font2"> vol. 95, no. 19, pp. 5184-5193, 2017.</span></p></li>
<li>
<p><span class="font2">[9] &nbsp;&nbsp;&nbsp;A. Jamal, A. Handayani, A. A. Septiandri, E. Ripmiatin and Y. Effendi, &quot;Dimensionality Reduction using PCA and K-Means Clustering for Breast Cancer Prediction,&quot; </span><span class="font2" style="font-style:italic;">LONTAR KOMPUTER: Jurnal Ilmiah Teknologi Informasi,</span><span class="font2"> vol. 9, no. 3, pp. 192-201, 2018.</span></p></li>
<li>
<p><span class="font2">[10] &nbsp;&nbsp;&nbsp;F. S. Jumeilah, &quot;Penerapan Support Vector Machine (SVM) untuk Pengkategorian Penelitian,&quot; </span><span class="font2" style="font-style:italic;">JURNAL RESTI (Rekayasa Sistem dan Teknologi Informasi),</span><span class="font2"> vol. 1, no. 1, pp. 19-25, 2017.</span></p></li>
<li>
<p><span class="font2">[11] &nbsp;&nbsp;&nbsp;M. Mohammadpour, M. Ghorbanian and S. Mozaffari, &quot;AdaBoost Performance Improvement Using PSO Algorithm,&quot; in </span><span class="font2" style="font-style:italic;">2016 Eight International Conference on Information and Knowledge Technology (IKT)</span><span class="font2">, Iran, 2016.</span></p></li>
<li>
<p><span class="font2">[12] &nbsp;&nbsp;&nbsp;R. Wang, &quot;AdaBoost for Feature Selection, Classification and Its Relation with SVM, A Review,&quot; </span><span class="font2" style="font-style:italic;">Physics Procedia</span><span class="font2">, vol. 25, pp.800-807, 2012.</span></p></li>
<li>
<p><span class="font2">[13] &nbsp;&nbsp;&nbsp;D. Panday, R. C. de Amorim and P. Lane, &quot;Feature weighting as a tool for unsupervised feature selection,&quot; </span><span class="font2" style="font-style:italic;">Information Processing Letters,</span><span class="font2"> vol. 129, pp. 44-52, 2018.</span></p></li>
<li>
<p><span class="font2">[14] &nbsp;&nbsp;&nbsp;M. H. Aghdam and S. Heidari, &quot;Feature Seletion Using Particel Swarm Optimization in Text Categorization,&quot; </span><span class="font2" style="font-style:italic;">Journal of Artificial Intelligence and Soft Computing Research,</span><span class="font2"> vol. 5, no. 4, pp. 231-238, 2015.</span></p></li>
<li>
<p><span class="font2">[15] &nbsp;&nbsp;&nbsp;F. Mar'i and A. A. Supianto, &quot;Clustering Credit Card Holder Berdasarkan Pembayaran Tagihan Menggunakan Improved K-Means dengan Particle Swarm Optimization,&quot; </span><span class="font2" style="font-style:italic;">Jurnal Teknologi Informasi dan Ilmu Komputer,</span><span class="font2"> vol. 5, no. 6, pp. 737-744, 2018.</span></p></li>
<li>
<p><span class="font2">[16] &nbsp;&nbsp;&nbsp;M. A. Muslim, A. Nurzahputra and B. Prasetiyo, &quot;Improving Accuracy of C4.5 Algorithm Using Split Feature Reduction Model and Bagging Ensemble for Credit Card Risk Prediction,&quot; in </span><span class="font2" style="font-style:italic;">International Conference on Information and Communications Technology (ICOIACT)</span><span class="font2">, Yogyakarta, 2018.</span></p></li>
<li>
<p><span class="font2">[17] &nbsp;&nbsp;&nbsp;F. Ardjani, K. Sadouni and M. Benyettou, &quot;Optimization of SVM MultiClass by Particle Swarm (PSO-SVM),&quot; in </span><span class="font2" style="font-style:italic;">International Workshop on Database Technology and Applications</span><span class="font2">, China, 2010.</span></p></li>
<li>
<p><span class="font2">[18] &nbsp;&nbsp;&nbsp;L. Y. Chuang, C. H. Ke and C. H. Yang, &quot;A Hybrid Both Filter and Wrapper Feature Selection Method for Microarray Classification,&quot; in </span><span class="font2" style="font-style:italic;">Internation MiltiConference of Engineers and Computer Scientist 2008</span><span class="font2">, Hong Kong, 2008.</span></p></li>
<li>
<p><span class="font2">[19] &nbsp;&nbsp;&nbsp;S. Gunasundari, S. Janakiraman and S. Meenambal, &quot;Multiswarm Heterogeneous Binary PSO using Win-Win approach for improved Feature Selection in Liver and Kidney disease Diagnosis,&quot; </span><span class="font2" style="font-style:italic;">Computerized Medical Imaging and Graphics,</span><span class="font2"> vol. 70, pp. 135-154, 2018.</span></p></li>
<li>
<p><span class="font2">[20] &nbsp;&nbsp;&nbsp;Z. H. Zhou, Ensemble Methods: Foundations and Algorithms, Chapman and Hall: CRC, 2012.</span></p></li>
<li>
<p><span class="font2">[21] &nbsp;&nbsp;&nbsp;J. Han, M. Kamber and J. Pei, Data Mining: Concepts and Techniques, Waltham, MA: Morgan Kaufman Publisher (Elsivier), 2012.</span></p></li>
<li>
<p><span class="font2">[22] &nbsp;&nbsp;&nbsp;A. Nurzahputra and M. A. Muslim, &quot;Peningkatan Akurasi Pada Algoritma C4.5 Menggunakan AdaBoost untuk Meminimalkan Resiko Kredit,&quot; in </span><span class="font2" style="font-style:italic;">Prosiding SNATIF</span><span class="font2">, Kudus, 2017, pp. 243-247.</span></p></li>
<li>
<p><span class="font2">[23] &nbsp;&nbsp;&nbsp;E. Listiana and M. A. Muslim, &quot;Penerapan Adaboost Untuk Klasifikasi Support Vector Machine Guna Meningkatkan Akurasi Pada Diagnosis Chronic Kidney Disease,&quot; in</span></p></li></ul>
<p><span class="font2" style="font-style:italic;">Prosiding SNATIF</span><span class="font2">, Kudus, 2017, pp. 875-881.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[24] &nbsp;&nbsp;&nbsp;Y. Wang and X. Li, &quot;Improvement of RBF Neural Network by AdaBoost Algorithm Combined with PSO,&quot; </span><span class="font2" style="font-style:italic;">Telkomnika,</span><span class="font2"> vol. 14, no. 3A, pp. 56, 2016.</span></p></li>
<li>
<p><span class="font2">[25] &nbsp;&nbsp;&nbsp;S. Vijayarani and S. Dhayanand, &quot;Data Mining Classification Algorithm for Kidney Disease Prediction,&quot; </span><span class="font2" style="font-style:italic;">International Journal on Cybernetics &amp;&nbsp;Informatics (IJCI),</span><span class="font2"> vol. 4, no. 4, pp. 1325, 2015.</span></p></li>
<li>
<p><span class="font2">[26] &nbsp;&nbsp;&nbsp;A. Subasi, &quot;Classification of EMG signals using PSO optimized SVM for diagnosis of neuromuscular disorders,&quot; </span><span class="font2" style="font-style:italic;">Computers in biology and medicine,</span><span class="font2"> vol. 43, no. 5, pp. 576-586, 2013.</span></p></li>
<li>
<p><span class="font2">[27] &nbsp;&nbsp;&nbsp;U. Bhosle and J. Deshmukh, &quot;Mammogram classification using AdaBoost with RBFSVM and Hybrid KNN–RBFSVM as base estimator by adaptively adjusting γ and C value,&quot; </span><span class="font2" style="font-style:italic;">International Journal Information and Technology,</span><span class="font2"> pp. 1-8, 2018.</span></p></li></ul>
<p><span class="font2">127</span></p>