---
layout: full_article
title: "Sentiment Analysis of the Enforcement of PSBB Part II in Jakarta"
author: "Gede Putra Aditya Brahmantha, I Wayan Santiyasa"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-64401 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-64401"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-64401"  
comments: true
---

<p><span class="font1">p-ISSN: 2301-5373</span></p>
<p><span class="font1">e-ISSN: 2654-5101</span></p>
<p><span class="font1">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font1">Volume 9 No. 2, November 2020</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font2" style="font-weight:bold;"><a name="bookmark1"></a>Sentiment Analysis of the Enforcement of PSBB Part II in Jakarta</span></h1>
<p><span class="font1">Gede Putra Aditya Brahmantha<sup>a1</sup></span><span class="font1" style="font-weight:bold;">, </span><span class="font1">I Wayan Santiyasa<sup>a2</sup></span></p>
<p><span class="font1"><sup>a</sup>Informatics Department <sup>a</sup>informatics Department, Udayana University Bali, Indonesia</span></p>
<p><a href="mailto:1adit.hermawan333@gmail.com"><span class="font1"><sup>1</sup>adit.hermawan333@gmail.com</span></a></p>
<p><a href="mailto:2santiyasa67@gmail.com"><span class="font1"><sup>2</sup>santiyasa67@gmail.com</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">In addition to communicating, Social Media is a place to issue opinions by the public on many things that are currently taking place, Twitter is one of these social medias that is widely used in conveying opinions regardless of whether these opinions are negative, positive, or even neutral. Tweets data about the Enforcement of PSBB Part II in Jakarta were obtained as many as 200 opinions using web crawling then advanced to the preprocessing stage before being classified using the K-Nearest Neighbor and Multinomial Naive Bayes algorithms. In 3 tests, the highest accuracy was 65.00% for K-Nearest Neighbor and the highest accuracy was 85.00% for Multinomial Naive Bayes method.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">k-nearest neighbor, sentiment analysis, crawling, text mining, multinomial naive bayes</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h3></li></ul>
<p><span class="font1">Large-Scale Social Restrictions (PSBB in Indonesian) Part II in Jakarta was announced on September 9, 2020 and enforced on September 14, 2020, have a significant impact on many communities in Jakarta. Many offices re-implement Work from Home practice for their employees and many business and companies are temporary closed down. Due to that, many people were at home. With PSBB in effect, the usage of social media is getting higher, and the internet networks is currently being used by the wider community to convey their aspirations and opinions freely, one of which is using social media [1].</span></p>
<p><span class="font1">Twitter is a social media microblog that allows its users to write about various topics and discuss current issues. Twitter has abundant active users, so it will provide comments or the latest information about things that are currently being discussed in the world and cause trending topics in Twitter [2].</span></p>
<p><span class="font1">Sentiment analysis or opinion mining is a process of understanding, extracting and processing textual data automatically to get sentiment information contained in an opinion sentence [3].</span></p>
<p><span class="font1">Previously related research is the Analysis of Public Opinion Sentiment on the Effects of PSBB on Twitter with the Decision Tree-KNN-Naïve Bayes Algorithm [4] written by Muhammad Syarifuddin at the enactment of the first PSBB, a collection of aspirations or comments from twitter users regarding the effects of PSBB, one of which is, can be used as an analysis of public opinion sentiment. Data regarding the effects of PSBB were obtained as many as 170 opinions, which then processed using data mining techniques, where there are text mining processes, tokenization, transformation, classification, and stem. Afterwards, it is calculated into three different algorithms to be compared, the algorithms used are Decision Tree, K-Nearest Neighbors (K-NN), and Naïve Bayes Classifier with the goal of finding the best accuracy on these classifiers. The highest result of this study is the Decision Tree algorithm with an accuracy value of 83.3%, precision 79% and a recall of 87.17%.</span></p>
<p><span class="font1">In this study, sentiment analysis was carried out to classify public opinion as negative or positive. This study used two different classification methods, namely K-Nearest Neighbor and Multinomial Naïve Bayes. This study also compares the accuracy of the two methods, which a better method will produce a higher accuracy.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark6"></a>2.1. &nbsp;&nbsp;&nbsp;&nbsp;Data Collection</span></h3></li></ul>
<p><span class="font1">In this research, secondary data is used. It is obtained by web crawling to get information in the form of tweets from people who live in Jakarta that contains the word &quot;PSBB&quot; which is included in the period September 9, 2020 to September 16, 2020. And this research will use 200 data divided into 100 data for the Negative Sentiment label, and 100 data for the Positive Sentiment label. Data labeling is performed manually by the author.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 1. </span><span class="font1">Example of Dataset</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Tweet</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Label</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">‘saya cinta PSBB’</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Positive</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">'RT @kompasiana: PSBB: Penyebaran Semakin Bertambah Banyak atau Pendapatan Semakin Berkurang Banyak? </span><a href="https://t.co/A6c9UVW8Rd'"><span class="font1">https://t.co/A6c9UVW8Rd'</span></a></p></td><td style="vertical-align:top;">
<p><span class="font1">Negative</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark7"></a><span class="font1" style="font-weight:bold;"><a name="bookmark8"></a>2.2. &nbsp;&nbsp;&nbsp;Preprocessing</span></h3></li></ul>
<p><span class="font1">At this stage, preprocessing will be carried out for data which are still considered as 'dirty'. Preprocessing is carried out to process the dirty data so that the data can be cleaned and identified. The preprocessing stage consists of the case folding stage, data cleansing, language normalization, stopword removal, stemming, and tokenization like these following steps:</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark9"></a><span class="font1" style="font-weight:bold;"><a name="bookmark10"></a>2.2.1. &nbsp;&nbsp;&nbsp;Case Folding</span></h3></li></ul>
<p><span class="font1">Case folding is the initial stage in preprocessing which goals is to change each word form to lowercase letters.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 2. </span><span class="font1">Case Folding Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Before Case Folding</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">After Case Folding</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">'RT @kompasiana: PSBB: Penyebaran Semakin Bertambah Banyak atau Pendapatan Semakin Berkurang Banyak? </span><a href="https://t.co/A6c9UVW8Rd'"><span class="font1">https://t.co/A6c9UVW8Rd'</span></a></p></td><td style="vertical-align:bottom;">
<p><span class="font1">‘rt @kompasiana: psbb: penyebaran semakin bertambah banyak atau pendapatan semakin berkurang banyak? </span><a href="https://t.co/a6c9uvw8rd%e2%80%99"><span class="font1">https://t.co/a6c9uvw8rd’</span></a></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark11"></a><span class="font1" style="font-weight:bold;"><a name="bookmark12"></a>2.2.2. &nbsp;&nbsp;&nbsp;Data Cleansing</span></h3></li></ul>
<p><span class="font1">Data Cleansing is the process of cleaning the text by removing irrelevant data such as usernames, hahstags, URLs, and emoticons.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 2. </span><span class="font1">Data Cleansing Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Before Data Cleansing</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">After Data Cleansing</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">‘rt @kompasiana: psbb: penyebaran semakin bertambah banyak atau pendapatan semakin berkurang banyak? </span><a href="https://t.co/a6c9uvw8rd%e2%80%99"><span class="font1">https://t.co/a6c9uvw8rd’</span></a></p></td><td style="vertical-align:middle;">
<p><span class="font1">psbb penyebaran semakin bertambah banyak atau pendapatan semakin berkurang banyak</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark13"></a><span class="font1" style="font-weight:bold;"><a name="bookmark14"></a>2.2.3. &nbsp;&nbsp;&nbsp;Language Normalization</span></h3></li></ul>
<p><span class="font1">Language normalization in this research replaces common word abbreviations into the original word and replaces non-standard words into standard words.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 4. </span><span class="font1">Example of word list for Language Normalization</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Before Language Normalization</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">After Language Normalization</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">kmn</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">kemana</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">dmn</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">dimana</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">gue</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">saya</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">mo</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">mau</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">jkt</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">jakarta</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark15"></a><span class="font1" style="font-weight:bold;"><a name="bookmark16"></a>2.2.4. &nbsp;&nbsp;&nbsp;Stopword Removal</span></h3></li></ul>
<p><span class="font1">A stopword is a list of unimportant and unused common words. In this process, these common words are deleted to reduce the number of words stored by the system.</span></p>
<p><span class="font1" style="font-weight:bold;">Tabel 5. </span><span class="font1">Stopword Removal Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Before Stopword Removal</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">After Stopword Removal</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">psbb penyebaran semakin bertambah banyak atau pendapatan semakin berkurang banyak</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">psbb penyebaran bertambah atau pendapatan berkurang</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark17"></a><span class="font1" style="font-weight:bold;"><a name="bookmark18"></a>2.2.5. &nbsp;&nbsp;&nbsp;Stemming</span></h3></li></ul>
<p><span class="font1">Stemming is replacing affixed words with basic words.</span></p>
<p><span class="font1" style="font-weight:bold;">Tabel 6. </span><span class="font1">Stemming Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Before Stemming</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">After Stemming</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">psbb &nbsp;penyebaran &nbsp;bertambah &nbsp;atau</span></p>
<p><span class="font1">pendapatan berkurang</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">psbb sebar tambah atau dapat kurang</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark19"></a><span class="font1" style="font-weight:bold;"><a name="bookmark20"></a>2.2.6 &nbsp;&nbsp;&nbsp;Tokenization</span></h3></li></ul>
<p><span class="font1">Tokenization is the process of cutting words from a text into multiple tokens. This process will remove any spaces.</span></p>
<p><span class="font1" style="font-weight:bold;">Tabel 7. </span><span class="font1">Tokenization Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Before Tokenization</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">After Tokenization</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">psbb sebar tambah atau dapat kurang</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">['psbb', &nbsp;&nbsp;'sebar', &nbsp;&nbsp;'tambah', &nbsp;&nbsp;'atau',</span></p>
<p><span class="font1">'dapat', 'kurang']</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<p><span class="font1" style="font-weight:bold;">2.3. &nbsp;&nbsp;&nbsp;Term Frequency Invers Document Frequency (TF-IDF)</span></p></li></ul>
<p><span class="font1">Term Frequency-Inverse Document Frequency (TF-IDF) is a method used to calculate the weight of each word that has been extracted. The use of this method is generally done to count common words in information retrieval. The TF-IDF weighting model is a method that integrates the term frequency (tf) and inverse document frequency (idf) models. Term frequency (tf) is a process for counting the number of occurrences of terms in a document and inverse document frequency (idf) is used to calculate terms that appear in various documents (comments) which are considered general terms, and are considered not important [5].</span></p>
<p><span class="font1">The step of weighting with TF-IDF are:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">a. &nbsp;&nbsp;&nbsp;Count term frequency </span><span class="font1" style="font-style:italic;">(tf)</span></p></li>
<li>
<p><span class="font1">b. &nbsp;&nbsp;&nbsp;Count weighting term frequency (W</span><span class="font0">tf</span><span class="font1">)</span></p></li></ul>
<p><span class="font9" style="font-style:italic;">W</span><span class="font9"> _ 1</span><span class="font9" style="font-style:italic;">1 + log10tft,d,jikatft,d &gt;&nbsp;0</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">(1) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font9">,</span></p></li></ul>
<ul style="list-style:none;"><li>
<p><span class="font1">c. &nbsp;&nbsp;&nbsp;Count document frequency (df)</span></p></li>
<li>
<p><span class="font1">d. &nbsp;&nbsp;&nbsp;Count the weight of inverse document frequency (idf)</span></p></li></ul>
<p><span class="font9" style="font-style:italic;">idf<sub>t</sub> = Log £-</span><span class="font8" style="font-style:italic;"><sup>a</sup>J</span><span class="font7" style="font-style:italic;">t</span></p>
<p><span class="font1">(2)</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">e. &nbsp;&nbsp;&nbsp;Count the weight of TF-IDF</span></p></li></ul>
<p><span class="font9" style="font-style:italic;">W</span><span class="font8" style="font-style:italic;">ta </span><span class="font9" style="font-style:italic;">= Wtf</span><span class="font8" style="font-style:italic;">t</span><span class="font9" style="font-style:italic;"><sub>4</sub> X idf</span><span class="font8" style="font-style:italic;">t</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">(3)</span></p></li></ul>
<p><span class="font1">Notes :</span></p>
<p><span class="font1">tf</span><span class="font0">t,d </span><span class="font1">= term frequency</span></p>
<p><span class="font9" style="font-style:italic;">Wt</span><span class="font1" style="font-style:italic;">f </span><span class="font7" style="font-style:italic;">t,d</span><span class="font1"> = weight of term frequency</span></p>
<p><span class="font1">df = the number of times the document contains a term</span></p>
<p><span class="font1">N = the total number of documents.</span></p>
<p><span class="font9" style="font-style:italic;">W</span><span class="font7" style="font-style:italic;">t</span><span class="font0" style="font-style:italic;">,</span><span class="font7" style="font-style:italic;">d</span><span class="font1"> = weight of TF-IDF.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark21"></a><span class="font1" style="font-weight:bold;"><a name="bookmark22"></a>2.4 &nbsp;&nbsp;&nbsp;K-Nearest Neighbor</span></h3></li></ul>
<p><span class="font1">One of the simplest classification methods used in data mining and machine learning is K-Nearest Neighbor (KNN). It's the most accepted method of classification because of its practical convenience and efficiency: it does not require the installation of models and has been shown to have superior performance for classifying several types of data. However, the superior classification performance of the KNN is highly dependent on the metric used to calculate the pairwise distance between data points. The KNN classification rules were established by the training sample only, without any additional data. In a more complicated approach, the KNN classification finds a group of k objects in the training set that is closest to the test object, and bases the assignment of labels on the dominance of a particular class in this test environment. The K-Nearest Neighbor (KNN) algorithm is a method for classifying objects based on the</span></p>
<p><span class="font1">closest training example in the feature space. KNN is a type of example-based learning, or lazy learning where functions are only approached locally and all calculations are deferred up to classification [6].</span></p>
<p><span class="font1">The steps are carried out as follows :</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">a. &nbsp;&nbsp;&nbsp;Determine the number of k.</span></p></li>
<li>
<p><span class="font1">b. &nbsp;&nbsp;&nbsp;Calculate the distance of the object of each data group. The distance calculation uses the Euclidean distance equation.</span></p></li></ul>
<p><span class="font9" style="font-style:italic;">D(x,y) =</span><span class="font9"> √∑‰</span><span class="font6">ι</span><span class="font9">(x<sub>i</sub> - y<sub>i</sub>)<sup>2</sup></span></p>
<p><span class="font1">(4)</span></p>
<p><span class="font1">Notes :</span></p>
<p><span class="font1">D = Distance</span></p>
<p><span class="font1">x = Train data</span></p>
<p><span class="font1">y = Test data</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">c. &nbsp;&nbsp;&nbsp;Obtain the classification results.</span></p></li></ul>
<ul style="list-style:none;"><li>
<h3><a name="bookmark23"></a><span class="font1" style="font-weight:bold;"><a name="bookmark24"></a>2.5 &nbsp;&nbsp;&nbsp;Multinomial Naive Bayes</span></h3></li></ul>
<p><span class="font1">The multinomial model counts the frequency of each word that appears in the document. For example, there is a document d and a class set c. To calculate the class of document d, it can be calculated with the formula [7] :</span></p>
<p><span class="font9" style="font-style:italic;">P(c∖term of document d') = P(c) x P( t<sub>1</sub>∖c) x P( t<sub>2</sub>∖c) x P( t<sub>3</sub>∖c) x ...x P( t<sub>n</sub>∖c)</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">(5)</span></p></li></ul>
<p><span class="font1">Notes :</span></p>
<p><span class="font9" style="font-style:italic;">P(c)</span><span class="font1"> = Prior probability of class c.</span></p>
<p><span class="font9" style="font-style:italic;">t<sub>n</sub></span><span class="font1"> = The n-th word in document d.</span></p>
<p><span class="font9" style="font-style:italic;">P(c∖term of document d)</span><span class="font1"> = The probability that a document belongs to class c.</span></p>
<p><span class="font9" style="font-style:italic;">P( t<sub>n</sub>∖c)</span><span class="font1"> = Probability of the n-th word known to class c.</span></p>
<p><span class="font1">The prior probability class c is determined by the formula:</span></p>
<p><span class="font9" style="font-style:italic;">P(c) = -</span></p>
<p><span class="font8" style="font-style:italic;">N</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">(6)</span></p></li></ul>
<p><span class="font1">Notes :</span></p>
<p><span class="font9" style="font-style:italic;">N<sub>c</sub></span><span class="font1" style="font-style:italic;">=</span><span class="font1"> The number of class c in the whole documents.</span></p>
<p><span class="font9" style="font-style:italic;">N</span><span class="font1"> = Total number of documents.</span></p>
<p><span class="font1">Meanwhile, the Multinomial formula used with TF-IDF word weighting as follows [7] :</span></p>
<div>
<p><span class="font9" style="font-style:italic;">P( t<sub>n</sub>∖c)</span></p>
</div><br clear="all">
<p><span class="font8" style="font-style:italic;">_ &nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font8" style="font-style:italic;text-decoration:underline;">W<sub>ct</sub>+1</span></p>
<p><span class="font8" style="font-style:italic;">(∑W'fVW'<sub>ct</sub>)+B'</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">(7)</span></p></li></ul>
<p><span class="font1">Notes :</span></p>
<p><span class="font9" style="font-style:italic;">W<sub>ct</sub></span><span class="font1"> = tfidf weighting score or W of term t in class c</span></p>
<p><span class="font9" style="font-style:italic;">∑W'⅛VW'</span><span class="font1"> = The total number of W from all terms in class c</span></p>
<p><span class="font9" style="font-style:italic;">B'</span><span class="font1"> = Total number of unique words of W (idf score not multiplied by tf) in all documents.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark25"></a><span class="font1" style="font-weight:bold;"><a name="bookmark26"></a>3. &nbsp;&nbsp;&nbsp;Research Results and Discussion</span></h3></li></ul>
<p><span class="font1">This study uses 200 data divided into 100 negative sentiments and 100 positive sentiments. From the dataset, it's divided by 80% for randomized training data and 20% for randomized test data.</span></p>
<p><span class="font1">The data must go through the preprocessing stage, namely changing all letters to lowercase, and then deleting irrelevant text, normalizing language, removing common words, returning words to their basic form, and finally separating sentences into words. After going through the preprocessing stage, the TF-IDF weighting was carried out following formula (3). After the weights are obtained, classification is carried out using the K-Nearest Neighbor and Multinomial Naive Bayes methods.</span></p>
<p><span class="font1">The results of the system evaluation are obtained by calculating the size of the correctly classified data divided by all the test data.</span></p>
<h2><a name="bookmark27"></a><span class="font5"><a name="bookmark28"></a>Tests Results</span></h2>
<p><span class="font3">100.00%</span></p>
<p><span class="font3">80.00%</span></p>
<p><span class="font3">60.00%</span></p>
<p><span class="font3">40.00%</span></p>
<p><span class="font3">20.00%</span></p>
<p><span class="font3">0.00%</span></p>
<p><span class="font3">Test 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Test 2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Test 3</span></p>
<p><span class="font3">KNN &nbsp;&nbsp;&nbsp;&nbsp;MNB</span></p>
<p><span class="font1" style="font-weight:bold;">Figure 1. </span><span class="font1">Graph of Test Results</span></p>
<p><span class="font1">In Figure 1, We conducted a number of experiments that obtained different accuracies. This is because the distribution of test data and training data is randomized from the percentages stated previously. From these results, the highest accuracy is 85.00% with the Multinomial Naive Bayes method and 65.00% with the K-Nearest Neighbor method with an average of 80.00% for Multinomial Naive Bayes and 58.33% for K-Nearest Neighbor.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark29"></a><span class="font1" style="font-weight:bold;"><a name="bookmark30"></a>4. &nbsp;&nbsp;&nbsp;Conclusions and Suggestions</span></h3></li></ul>
<p><span class="font1">From the results of the research that has been completed, it can be seen that the Multinomial Naive Bayes method produces higher accuracy than the K-Nearest Neighbor for classify the Sentiments of Enforcement of PSBB in Jakarta at least on this research. Furthermore, it is possible to improvise in preprocessing stage, such as a dictionary-based approach to language normalization and to experiment with changes in the k value in K-Nearest Neighbor to produce higher accuracy.</span></p>
<h3><a name="bookmark31"></a><span class="font1" style="font-weight:bold;"><a name="bookmark32"></a>References</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;L. Septiani and Y. Sibaroni, “Sentiment Analysis Terhadap Tweet Bernada Sarkasme</span></p></li></ul>
<p><span class="font1">Berbahasa Indonesia,” </span><span class="font1" style="font-style:italic;">J. Linguist. Komputasional</span><span class="font1">, 2019, doi: 10.26418/jlk.v2i2.23.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;S. Fransiska, “Seri Sains dan Teknologi ANALISIS SENTIMEN TWITTER UNTUK</span></p></li></ul>
<p><span class="font1">REVIEW FILM MENGGUNAKAN ALGORITMA NAIVE BAYES CLASSIFIER ( NBC ) PADA SENTIMEN R Jurnal Siliwangi Vol . 5 . No . 2 , 2019 Seri Sains dan Teknologi P-ISSN 2477-3891 E-ISSN 2615-4765,” vol. 5, no. 2, 2019.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[3] &nbsp;&nbsp;&nbsp;G. A. Buntoro, “Analisis Sentimen Hatespeech Pada Twitter Dengan Metode Naïve</span></p></li></ul>
<p><span class="font1">Bayes Classifier Dan Support Vector Machine,” </span><span class="font1" style="font-style:italic;">J. Din. Inform.</span><span class="font1">, 2016, doi: 10.1016/j.cya.2015.11.011.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[4] &nbsp;&nbsp;M. Syarifuddin, “ANALISIS SENTIMEN OPINI PUBLIK TERHADAP EFEK PSBB PADA</span></p></li></ul>
<p><span class="font1">TWITTER DENGAN ALGORITMA DECISION TREE-KNN-NAÏVE BAYES,” vol. 15, no. 1, pp. 87–94, 2020, doi: 10.33480/inti.v15i1.1433.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;C. D. Manning, P. Raghavan, and H. Schutze, </span><span class="font1" style="font-style:italic;">Introduction to Information Retrieval</span><span class="font1">.</span></p></li></ul>
<p><span class="font1">2008.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[6] &nbsp;&nbsp;&nbsp;A. K. Nikhath, K. Subrahmanyam, and R. Vasavi, “Building a K-Nearest Neighbor</span></p></li></ul>
<p><span class="font1">Classifier for Text Categorization,” </span><span class="font1" style="font-style:italic;">Int. J. Comput. Sci. Inf. Technol.</span><span class="font1">, 2016.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[7] &nbsp;&nbsp;&nbsp;A. Rahman, W. Wiranto, and A. Doewes, “Online News Classification Using Multinomial</span></p></li></ul>
<p><span class="font1">Naive Bayes,” </span><span class="font1" style="font-style:italic;">ITSMART J. Teknol. dan Inf.</span><span class="font1">, vol. 6, no. 1, pp. 32–38, 2017, doi: 10.20961/ITSMART.V6I1.11310.</span></p>
<p><span class="font1">This page is intentionally left blank</span></p>
<p><span class="font1">266</span></p>