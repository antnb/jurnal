---
layout: full_article
title: "Comparison of K-Nearest Neighbor And Modified K-Nearest Neighbor With Feature Selection Mutual Information And Gini Index In Informatics Journal Classsification"
author: "Benedict Emanuel Sutrisna, AAIN Eka Karyawati, Luh Arida Ayu Rahning Putri, I Wayan Santiyasa, Agus Muliantara, I Made Widiartha"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-85009 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-85009"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-85009"  
comments: true
---

<p><span class="font2">p-ISSN: 2301-5373</span></p>
<p><span class="font2">e-ISSN: 2654-5101</span></p>
<p><span class="font2">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font2">Volume 10, No 3. February 2022</span></p>
<p><span class="font3" style="font-weight:bold;">Comparison of K-Nearest Neighbor And Modified K-Nearest Neighbor With Feature Selection Mutual Information And Gini Index In Informatics Journal Classsification</span></p>
<p><span class="font2">Benedict Emanuel Sutrisna<sup>a1</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">AAIN Eka Karyawati<sup>a2</sup></span><span class="font2" style="font-weight:bold;">, </span><span class="font2">Luh Arida Ayu Rahning Putri<sup>a3</sup>, I Wayan Santiyasa<sup>a4</sup>, Agus Muliantara <sup>a5</sup>, I Made Widiartha<sup>a6</sup></span><span class="font0">,</span></p>
<p><span class="font2"><sup>a</sup>Program Studi Informatika, Fakultas Matematika dan Ilmu Pengetahuan Alam, Universitas Udayana </span><a href="mailto:1benemanuel0805@gmail.com"><span class="font2"><sup>1</sup>benemanuel0805@gmail.com</span></a></p>
<p><a href="mailto:2eka.karyawati@unud.ac.id"><span class="font2"><sup>2</sup>eka.karyawati@unud.ac.id</span></a></p>
<p><a href="mailto:3rahningputri@unud.ac.id"><span class="font2"><sup>3</sup>rahningputri@unud.ac.id</span></a></p>
<p><a href="mailto:4santiyasa@unud.ac.id"><span class="font2"><sup>4</sup>santiyasa@unud.ac.id</span></a></p>
<p><a href="mailto:5muliantara@unud.ac.id"><span class="font2"><sup>5</sup>muliantara@unud.ac.id</span></a></p>
<p><a href="mailto:6madewidiartha@unud.ac.id"><span class="font2"><sup>6</sup>madewidiartha@unud.ac.id</span></a></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font2" style="font-style:italic;">With the rapid development of informatics where thousands of informatics journals have been made, a new problem has occured where grouping these journals manually has become too difficult and expensive. The writer proposes using text classification for grouping these informatics journals. This research examines the combinations of two machine learning methods, K-Nearest Neighbors (KNN) and Modified K-Nearest Neighbors with two feature selection methods, Gini Index (GI) and Mutual Information (MI) to determine the model that produces the higherst evaluation score. The data are informatics journals stored in pdf files where they are given one of 3 designated labels: Information Retrieval, Database or Others. 252 data were collected from the websites, neliti.com and garuda.ristekbrin.go.id. This research examines and compares which of the two methods, KNN and MKNN at classifying informatics journal as well as determining which combination of parameters and feature selection that produces the best result. This research finds that the combination of method and feature selection that produces the best evaluation score is MKNN with GI as feature selection producing precision score, recall score and f1-score of 97.7%</span></p>
<p><span class="font2" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font2" style="font-style:italic;">Text Classification, KNN, MKNN, Mutual Information, Gini Index, Informatics Journal.</span></p>
<ul style="list-style:none;"><li><a name="caption1"></a>
<h2><a name="bookmark0"></a><span class="font2" style="font-weight:bold;"><a name="bookmark1"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font2">The field of informatics is experiencing rapid development. Hundreds of research in various fields are conducted each year where their results would be used as material for future research. Though not all findings will be relevant towards a research that’s being conducted, as such it would be prudent to group those research to make it easier to find relevant references for future research. Unfortunately, the quick growth of informatics with hundreds of research being published each year makes grouping these research through human efforts near impossible and very expensive. This problem can be overcome with computers through text classifications.</span></p>
<p><span class="font2">According to [1] various classification methods can be used for document classification in various domains, such as digital libraries and scientific literature</span><span class="font2" style="font-style:italic;">.</span><span class="font2"> According to [2] one algorithm that can solve the classification problem is K-Nearest Neighbor (KNN) which has an easy to understand and implement algorithm, however it has a weakness where larger dimensionality of data will negatively affect its performance. Several research have been made to overcome this problem, Research conducted by [3] found that feature selection improves the evaluation scores of KNN and Naïve Bayes compared to when both don’t use feature selection. [4] created a variant of KNN named Modified K-Nearest Neighbor which has a better evaluation score than KNN. However feature selection was not used in said research, thus it is not known how feature selection would affect its performance. According</span></p>
<p><span class="font2">to [5] the use of the feature selection method Mutual Information (MI) improves the evaluation score of the Support Vector Machine algorithm in classifying Indonesian news articles. [6] found that the Gini Index (GI) feature selection method increases the evaluation score of KNN in classifying cognitive level documents. Based on those sources, the writer believes that both feature selection methods can be used on informatics journals, but wants to know which method produces the highest evaluation score if only use one feature selection method.</span></p>
<p><span class="font2">Based on the existing problem and the related research of which are the basis of this research, the writer intends to compare KNN and MKNN with MI and GI as feature selection with the hopes that this research find the combination algorithm and feature selection with the highest evaluation score.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font2" style="font-weight:bold;"><a name="bookmark3"></a>2. &nbsp;&nbsp;&nbsp;Reseach Methods</span><br><br><span class="font2" style="font-weight:bold;"><a name="bookmark4"></a>2.1 &nbsp;&nbsp;&nbsp;Research Stage</span></h2></li></ul>
<p><span class="font2">This research is divided in to two stages. In the first stage, models, which are combinations of algorithms and feature selection methods, are divided in to 3 categories based on which feature selection methods are used, namely: none, GI, and MI. The best model of each category is chosen to continue for the second stage. In the second stage, the 3 chosen models is tested again to determine the best model. Testing in this research is divided in to 2 phases, the training phase and the testing phase. The training phase is where training data is processed so that the model can use it in testing phase. It consists of preprocessing, TF-IDF weighting and feature selection. The testing phase is where the testing data is classified by the model and its results are evaluated. It consists of preprocessing, TF-IDF weighting, feature selection, classification, and evaluation.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark5"></a><span class="font2" style="font-weight:bold;"><a name="bookmark6"></a>2.2 &nbsp;&nbsp;&nbsp;Data Collection</span></h2></li></ul>
<p><span class="font2">Data is collected from 2 web sources,</span><a href="https://www.neliti.com/id/conferences/semnasif"><span class="font2"> </span><span class="font2" style="text-decoration:underline;">https://www.neliti.com/id/conferences/semnasif</span><span class="font2"> </span></a><span class="font2">and </span><a href="https://garuda.ristekbrin.go.id/area"><span class="font2" style="text-decoration:underline;">https://garuda.ristekbrin.go.id/area</span><span class="font2">.</span></a><span class="font2"> 252 information journals were collected and divided evenly in to 3 labels, Information Retrieval, Information and Database Systems and Their Applications, and others. Data labeling is done by the writer and evaluated by 12 fellow students from Text Mining and Big Data disciplines using the Kappa statistic.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark7"></a><span class="font8" style="font-weight:bold;"><a name="bookmark8"></a>2.3</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Preprocessing</span></h2></li></ul>
<p><span class="font2">Preprocessing aims to make the input documents more consistent to facilitate text representation, which is necessary for most text analytics task [1]. As can be seen in Figure 1, this research applies several preprocessing methods, namely case folding, punctuation removal, stemming, stop word removal and tokenization. Case folding is the process of converting letter in to the same case, particularly uppercase letters in to lowercase letters. Stop words removal is the removal of very common and low information words known as stop words. Stemming is the process of cutting inflected words in to their word stem. Tokenization is the process of dividing text in to several units called tokens, the tokens in this research consist of individual words.</span></p><img src="https://jurnal.harianregional.com/media/85009-1.jpg" alt="" style="width:349pt;height:128pt;">
<p><span class="font2" style="font-weight:bold;">Figure 1. </span><span class="font2">Preprocessing</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark9"></a><span class="font8" style="font-weight:bold;"><a name="bookmark10"></a>2.4</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;TF-IDF Weighting</span></h2></li></ul>
<p><span class="font2">Term Frequency – Inverse Document Frequency (TF-IDF) is a composite weighting method for each tem in every document. TF-IDF assumes has a good class of distinction occurs if a term has high freqeuncy in one document and low frequency in other documents [2].</span></p>
<p><span class="font2">The following are the steps of TF-IDF, see Figure 2:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">a. &nbsp;&nbsp;&nbsp;Calculate term frequency of term t in documet d (tf</span><span class="font0">t,d</span><span class="font2">)</span></p></li>
<li>
<p><span class="font2">b. &nbsp;&nbsp;&nbsp;Calculate document frequency of term t (df</span><span class="font0">t</span><span class="font2">)</span></p></li>
<li>
<p><span class="font2">c. &nbsp;&nbsp;&nbsp;Calculate inverse document frequency</span></p></li></ul>
<p><span class="font11" style="font-style:italic;">N</span></p><a name="caption2"></a>
<h1><a name="bookmark11"></a><span class="font13" style="font-style:italic;"><a name="bookmark12"></a>ιdf<sub>t</sub> = log-</span><span class="font2"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)</span></h1>
<p><span class="font13" style="font-style:italic;"><sup>a</sup></span><span class="font11" style="font-style:italic;">Jt</span></p>
<p><span class="font2">With idf</span><span class="font0">t </span><span class="font2">as the inverse document frequency of term t, df as the document frequency of term t, and N as the total number of documents</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">d. &nbsp;&nbsp;&nbsp;Calculate TF-IDF</span></p></li></ul>
<p><span class="font13">‰ = ‰ </span><span class="font13" style="font-style:italic;">× idf<sub>t</sub></span></p>
<div>
<p><span class="font2">(2)</span></p>
</div><br clear="all">
<p><span class="font2">With W</span><span class="font0">t,d </span><span class="font2">as weight of term t in document d.</span></p>
<div><img src="https://jurnal.harianregional.com/media/85009-2.jpg" alt="" style="width:298pt;height:136pt;">
<p><span class="font2" style="font-weight:bold;">Figure 2. </span><span class="font2">TF-IDF Weighting</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark13"></a><span class="font7"><a name="bookmark14"></a>2.5</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Gini Index</span></h2></li></ul>
<p><span class="font2">Gini Index (GI) is a measurement of statistical dispersion intended to represent wealth distribution of a country developed by Corrado Gini. GI is often used to measure discriminative power in a feature. GI is typically used for categorical variables, but can be generalized to numeric attributes through discretization [7]. The GI formula is as follows:</span></p>
<h1><a name="bookmark15"></a><span class="font13" style="font-style:italic;"><a name="bookmark16"></a>GI(x) = 1-∑</span><span class="font11" style="font-style:italic;">Y=</span><span class="font13" style="font-style:italic;"><sub>1</sub>P(i)<sup>2</sup></span></h1>
<div>
<p><span class="font7">(3)</span></p>
</div><br clear="all">
<p><span class="font2">With Y as total labels, x as term, and p(i) as probability of term x in document labeled i.</span></p>
<p><span class="font2">The steps of Gini Index can be seen in Figure 3.</span></p>
<div><img src="https://jurnal.harianregional.com/media/85009-3.jpg" alt="" style="width:400pt;height:148pt;">
<p><span class="font2" style="font-weight:bold;">Figure 3. </span><span class="font2">Gini Index</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark17"></a><span class="font7"><a name="bookmark18"></a>2.6</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Mutual Information</span></h2></li></ul>
<p><span class="font2">According to [8] mutual information (MI) is the measurement of the amount of information that one random variable contains about another random variable. MI is the reduction of uncertainty of a random variable caused by information from another random variable. MI determines the correlation between two words in a data set, if the MI score is large then the two terms often co-occur thus they relate semantically. Conversely a small MI score means that when one of them appears then the other does not, indicating no semantic relation. The formula for MI is as follows:</span></p>
<h1><a name="bookmark19"></a><span class="font13" style="font-style:italic;"><a name="bookmark20"></a>I(x,y)</span><span class="font13"> = </span><span class="font13" style="font-style:italic;">Σ<sub>x</sub>EX∑yEγP(x.y)log (^<sup>yy</sup></span><span class="font11" style="font-style:italic;">y--</span><span class="font13" style="font-style:italic;">)</span></h1>
<div>
<p><span class="font7">(4)</span></p>
</div><br clear="all">
<p><span class="font2">With p(x,y) as joint probability of x and y, p(x) as probability of x, and p(y) as probability of y.</span></p>
<p><span class="font2">The steps of Mutual Information can be seen in Figure 4.</span></p>
<div><img src="https://jurnal.harianregional.com/media/85009-4.jpg" alt="" style="width:415pt;height:148pt;">
<p><span class="font2" style="font-weight:bold;">Figure 4. </span><span class="font2">Mutual Information</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font7"><a name="bookmark22"></a>2.7</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;K-Nearest Neighbours</span></h2></li></ul>
<p><span class="font2">K-nearest neighbours (KNN) locally determines the decision boundary (label). For 1NN, each document is inserted in to the label of its nearest neighbours. For KNN, each document is inserted in to the majority label of its k nearest neighbours, with k as a parameter. KNN classification is based on contiguity hypothesis, which assumes a document d has the same class as its neighbouring training document [2]. The following are the steps of KNN classification, with the flowchart shownin Figure 5: </span><span class="font7">a. </span><span class="font2">Determine the value of k.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7">b.</span><span class="font2"> &nbsp;&nbsp;&nbsp;Calculate the distance of the object with each data point. Calculation is done using Euclidian distance with the following formula:</span></p></li></ul>
<h1><a name="bookmark23"></a><span class="font13" style="font-style:italic;"><a name="bookmark24"></a>D (x, y')</span><span class="font13"> = </span><span class="font13" style="font-style:italic;">√∑t</span><span class="font11" style="font-style:italic;">=</span><span class="font13" style="font-style:italic;"><sub>1</sub>(x</span><span class="font11" style="font-style:italic;">i </span><span class="font13" style="font-style:italic;">- y<sub>i</sub>)<sup>2</sup></span><span class="font2"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5)</span></h1>
<p><span class="font2">With D as distance, and x and y as training data and testing data respectively.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7">c.</span><span class="font2"> &nbsp;&nbsp;&nbsp;Gather the data points with the smallest distance as many as k.</span></p></li>
<li>
<p><span class="font7">d.</span><span class="font2"> &nbsp;&nbsp;&nbsp;Determine the class majority of the gathered data points.</span></p></li></ul><img src="https://jurnal.harianregional.com/media/85009-5.jpg" alt="" style="width:328pt;height:229pt;">
<p><span class="font2" style="font-weight:bold;">Figure 5. </span><span class="font2">K-Nearest Neighbor</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark25"></a><span class="font7"><a name="bookmark26"></a>2.8</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Modified K-Nearest Neighbours</span></h2></li></ul>
<p><span class="font7">According to [4] Modified K-Nearest Neighbours (MKNN) is a variation of KNN which computes a kind of weight named validity on training data based on the number of same labeled neighbours divided by the total of neighbors. The following is the algorithm of MKNN, with the flowchart shown in Figure 6:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7">a.</span><span class="font2"> &nbsp;&nbsp;&nbsp;Determine the value of K.</span></p></li>
<li>
<p><span class="font7">b.</span><span class="font2"> &nbsp;&nbsp;&nbsp;Determine validity (v) for each training data with the formula:</span></p></li></ul>
<p><a href="#bookmark27"><span class="font13" style="font-style:italic;">v(x)</span><span class="font13"> = </span><span class="font13" style="font-style:italic;"><sup>1</sup>∑^=1^ (lbl(x),lbl(N<sub>l</sub>(x)'))</span><span class="font2">(6)</span></a></p>
<p><span class="font7">With the function S to calculate similarity between x and the i</span><span class="font5">th </span><span class="font7">nearest neighbour with the formula:</span></p>
<p><a href="#bookmark28"><span class="font9">'-<sup>b</sup></span><span class="font13">' &nbsp;&nbsp;{'L</span><span class="font9"><sup>b</sup></span></a></p>
<p><span class="font7">With H as the number of neigbors to calculate v, x as designated training data, lbl(x) as label of x, and N</span><span class="font5">i</span><span class="font7">(x) as i</span><span class="font5">th </span><span class="font7">nearest neighbor of x</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7">c.</span><span class="font12"> &nbsp;&nbsp;&nbsp;Calculate the weight of k nearest neighbor with the formula:</span></p></li></ul>
<p><a href="#bookmark29"><span class="font13" style="font-style:italic;">W(i)</span><span class="font13"> = </span><span class="font9"><sup>r</sup></span><span class="font2">(θ </span><span class="font13">×</span></a></p>
<p><span class="font11">+ O ■ 5</span></p>
<p><span class="font12">With W(i) as weight of i</span><span class="font10">th </span><span class="font12">neighbour and d as Euclidean distance</span></p>
<ul style="list-style:none;"><li>
<p><span class="font7">d. &nbsp;&nbsp;&nbsp;</span><span class="font12">Compute the sum weights of every neighbour according to their label.</span></p></li>
<li>
<p><span class="font7">e. &nbsp;&nbsp;&nbsp;</span><span class="font12">Choose the label with the highest total weight.</span></p>
<div><img src="https://jurnal.harianregional.com/media/85009-6.jpg" alt="" style="width:394pt;height:238pt;">
<p><span class="font2" style="font-weight:bold;">Figure 6. </span><span class="font2">Modified K-Nearest Neighbor</span></p>
</div><br clear="all"></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark30"></a><span class="font7"><a name="bookmark31"></a>2.9</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Evaluation</span></h2></li></ul>
<p><span class="font2">Measurement of each model’s effectiveness in classification is done by using precision, recall and f1-score as evaluation scores. Precision is the ratio of total true positive to the sum total of true positive and false positive prediction. Recall is the ratio of total true positive to the sum total true positive and false negative prediction. F1-score is a calculation that combines precision and recall. The formula of precision, recall and f1-score is as the following.</span></p>
<div>
<h1><a name="bookmark32"></a><span class="font13" style="font-style:italic;"><a name="bookmark33"></a>Precision</span><span class="font13"> =</span></h1>
</div><br clear="all">
<div>
<p><span class="font11" style="font-style:italic;">TTP</span></p>
<p><span class="font11" style="font-style:italic;">TTP+TFP</span></p>
</div><br clear="all">
<div>
<h1><a name="bookmark34"></a><span class="font13" style="font-style:italic;"><a name="bookmark35"></a>Recall =</span></h1>
</div><br clear="all">
<div>
<p><span class="font11" style="font-style:italic;">TTP</span></p>
<p><span class="font11" style="font-style:italic;">TTP+TFN</span></p>
</div><br clear="all">
<div>
<h1><a name="bookmark36"></a><span class="font13" style="font-style:italic;"><a name="bookmark37"></a>F1 =</span></h1>
</div><br clear="all">
<div>
<p><span class="font11" style="font-style:italic;">2</span></p>
<p><span class="font11" style="text-decoration:underline;">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sub>ι</sub> 1</span></p>
<p><span class="font11" style="font-style:italic;">Precisian Recall</span></p>
</div><br clear="all">
<div>
<p><span class="font2">(9)</span></p>
<p><span class="font2">(10)</span></p>
<p><span class="font2">(11)</span></p>
</div><br clear="all">
<p><span class="font2">With:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">• &nbsp;&nbsp;TTP is the total true positive prediction</span></p></li>
<li>
<p><span class="font2">• &nbsp;&nbsp;TFP is the total false positive</span></p></li>
<li>
<p><span class="font2">• &nbsp;&nbsp;TFN is the total false negative</span></p></li></ul>
<p><span class="font2">The precision, recall and f1-score of each model is recorded and compared with emphasis on f1-score for deciding the best model.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark38"></a><span class="font2" style="font-weight:bold;"><a name="bookmark39"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span><br><br><span class="font8"><a name="bookmark40"></a>3.1</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Choosing the best model of each category</span></h2></li></ul>
<p><span class="font2">The following are comparisons between KNN and MKNN with various parameters in 3 categories, without feature selection, with GI, and with MI. The best model of each category will be compared in the next round of testing.</span></p>
<h2><a name="bookmark41"></a><span class="font2" style="font-weight:bold;"><a name="bookmark42"></a>Comparison of Models without Feature Selection</span></h2>
<p><span class="font2">Table 1 is the evaluation result of KNN and MKNN without feature selection. The testing finds that KNN with the parameters k = 3 produced the best result with an f1-score of 25.1%</span></p>
<p><span class="font2" style="font-weight:bold;">Table 1. </span><span class="font2">Evaluation Results of Models without Feature Selection</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:top;">
<p><span class="font1">Metode</span></p></td><td colspan="3" style="vertical-align:top;">
<p><span class="font1">Precision (average)</span></p></td><td colspan="3" style="vertical-align:top;">
<p><span class="font1">Recall (average)</span></p></td><td colspan="3" style="vertical-align:top;">
<p><span class="font1">F1-score (average)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">KNN</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">31.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">24.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">28.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">36.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">38.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">37.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">23.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">25.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">23.4%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=10)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=20)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=30)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">11.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">33.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">16.7%</span></p></td></tr>
</table>
<h2><a name="bookmark43"></a><span class="font2" style="font-weight:bold;"><a name="bookmark44"></a>Comparison of Models with GI</span></h2>
<p><span class="font2">Table 2 is the evaluation result of KNN and MKNN with GI as feature selection. The testing finds that MKNN with the parameters k = 3 and h = 30 produced the best result with an f1-score of 95.5%.</span></p>
<p><span class="font13" style="font-weight:bold;">Table 2. </span><span class="font2">Evaluation Results of Models with </span><span class="font13">GI</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:top;">
<p><span class="font1">Metode</span></p></td><td colspan="3" style="vertical-align:middle;">
<p><span class="font1">Precision (average)</span></p></td><td colspan="3" style="vertical-align:middle;">
<p><span class="font1">Recall (average)</span></p></td><td colspan="3" style="vertical-align:middle;">
<p><span class="font1">F1-score (average)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">KNN</span></p></td><td style="vertical-align:top;">
<p><span class="font1">95.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.8%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.8%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.9%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.9%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.3%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=10)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">91.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.5%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">89.5%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">89.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">89.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">88.9%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=20)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">92.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">92.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">92.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">92.2%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=30)</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">95.8%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">95.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">95.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">95.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.8%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">95.5%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.8%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">94.7%</span></p></td></tr>
</table>
<h2><a name="bookmark45"></a><span class="font2" style="font-weight:bold;"><a name="bookmark46"></a>Comparison of Models with MI</span></h2>
<p><span class="font2">Table 3 is the evaluation result of KNN and MKNN with MI as feature selection. The testing finds that MKNN with the parameters k = 3 and h = 20 produced the best result with an f1-score of 91.3%.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 3. </span><span class="font2">Evaluation Results of Models with MI</span></p>
<table border="1">
<tr><td rowspan="2" style="vertical-align:top;">
<p><span class="font1">Metode</span></p></td><td colspan="3" style="vertical-align:middle;">
<p><span class="font1">Precision (average)</span></p></td><td colspan="3" style="vertical-align:middle;">
<p><span class="font1">Recall (average)</span></p></td><td colspan="3" style="vertical-align:middle;">
<p><span class="font1">F1-score (average)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 3</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 5</span></p></td><td style="vertical-align:top;">
<p><span class="font1">k = 7</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">KNN</span></p></td><td style="vertical-align:top;">
<p><span class="font1">91.9%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">88.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">87.8%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">85.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">84.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">89.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">82.9%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">80.6%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=10)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">92.9%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">89.5%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">88.9%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">92.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">88.6%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">88.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.5%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">85.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">84.0%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=20)</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">93.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">89.4%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">91.9%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">88.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">87.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">91.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">86.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">85.3%</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">MKNN (h=30)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">91.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">88.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">87.3%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">91.2%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">87.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">86.1%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">90.8%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">85.7%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">84.3%</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark47"></a><span class="font8"><a name="bookmark48"></a>3.2</span><span class="font2" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;Comparison Between Models</span></h2></li></ul>
<p><span class="font2">This section compares the best models chosen in section 3.1. Table 4 is the evaluation result from testing KNN with k = 5 and no feature selection (model 1). Table 5 is the evaluation result from testing MKNN with k = 3, h = 30 and GI as feature selection (model 2). Table 6 is the evaluation result from testing MKNN with k = 3, h = 20 and MI as feature selection (model 3). From the testing of the three models, model 2 produced the best result with an average f1-score of 97.7%, followed by model 3 producing an average f1-score of 95%, and finally model 1 produced an average f1-score of 30% which is the worst of the results.</span></p>
<p><span class="font2" style="font-weight:bold;">Table 4. </span><span class="font2">Testing Results of Model 1</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font6">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">F1-score</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">IR</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">0.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">DB</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">38.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">100.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">55.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">Other</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">75.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">23.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">35.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Average</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">37.7%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">41.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font6" style="font-weight:bold;">30.0%</span></p></td></tr>
</table>
<p><span class="font2" style="font-weight:bold;">Table 5. </span><span class="font2">Testing Results of Model 2</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font1">Precision</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font1">F1-score</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">IR</span></p></td><td style="vertical-align:top;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">93.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">96.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">DB</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">93.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1" style="font-weight:bold;">97.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Other</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1" style="font-weight:bold;">100.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Rata-rata</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">97.7%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">97.7%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1" style="font-weight:bold;">97.7%</span></p></td></tr>
</table>
<p><span class="font2" style="font-weight:bold;">Table 6. </span><span class="font2">Testing Results of Model 3</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font1">Precision</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font1">F1-score</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">IR</span></p></td><td style="vertical-align:top;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">100.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">DB</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">88.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1" style="font-weight:bold;">93.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Other</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">100.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">85.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1" style="font-weight:bold;">92.0%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Rata-rata</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">96.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">95.0%</span></p></td><td style="vertical-align:middle;">
<p><span class="font1" style="font-weight:bold;">95.0%</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark49"></a><span class="font2" style="font-weight:bold;"><a name="bookmark50"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font2">This research found that in classifying informatics journals the best combination of algorithm and feature selection method is MKNN with parameters k = 3 and h = 30 with GI as feature selection, producing an average f1-score of 97.7%. It is worth noting that MKNN with MI as feature selection also produced good results with an average f1-score of 95%. Meanwhile both KNN and MKNN without feature selection scored poorly, the highest score that could be produced being an average f1-score of 30%. In conclusion, the best method to classify informatics journals is MKNN with a feature selection method, preferably GI, but MI is also capable of producing satisfying results.</span></p>
<h2><a name="bookmark51"></a><span class="font2" style="font-weight:bold;"><a name="bookmark52"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] C. C. Aggarwal and C. Zhai, Eds., </span><span class="font2" style="font-style:italic;">Mining Text Data</span><span class="font2">. Boston, MA: Springer US, 2012.</span></p></li>
<li>
<p><span class="font2">[2] C. Manning, P. Raghavan, and H. Schuetze, </span><span class="font2" style="font-style:italic;">Introduction to Information Retrieval</span><span class="font2">. Cambridge:</span></p></li></ul>
<p><span class="font2">Cambridge University Press, 2009.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[3] &nbsp;&nbsp;&nbsp;M. Azam, T. Ahmed, F. Sabah, and M. I. Hussain, “Feature Extraction based Text Classification using K-Nearest Neighbor Algorithm,” </span><span class="font2" style="font-style:italic;">International Journal of Computer Science and Network Security</span><span class="font2">, vol. 18, no. 12, pp. 95–101, Dec. 2018.</span></p></li>
<li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;H. Parvin, H. Alizadeh, and B. Minaei-Bidgoli, “MKNN: Modified K-Nearest Neighbor,” in </span><span class="font2" style="font-style:italic;">Proceedings of the World Congress on Engineering and Computer Science 2008</span><span class="font2">, San Francisco, USA, Oct. 2008, pp. 831–834.</span></p></li>
<li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;L. G. Irham, A. Adiwijaya, and U. N. Wisesty, “Klasifikasi Berita Bahasa Indonesia Menggunakan Mutual Information dan Support Vector Machine,” </span><span class="font2" style="font-style:italic;">mib</span><span class="font2">, vol. 3, no. 4, pp. 284–292, Oct. 2019.</span></p></li>
<li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;T. Setiyorini and R. T. Asmono, “Penerapan Gini Index dan K-Nearest Neighbor Untuk Klasifikasi Tingkat Kognitif Soal Pada Taksonomi Bloom,” </span><span class="font2" style="font-style:italic;">Jurnal Pilar Nusa Mandiri</span><span class="font2">, vol. Vol. 13, no. 2, pp. 209–216, Sep. 2017.</span></p></li>
<li>
<p><span class="font2">[7] C. C. Aggarwal, </span><span class="font2" style="font-style:italic;">Data Mining</span><span class="font2">. Cham: Springer International Publishing, 2015.</span></p></li>
<li>
<p><span class="font2">[8] T. M. Cover and J. A. Thomas, </span><span class="font2" style="font-style:italic;">Elements of Information Theory</span><span class="font2">, 2nd ed. Hoboken, New Jersey:</span></p></li></ul>
<p><span class="font2">John Wiley &amp;&nbsp;Sons, Inc, 2006.</span></p>
<p><span class="font8" style="font-style:italic;">This page is intentionally left blank.</span></p>
<p><span class="font2">296</span></p>