---
layout: full_article
title: "A Feature-Driven Decision Support System for Heart Disease Prediction Based on Fisher's Discriminant Ratio and Backpropagation Algorithm"
author: "Muh Dimas Yudianto, Tresna Maulana Fahrudin, Aryo Nugroho"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-56815 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-56815"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-56815"  
comments: true
---

<p><span class="font1" style="font-weight:bold;">LONTAR KOMPUTER VOL. 11, NO. 2 AUGUST 2020</span></p>
<p><span class="font1" style="font-weight:bold;">DOI : 10.24843/LKJITI.2020.v11.i02.p01</span></p>
<p><span class="font1" style="font-weight:bold;">Accredited B by RISTEKDIKTI Decree No. 51/E/KPT/2017</span></p>
<p><span class="font1" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font1" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>A Feature-Driven Decision Support System for Heart Disease Prediction Based on Fisher's Discriminant Ratio and Backpropagation Algorithm</span></h1>
<p><span class="font1">Muh Dimas Yudianto<sup>1</sup>, Tresna Maulana Fahrudin<sup>2</sup>, Aryo Nugroho<sup>3</sup></span></p>
<p><span class="font1"><sup>123</sup> Fakultas Ilmu Komputer, Universitas Narotama</span></p>
<p><span class="font1">JL.Arif Rachman Hakim 51 Surabaya, Jawa Timur, Indonesia </span><a href="mailto:1dimasyudianto92@gmail.com"><span class="font6" style="text-decoration:underline;"><sup>1</sup></span><span class="font1" style="text-decoration:underline;">dimasyudianto92@gmail.com</span><span class="font1">,</span></a><a href="mailto:2tresna.maulana@narotama.ac.id"><span class="font1"> </span><span class="font1" style="text-decoration:underline;"><sup>2</sup>tresna.maulana@narotama.ac.id</span><span class="font1">,</span></a><span class="font1"> </span><a href="mailto:3aryo.nugroho@narotama.ac.id"><span class="font1" style="text-decoration:underline;"><sup>3</sup>aryo.nugroho@narotama.ac.id</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">Coronary heart disease included a group of cardiovascular, and it is a leading cause of death in low and middle-income countries. Risk factors for coronary heart disease are divided into two, namely primary and secondary risk factors. The need to identify characteristics or risk factors in heart disease patients by making the classification model. The modeling of heart disease classification to know how the system can able to reach the best prediction accuracy. Fisher's Discriminant Ratio is one of the methods for feature selection, which is used to get high discriminant features. While Backpropagation is one of the classification models to recognize patterns in heart disease patients. The experiment results showed that the accuracy of the classification model using 13 original features reached 92%. By reducing the features based on the score of the feature selection, then the lowest feature was removed from original features and left there were 12 features involved in the classification model which the accuracy increased to 93%. Furthermore, the results of determining the threshold (accuracy does not decrease continuously) and consider the effect of eliminating the lowest features that are considered quite fluctuating on accuracy. The accuracy reached 90% by eliminating the five lowest features and left eight existing features.</span></p>
<p><span class="font1" style="font-weight:bold;">Keywords: </span><span class="font1" style="font-style:italic;">Heart Disease, Discriminant Features, Fisher's Discriminant Ratio, Neural Network, Backpropagation</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font1">Coronary Heart Disease (CHD) is a heart disease that is a leading cause of death in low and middle-income countries such as Indonesia. Based on death cases caused by cardiovascular disease reached 17.1 million people per year [1]. Cardiovascular included coronary heart disease and stroke, which ranks first in chronic diseases in the world [2]. The second factor causing coronary heart disease is antioxidants [3]. Antioxidants are compounding that function to reduce the formation of free radioactive obtained from food intake. One part of antioxidants is vitamin E. The main function of vitamin E in the body is as a natural antioxidant that plays a role in capturing and inhibiting the process of lipid oxidation in the body. To inhibit oxidation, vitamin E will provide a hydrogen atom from the OH group into radical lipid peroxide, which is radical. Therefore, vitamin E is formed stable and not easily damaged and able to stop the free radical sequence with fat [4].</span></p>
<p><span class="font1">Hypercholesterolemia is a dangerous condition characterized by high levels of cholesterol in the blood. This is a serious problem because it is one of the main risk factors for coronary heart disease [5]. Coronary heart disease has a high mortality and illness. Although the basic cause of coronary heart disease is not known with certainty, experts have identified many factors related to the occurrence of heart disease, which is called a risk factor. The risk for coronary heart disease consists of 2 conditions, namely primary (independent) and secondary risk factors [6].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">a. &nbsp;Primary risk factors: these factors can cause arterial disorders in the form of atherosclerosis</span></p></li></ul>
<p><span class="font1">without having to be helped by other factors (independent), such as hyperlipidemia, smoking, and hypertension.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">b. &nbsp;&nbsp;&nbsp;Secondary risk factors: these factors can only cause arterial abnormalities if other factors are found together, such as diabetes mellitus (DM), obesity, stress, lack of exercise, alcohol, and family history [7].</span></p></li></ul>
<p><span class="font1">These earlier works related to heart disease research was carried out by [8] using the 13 features from [9]. All used a GA-based RFNN procedure to diagnose heart disease. The outcomes told that the percentage of accuracy rate reached 97.78%.</span></p>
<p><span class="font1">The other research was also carried out by [10] using data collection of Statlog Heart Disease, Cleveland heart disease, and Pima Indian Diabetes datasets from [9]. The true results of classifiers have given 93.55% and 73.77% for the Cleveland Heart Disease dataset, with two and five class labels. And 92.54% for the Pima India Diabetes dataset, also 94.44% for the Statlog Heart Disease dataset.</span></p>
<p><span class="font1">This research will propose the feature selection before classification using Backpropagation. The feature selection is expected to improve the quality of the dataset before classification. Various classification algorithms are widely known, such as Naïve Bayes, K-Nearest Neighbor [11], and others, but this study uses the Backpropagation algorithm, which is part of the Artificial Neural Network [12].</span></p>
<p><span class="font1" style="font-weight:bold;">2. Research Methods</span></p><img src="https://jurnal.harianregional.com/media/56815-1.jpg" alt="" style="width:424pt;height:204pt;">
<p><span class="font1" style="font-weight:bold;">Figure 1. </span><span class="font1">Proposed System Design of Heart Disease Research</span></p>
<p><span class="font1">The proposed system design of heart disease research is illustrated in Figure 1, begin from the collecting heart disease dataset, preprocessing dataset using Z-score normalization, selecting feature using Fisher's Discriminant Ratio, building classification model using Backpropagation and evaluating the classification model</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2.1 &nbsp;&nbsp;&nbsp;Collecting Heart Disease Dataset</span></h2></li></ul>
<p><span class="font1">The dataset used in this study was taken from [9] the dataset consists of heart disease status with 13 predictor features, 2 class labels, and 270 samples. We train the model using training data, which was collected from the original dataset, while the testing data was obtained from training data without labels. We want to see the accuracy of the prediction label on the testing data that match with the actual label. The features used in the heart disease dataset following Table 1.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 1. </span><span class="font1">Heart Disease Features</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font1">No.</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Features</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Values</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">1.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Age</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Continuous {29 to 76 years}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">2.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Gender</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nominal</span></p>
<p><span class="font1">{Male=1, Female=0}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">3.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Chast</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nominal</span></p>
<p><span class="font1">{typical angina=1, atypical angina=2, non anginal</span></p>
<p><span class="font1">pain=3, asymptotic= 4}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">4.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Resting Blood Pressure</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Continuous in mmHg (unit)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">5.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Serum Cholesterol</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Continuous in mg/dl (unit)</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">6.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Fasting Blood Sugar</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nominal</span></p>
<p><span class="font1">{&gt;120mg/dl=1, &lt;120mg/dl=2}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">7.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Resting electrocardiographic result</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nominal</span></p>
<p><span class="font1">{Normal=0, Having ST-T=1, left ventricular hyperthophy=2}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">8.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Maximum heart rate achieved</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Continuous in statistics</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">9.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Exercise-induced angina</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nomimal {yes=1, none=0}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">10.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Oldpeak</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Continuous</span></p>
<p><span class="font1">Displaying an integer or floating value</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">11.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Slope</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nominal</span></p>
<p><span class="font1">{upsloping=1, flat=2, downsloping=3}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">12.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Number of major vessels</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Continuous displaying values as integers or floats</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">13.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Thal</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nominal</span></p>
<p><span class="font1">{normal=3, fixed defect=6, reversible defect=7}</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">14.</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Class</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Nominal</span></p>
<p><span class="font1">{absence=0, presence=1}</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font1" style="font-weight:bold;"><a name="bookmark7"></a>2.2 &nbsp;&nbsp;&nbsp;Normalization</span></h2></li></ul>
<p><span class="font2">Normalization procedure with Z-score is measuring arithmetic mean values and standard deviations from existing data. If the input numbers are not distributed, the normalization of Z-scores cannot maintain the input distribution at the output. This is expected to significant facts, and the standard deviation is the optimal position and only the computation for the Gaussian distribution. For random distribution, the mean and standard deviation are fair estimates of position and measure, severally, but not optimal to drop data refinement assuring data dependences </span><span class="font1">[13]. The following Z-score formula in equation </span><span class="font2">(1). </span><span class="font1">In our experiments, the testing data was obtained from training data that was previously used to create a model, but it is without the label. Thus, the original value of the dataset has been normalized using the Z-score. If the process is separate between training data and using testing data other than training, then the Z-score can be applied by entering testing data into the training data distribution first.</span></p>
<div>
<p><span class="font1">(1)</span></p>
</div><br clear="all">
<p><span class="font9" style="font-style:italic;">(S</span><span class="font8" style="font-style:italic;">y</span><span class="font9" style="font-style:italic;">)</span></p>
<p><span class="font1">In the formula above, </span><span class="font1" style="font-style:italic;">Y</span><span class="font1"> is the actual data for each feature, </span><span class="font1" style="font-style:italic;">My</span><span class="font1"> is the average of each feature, and </span><span class="font10" style="font-style:italic;">Sy</span><span class="font1"> is the standard deviation of each feature.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font1" style="font-weight:bold;"><a name="bookmark9"></a>2.3 &nbsp;&nbsp;&nbsp;Fisher's Discriminant Ratio</span></h2></li></ul>
<p><span class="font1">Fisher's Discriminant Ratio (FDR) is generally used to measure the power of discrimination of individual features in separating two classes based on their values. μ1 and μ2 each is the average value of two classes, σ1 and σ2 each is a variant of two classes in the feature to be measured. FDR is formulated as in the following equation (2).</span></p>
<div>
<p><span class="font11" style="font-style:italic;">FDR</span></p>
</div><br clear="all">
<div>
<p><span class="font9" style="text-decoration:underline;">(μi </span><span class="font9" style="font-style:italic;text-decoration:underline;">-μ)2<sup>2 </sup></span><span class="font9"><sup>( </sup></span><span class="font9" style="font-style:italic;"><sup>2</sup>I+<sup>2</sup>)<sup>2</sup>)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(2)</span></p>
</div><br clear="all">
<p><span class="font1">The results given by FDR are features that have large differences in the average of the class and small variants of each class. Therefore a high FDR value will be obtained. If two features have the same absolute mean difference but differ in the number of variants of the value </span><span class="font10" style="font-style:italic;">(σ</span><span class="font8" style="font-style:italic;">?</span><span class="font10"> + </span><span class="font10" style="font-style:italic;">σ</span><span class="font10"><sub>2</sub></span><span class="font8"><sup>2</sup></span><span class="font1">), then features with a smaller number of variants will get a higher FDR value. On the other hand, if two features have the same number of variants but a greater average difference, a higher FDR value will be obtained [14].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font1" style="font-weight:bold;"><a name="bookmark11"></a>2.4 &nbsp;&nbsp;&nbsp;Backpropagation</span></h2></li></ul>
<p><span class="font1">Backpropagation has numerous units that are in one or more hidden layers [15]. Figure 2 explains the Backpropagation architecture with input N (with bias), the hidden layer that happens from unit P (with bias), and the unit of output M. is the line weight from the input unit to the hidden display unit ( is the line weight connecting the bias to the input unit to hidden units). Is from the hidden layer unit to output unit Y ( is the weight of the bias in the hidden layer to the output unit ).</span></p>
<div><img src="https://jurnal.harianregional.com/media/56815-2.jpg" alt="" style="width:228pt;height:178pt;">
<p><span class="font1" style="font-weight:bold;">Figure 2. </span><span class="font1">Backpropagation Architecture</span></p>
</div><br clear="all">
<p><span class="font1">The activation function in the Backpropagation method used in this study is the sigmoid function. The sigmoid function has values in the range of 0 to 1. Therefore, this function is used for neural networks that require output values located at intervals of 0 to 1 [16]. The sigmoid function formula follows in equation (3).</span></p>
<div>
<p><span class="font1">(3)</span></p>
</div><br clear="all">
<p><span class="font11" style="font-style:italic;">f</span><span class="font11">(x) = ι+e-* </span><span class="font1">with derivatives </span><span class="font11" style="font-style:italic;">f(<sup>χ</sup>)</span><span class="font11"> = </span><span class="font11" style="font-style:italic;">f(<sup>x</sup>)</span><span class="font11"> (1 — </span><span class="font11" style="font-style:italic;">f(<sup>x</sup>))</span></p>
<p><span class="font1">While the curve of the sigmoid function is illustrated in Figure 3.</span></p><img src="https://jurnal.harianregional.com/media/56815-3.jpg" alt="" style="width:228pt;height:137pt;">
<p><span class="font1" style="font-weight:bold;">Figure 3. </span><span class="font1">Sigmoid Function</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font1" style="font-weight:bold;"><a name="bookmark13"></a>2.5 &nbsp;&nbsp;&nbsp;Confusion Matrix</span></h2></li></ul>
<p><span class="font1">The confusion matrix contains information that compares the results of the classification that should be, namely, the match between the actual label and prediction label. The following Figure 4 illustrates the confusion matrix [17].</span></p>
<p><span class="font5" style="font-weight:bold;">Classifier Prediction</span></p>
<p><span class="font5" style="font-weight:bold;">Actual</span></p>
<p><span class="font5" style="font-weight:bold;">Value</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font0">Positive</span></p></td><td style="vertical-align:top;">
<p><span class="font0">Negative</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">True</span></p>
<p><span class="font0">Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">False</span></p>
<p><span class="font0">Negative</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">Negative</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">False</span></p>
<p><span class="font0">Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">True</span></p>
<p><span class="font0">Negative</span></p></td></tr>
</table>
<p><span class="font1" style="font-weight:bold;">Figure 4. </span><span class="font1">Confusion Matrix</span></p>
<p><span class="font1">The explanation of TP, TN, FN, FP as follows:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">a.</span><span class="font1" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;TP </span><span class="font1">is True Positive, which is a match between the actual label and the predictive label on a sample of patients affected by heart disease</span></p></li>
<li>
<p><span class="font1">b.</span><span class="font1" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;TN </span><span class="font1">is True Negative, which is a match between the actual label and the predictive label on a sample of patients not affected by heart disease</span></p></li>
<li>
<p><span class="font1">c.</span><span class="font1" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;FN </span><span class="font1">is False Negative, which is a mismatch between the actual label and the predictive label on a sample of patients that are predicted to be negative (not affected by heart disease) but the facts are positive (affected by heart disease)</span></p></li>
<li>
<p><span class="font1">d.</span><span class="font1" style="font-weight:bold;"> &nbsp;&nbsp;&nbsp;FP </span><span class="font1">is False Positive, which is a mismatch between the actual label and the predictive label on a sample of patients that are predicted to be positive (affected by heart disease) but the facts are negative (not affected by heart disease)</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font1" style="font-weight:bold;"><a name="bookmark15"></a>2.6 &nbsp;&nbsp;&nbsp;Evaluation Result</span></h2></li></ul>
<p><span class="font1">The evaluation result is an assessment using a formula by comparing the portion of data that is correctly classified and the portion of data that is misclassified [18]. Table 2 showed the evaluation result using accuracy, precision, and recall.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 2</span><span class="font1">. Evaluation Result</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font1">Evaluation</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Formula</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Accuracy</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1" style="font-style:italic;">TP</span><span class="font10"> + </span><span class="font1" style="font-style:italic;">TN</span></p>
<p><span class="font4">∗</span><span class="font10"> 100%</span></p>
<p><span class="font1" style="font-style:italic;">TP</span><span class="font10"> + </span><span class="font1" style="font-style:italic;">TN</span><span class="font10"> + </span><span class="font1" style="font-style:italic;">FP</span><span class="font10"> + </span><span class="font1" style="font-style:italic;">FN</span></p>
<p><span class="font1" style="font-style:italic;">TP</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">Precision</span></p></td><td style="vertical-align:top;">
<p><span class="font4">∗</span><span class="font10"> 100%</span></p>
<p><span class="font1" style="font-style:italic;">FP</span><span class="font10"> + </span><span class="font1" style="font-style:italic;">TP</span></p>
<p><span class="font1" style="font-style:italic;">TP</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font4">∗</span><span class="font10"> 100%</span></p>
<p><span class="font1" style="font-style:italic;">FN</span><span class="font10"> + </span><span class="font1" style="font-style:italic;">TP</span></p></td></tr>
</table>
<p><span class="font1">The explanation of accuracy, precision, and recall as follows:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">a. &nbsp;&nbsp;&nbsp;Accuracy is the percentage of comparison between correctly classified data and the whole data.</span></p></li>
<li>
<p><span class="font1">b. &nbsp;&nbsp;&nbsp;Precision is the percentage of the amount of confident category data (heart disease) that is precisely classified divided by the total data classified as positive.</span></p></li>
<li>
<p><span class="font1">c. &nbsp;&nbsp;&nbsp;Recall is the percentage of the amount of confident category data (heart disease) accurately classified by the system.</span></p></li></ul>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font1" style="font-weight:bold;"><a name="bookmark17"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font1">The experiment result of this research reported about the normalization of data distribution, feature selection using Fisher's Discriminant Ratio, which was represented in feature ranking, classification for building model using Backpropagation, and also evaluation using confusion matrix.</span></p>
<p><span class="font1" style="font-weight:bold;">3.1 Preprocessing using Z-score normalization</span></p><img src="https://jurnal.harianregional.com/media/56815-4.jpg" alt="" style="width:397pt;height:188pt;">
<p><span class="font1" style="font-weight:bold;">Figure 5. </span><span class="font1">The Data Distribution Before Normalization</span></p>
<p><span class="font1">Figure 5 illustrates the condition of the original data of heart disease before the normalization process. The range or scale of data for each feature varies, feature values are mixed between units, tens, and hundreds. This results in the dimensions of the dataset being unbalanced. The </span><span class="font1" style="font-style:italic;">X</span><span class="font1">-axis represents the data sequence number, the </span><span class="font1" style="font-style:italic;">Y</span><span class="font1">-axis is the data value, and the colored lines show different features, whereas the results of normalization using the Z-score are illustrated in Figure 6.</span></p><img src="https://jurnal.harianregional.com/media/56815-5.jpg" alt="" style="width:409pt;height:196pt;">
<p><span class="font1" style="font-weight:bold;">Figure 6. </span><span class="font1">The Data Distribution After Normalization</span></p>
<p><span class="font1">Figure 6 illustrates the normalized heart disease data distribution, where the data scale for each feature is on a balanced scale, it is between -3 to 3. The </span><span class="font1" style="font-style:italic;">X</span><span class="font1">-axis represents the data sequence number, the </span><span class="font1" style="font-style:italic;">Y</span><span class="font1">-axis is the Z-score value, and the colored lines show different features.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font1" style="font-weight:bold;"><a name="bookmark19"></a>3.2 &nbsp;&nbsp;&nbsp;Feature Selection using Fisher's Discriminant Ratio (FDR)</span></h2></li></ul><img src="https://jurnal.harianregional.com/media/56815-6.jpg" alt="" style="width:359pt;height:185pt;">
<p><span class="font1" style="font-weight:bold;">Figure 7</span><span class="font1">. Feature Selection using Fisher's Discriminant Ratio</span></p>
<p><span class="font1">The feature selection process will test each of the features, which is the most influential features of the dataset. At the beginning process, Fisher's Discriminant Ratio (FDR) splits the dataset into two groups according to their class. Second, it calculates the average of each feature in its own class. Third, it calculates the total variance of each feature in its own class. Fourth, it calculates the FDR value using equation (2) from the second and third calculation results. The </span><span class="font1" style="font-style:italic;">X</span><span class="font1">-axis shows the names of the predictor features, while the </span><span class="font1" style="font-style:italic;">Y</span><span class="font1">-axis is the FDR score for each predictor feature.</span></p>
<p><span class="font1">Figure 7 was illustrated the feature selection, which was represented in Feature Ranking by FDR. In the test results, it was reported that the 'thal' feature has a high discriminant value on the dataset reached 0.75976, while 'fasting blood sugar' feature has a low discriminant value only reached of 0.000541.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark20"></a><span class="font1" style="font-weight:bold;"><a name="bookmark21"></a>3.3 &nbsp;&nbsp;&nbsp;Classification using Backpropagation Algorithm</span></h2></li></ul>
<p><span class="font1">The backpropagation method of this research used 13 features with two classes. Backpropagation architecture in this experiment consists of 13 input neurons (13 features) and one output neuron (two classes: 0 or 1). The number of hidden layers in this experiment used one hidden layer with four neurons. To determine the number of neurons in the hidden layer, used the formula √ (</span><span class="font1" style="font-style:italic;">m x n</span><span class="font1">), where </span><span class="font1" style="font-style:italic;">m</span><span class="font1"> is the input layer, and </span><span class="font1" style="font-style:italic;">n</span><span class="font1"> is the output layer. Therefore, the number of neurons in the hidden layer are obtained optimally. The tools used in this experiment are Python programming language, we configure the Backpropagation with the number of learning rates = 30, target error = 0.5.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark22"></a><span class="font1" style="font-weight:bold;"><a name="bookmark23"></a>3.4 &nbsp;&nbsp;&nbsp;Feature Selection and Classification Accuracy Improvement</span></h2></li></ul><img src="https://jurnal.harianregional.com/media/56815-7.jpg" alt="" style="width:387pt;height:167pt;">
<p><span class="font1" style="font-weight:bold;">Figure 8. </span><span class="font1">The Effect of Feature Selection on The Accuracy Classification</span></p>
<p><span class="font1">Figure 8 illustrated the classification result when all features involved in the classification model reached 92%, and the next step was carried out to remove the first lowest feature with an accuracy value reached 93%. Then, it was removed the two lowest features with an accuracy reached 28%, and the accuracy was increased to reach 90% when removed the three lowest features. It was continued to remove the four lowest features with an accuracy that decreased to 88%, and the accuracy was increased to reach 90% when removed five lowest features. The eight features obtained are the features that have the best discrimination level, while the five eliminated features do not mean anything to the dataset because the level of discrimination is low. When it was removed the six lowest features, the accuracy was decreased to 89% and getting decreased until it removed the 12 lowest features, in which the accuracy reached 28%.</span></p>
<p><span class="font1">The feature selection process as a way to determine whether the effect of accuracy is generated when built classification model by reducing the lowest number of features through feature selection by the FDR. We analyze the results of this experiment to show that when removing the two lowest features, accuracy reaches 28%. This indicates that the second-lowest feature (serum cholesterol) is an important feature, while the first lowest feature is not important (fasting blood sugar). Then, the model chosen is the dataset that has eliminated the first lowest feature (fasting blood sugar) that can achieve 93% accuracy. Therefore, it remains decided that the highest-level accuracy in the classification model of the heart disease dataset was reached 93% by removing one feature. However, to determine the number of features that need to be removed from the dataset does not depend on increasing accuracy at the beginning of removing the lowest features, but also looking at fluctuations or accuracy that occur when a number of features are removed.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark24"></a><span class="font1" style="font-weight:bold;"><a name="bookmark25"></a>3.5 &nbsp;&nbsp;&nbsp;Evaluation of Classification Accuracy</span></h2></li></ul>
<p><span class="font1">To know the performance of the classification model based on the Backpropagation algorithm, it needs to use the confusion matrix. This matrix helped to know the frequency of match between the actual label and predicted label.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 3. </span><span class="font1">Confusion Matrix Result of Heart Disease (Original Features)</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font1">Presence</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Absence</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Presence</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">143</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">7</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Absence</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">15</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">105</span></p></td></tr>
</table>
<p><span class="font1" style="font-weight:bold;">Table 4. </span><span class="font1">Classification Accuracy Result of Heart Disease (Original Features) Target &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Precision &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recall &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font7">Accuracy</span></p>
<p><span class="font1">0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.91 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.95</span></p>
<p><span class="font1">1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.94 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.88 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.92</span></p>
<p><span class="font1">Table 3 reported that there are 143 heart disease patients who match between the actual label: presence and predicted label: presence (True Positive), while seven patients who are no match between the actual label: presence and predicted label: absence (False Negative). The other cases reported there are 105 heart disease patients who match between the actual label: absence and predicted label: absence (True Negative), while 15 patients who are no match between the actual label: absence and predicted label: absence (False Positive). Therefore, the evaluation results in Table 4 reported that the precision of target 0: 91% and recall 95%, while the precision of target 1: 94% and recall 88%. Then, the accuracy of the classification reached 92%. Table 3 and Table 4 are reported of the experiment using 13 original features of heart disease.</span></p>
<p><span class="font1">In the second dataset by using Fisher Discriminant Ratio (FDR) results which was removed the first lowest feature scores, the test results obtained are:</span></p>
<table border="1">
<tr><td colspan="3" style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">Table 5. </span><span class="font1">Confusion Matrix Result of Heart Disease (FDR Features)</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font1">Presence</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Absence</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Presence</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">142</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">8</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">Absence</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">110</span></p></td></tr>
</table>
<p><span class="font1" style="font-weight:bold;">Table 6. </span><span class="font1">Classification Accuracy Result of Heart Disease (FDR Features)</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font1">Target</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Precision</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Recall</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Accuracy</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">0</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.93</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">0.95</span></p></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">1</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0.93</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0.92</span></p></td><td style="vertical-align:top;">
<p><span class="font1">0.93</span></p></td></tr>
</table>
<p><span class="font1">Table 5 reported that there are 142 heart disease patients who match between the actual label: presence and predicted label: presence (True Positive), while eight patients who are no match between the actual label: presence and predicted label: absence (False Negative). The other cases reported there are 110 heart disease patients who match between the actual label: absence and predicted label: absence (True Negative), while ten patients who are no match between the actual label: absence and predicted label: absence (False Positive). Therefore, the evaluation results in Table 6 reported that the precision of target 0: 93% and recall 95%, while the precision of target 1: 93% and recall 92%. Then, the accuracy of the classification reached 93%. Table 5 and Table 6 are reported of the experiment using 12 features of heart disease based on FDR scores. %. The results of the accuracy level in this study are similar to the research of [10] with an accuracy rate of CHD 93.55%. But must get the same results, this study provides another contribution in the form of feature selection from 13 existing features become smaller. There is also a study with the same result, which is 93.33% using the χ2-Gaussian Naive Bayes method [19].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark26"></a><span class="font1" style="font-weight:bold;"><a name="bookmark27"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font1">The classification of heart disease using the Fisher Discriminant Ratio (FDR) and Backpropagation obtained pretty good results. Feature selection using FDR applied to 13 features that had been carried out the normalization process with the Z-score before, it was</span></p>
<p><span class="font1">given results that 'thal' feature as the highest discriminant feature with a score of 0.75976 while 'fasting blood sugar' feature as the lowest feature with a score of 0.000541. The classification model using Backpropagation reached an accuracy to 92% with 13 original features of the heart disease dataset. The feature selection using Fisher's Discriminant Ratio was given the important information that there is the one lowest discriminant feature with the lowest score of the heart disease dataset, which recommended removing from the dataset. Therefore, the combination between FDR and Backpropagation, given the improvement of classification model accuracy of heart disease dataset, reached 93. The suggestion for future works is needed to evaluate the feature not only single feature evaluation like Fisher's Discriminant Ratio, but also use multi-features evaluation like exhaustive search algorithm to obtain the best combination feature and can improve the accuracy of the classification model.</span></p>
<p><span class="font1" style="font-weight:bold;">References</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[1] &nbsp;&nbsp;&nbsp;L. DeRoo </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;Placental abruption and long-term maternal cardiovascular disease mortality: a population-based registry study in Norway and Sweden,&quot; </span><span class="font2" style="font-style:italic;">European Journal of Epidemiology</span><span class="font2">, vol. 31, no. 5, pp. 501–511, 2016.</span></p></li>
<li>
<p><span class="font2">[2] &nbsp;L. Soares-Miranda, D. S. Siscovick, B. M. Psaty, W. Longstreth Jr, and D.</span></p></li></ul>
<p><span class="font2">Mozaffarian, &quot;Physical activity and risk of coronary heart disease and stroke in older adults: the cardiovascular health study,&quot; </span><span class="font2" style="font-style:italic;">Circulation</span><span class="font2">, vol. 133, no. 2, pp. 147–155, 2016.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[3] &nbsp;P. Zhang, X. Xu, X. Li, and others, &quot;Cardiovascular diseases: oxidative damage</span></p></li></ul>
<p><span class="font2">and antioxidant protection,&quot; </span><span class="font2" style="font-style:italic;">European Review for Medical and Pharmacological Sciences</span><span class="font2">, vol. 18, no. 20, pp. 3091–3096, 2014.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[4] &nbsp;&nbsp;&nbsp;K. A. Wojtunik-Kulesza, A. Oniszczuk, T. Oniszczuk, and M. Waksmundzka-Hajnos, “The influence of common free radicals and antioxidants on development of Alzheimer’s Disease,” </span><span class="font2" style="font-style:italic;">Biomedicine &amp;&nbsp;Pharmacotherapy</span><span class="font2">, vol. 78, pp. 39–49, 2016.</span></p></li>
<li>
<p><span class="font2">[5] &nbsp;&nbsp;&nbsp;U. Bhalani and P. Tirgar, &quot;A Comparative Study for Investigation into Beneficial Effects of Ketoconazole and Ketoconazole+ Cholestyramine Combination in Hyperlipidemia and The Complications Associated With It.,&quot; </span><span class="font2" style="font-style:italic;">Advances in Bioresearch</span><span class="font2">, vol. 6, no. 4, 2015.</span></p></li>
<li>
<p><span class="font2">[6] &nbsp;&nbsp;&nbsp;J. L. Mega </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;Genetic risk, coronary heart disease events, and the clinical benefit of statin therapy: an analysis of primary and secondary prevention trials,&quot; </span><span class="font2" style="font-style:italic;">The Lancet</span><span class="font2">, vol. 385, no. 9984, pp. 2264–2271, 2015.</span></p></li>
<li>
<p><span class="font2">[7] &nbsp;&nbsp;&nbsp;J. Bruthans </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;Educational level and risk profile and risk control in patients with coronary heart disease,&quot; </span><span class="font2" style="font-style:italic;">European Journal of Preventive Cardiology</span><span class="font2">, vol. 23, no. 8, pp. 881–890, 2016.</span></p></li>
<li>
<p><span class="font2">[8] &nbsp;&nbsp;&nbsp;K. Uyar and A. Ihan, &quot;Diagnosis of heart disease using genetic algorithm based trained recurrent fuzzy neural networks,&quot; </span><span class="font2" style="font-style:italic;">Procedia Computer Science</span><span class="font2">, vol. 120, pp. 588–593, 2017.</span></p></li>
<li>
<p><span class="font2">[9] &nbsp;&nbsp;&nbsp;&quot;UCI Machine Learning Repository: &nbsp;&nbsp;Heart Disease Data Set.&quot;</span></p></li></ul>
<p><a href="https://archive.ics.uci.edu/ml/datasets/Heart+Disease"><span class="font2">https://archive.ics.uci.edu/ml/datasets/Heart+Disease</span></a><span class="font2"> (accessed May 02, 2020).</span></p>
<ul style="list-style:none;"><li>
<p><span class="font2">[10] &nbsp;&nbsp;&nbsp;K. B. Nahato, K. H. Nehemiah, and A. Kannan, &quot;Hybrid approach using fuzzy sets and extreme learning machine for classifying clinical datasets,&quot; </span><span class="font2" style="font-style:italic;">Informatics in Medicine Unlocked</span><span class="font2">, vol. 2, pp. 1–11, 2016.</span></p></li>
<li>
<p><span class="font2">[11] &nbsp;&nbsp;&nbsp;A. R. Pratama, M. Mustajib, and A. Nugroho, “Deteksi Citra Uang Kertas dengan Fitur RGB Menggunakan K-Nearest Neighbor,” </span><span class="font2" style="font-style:italic;">Jurnal Eksplora Informatika</span><span class="font2">, vol. 9, no. 2, pp. 163–172, Mar. 2020, doi: 10.30864/eksplora.v9i2.336.</span></p></li>
<li>
<p><span class="font2">[12] &nbsp;&nbsp;&nbsp;Mirwan, A. Nugroho, F. Hendarta, R. Hidayatillah, F. Hassan, and K. P. Nana, &quot;Virtual Assistant Using Lstm Networks In Indonesian,&quot; in </span><span class="font2" style="font-style:italic;">2018 International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)</span><span class="font2">, Nov. 2018, pp. 652–655, doi: 10.1109/ISRITI.2018.8864448.</span></p></li>
<li>
<p><span class="font2">[13] &nbsp;&nbsp;&nbsp;S. Jain, S. Shukla, and R. Wadhvani, &quot;Dynamic selection of normalization techniques using data complexity measures,&quot; </span><span class="font2" style="font-style:italic;">Expert Systems with Applications</span><span class="font2">, vol. 106, pp. 252–262, Sep. 2018, doi: 10.1016/j.eswa.2018.04.008.</span></p></li>
<li>
<p><span class="font2">[14] &nbsp;&nbsp;&nbsp;M. F. Tresna, S. Iwan, and R. B. Ali, &quot;Data Mining Approach for Breast Cancer Patient Recovery,&quot; </span><span class="font2" style="font-style:italic;">EMITTER International Journal of Engineering Technology</span><span class="font2">, vol. 5, no. 1, pp. 36–71, 2017.</span></p></li>
<li>
<p><span class="font2">[15] &nbsp;&nbsp;&nbsp;N. Borisagar, D. Barad, and P. Raval, &quot;Chronic Kidney Disease Prediction Using Back Propagation Neural Network Algorithm,&quot; in </span><span class="font2" style="font-style:italic;">Proceedings of International Conference on Communication and Networks</span><span class="font2">, 2017, pp. 295–303.</span></p></li>
<li>
<p><span class="font2">[16] &nbsp;&nbsp;&nbsp;A. Wanto, A. P. Windarto, D. Hartama, and I. Parlina, &quot;Use of Binary Sigmoid Function And Linear Identity In Artificial Neural Networks For Forecasting Population Density,&quot; </span><span class="font2" style="font-style:italic;">International Journal of Information System and Technology</span><span class="font2">, vol. 1, no. 1, pp. 43–54, 2017.</span></p></li>
<li>
<p><span class="font2">[17] &nbsp;&nbsp;&nbsp;B. M. Jadav and V. B. Vaghela, &quot;Sentiment analysis using support vector machine based on feature selection and semantic analysis,&quot; </span><span class="font2" style="font-style:italic;">International Journal of Computer Applications</span><span class="font2">, vol. 146, no. 13, 2016.</span></p></li>
<li>
<p><span class="font2">[18] &nbsp;&nbsp;&nbsp;Z. Cömert and A.F Kocamaz, &quot;Comparison of machine learning techniques for fetal heart rate classification,&quot; </span><span class="font2" style="font-style:italic;">Acta Physica Polonica A</span><span class="font2">, vol. 132, no. 3, pp. 451– 454, 2017.</span></p></li>
<li>
<p><span class="font2">[19] &nbsp;&nbsp;&nbsp;L. Ali </span><span class="font2" style="font-style:italic;">et al.</span><span class="font2">, &quot;A Feature-Driven Decision Support System for Heart Failure Prediction Based on χ2 Statistical Model and Gaussian Naive Bayes,&quot; </span><span class="font2" style="font-style:italic;">Computational and Mathematical Methods in &nbsp;Medicine</span><span class="font2">, &nbsp;2019, doi:</span></p></li></ul>
<p><span class="font2">10.1155/2019/6314328.</span></p>
<p><span class="font1">75</span></p>