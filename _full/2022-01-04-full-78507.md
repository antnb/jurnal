---
layout: full_article
title: "IDENTIFICATION OF HOAX BASED ON TEXT MINING USING K-NEAREST NEIGHBOR METHOD"
author: "I Wayan Santiyasa, Gede Putra Aditya Brahmantha, I Wayan Supriana, I GA Gede Arya Kadyanan, I Ketut Gede Suhartana, Ida Bagus Made Mahendra"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-78507 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-78507"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-78507"  
comments: true
---

<p><span class="font1">p-ISSN: 2301-5373</span></p>
<p><span class="font1">e-ISSN: 2654-5101</span></p>
<p><span class="font1">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font1">Volume 10, No 2. November 2021</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font5" style="font-weight:bold;"><a name="bookmark1"></a>Identification Of Hoax Based On Text Mining Using K-Nearest Neighbor Method</span></h1>
<p><span class="font1">I Wayan Santiyasa<sup>a1</sup>, Gede Putra Aditya Brahmantha</span><span class="font0">a</span><span class="font1"><sup>2</sup>, I Wayan Supriana</span><span class="font0">a</span><span class="font1"><sup>3</sup></span><span class="font17">, </span><span class="font1">I GA Gede Arya Kadyanan<sup>a4</sup>, I Ketut Gede Suhartana<sup>a5</sup>, Ida Bagus Made Mahendra<sup>a6</sup></span></p>
<p><span class="font1"><sup>a</sup>Informatics Department</span></p>
<p><span class="font1"><sup>a</sup>Faculty of Math and Natural Sciences, Udayana University</span></p>
<p><span class="font1">Bali, Indonesia</span></p>
<p><a href="mailto:1santiyasa67@gmail.com"><span class="font1"><sup>1</sup>santiyasa67@gmail.com</span></a></p>
<p><a href="mailto:2adit.hermawan333@gmail.com"><span class="font1"><sup>2</sup>adit.hermawan333@gmail.com</span></a></p>
<p><a href="mailto:3wayan.supriana@unud.ac.id"><span class="font1"><sup>3</sup>wayan.supriana@unud.ac.id</span></a></p>
<p><a href="mailto:4gungde@unud.ac.id"><span class="font1"><sup>4</sup>gungde@unud.ac.id</span></a></p>
<p><a href="mailto:5suhartana@unud.ac.id"><span class="font1"><sup>5</sup>suhartana@unud.ac.id</span></a></p>
<p><a href="mailto:6ibm.mahendra@unud.ac.id"><span class="font1"><sup>6</sup>ibm.mahendra@unud.ac.id</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">At this time, information is very easy to obtain, information can spread quickly to all corners of society. However, the information that spreaded are not all true, there is false information or what is commonly called hoax which of course is also easily spread by the public. the public only thinks that all the information circulating on the internet is true. From every news published on the internet, it cannot be known directly that the news is a hoax or valid one. The test uses 740 random contents / issue data that has been verified by an institution where 370 contents are hoaxes and 370 contents are valid. The test uses the K-Nearest Neighbor algorithm, before the classification process is performed, the preprocessing stage is performed first and uses the TF-IDF equation to get the weight of each feature, then classified using K-Nearest Neighbor and the test results is evaluated using 10-Fold Cross Validation. The working principle of the K-Nearest Neighbor algorithm is that the data testing is classified based on the closest neighbors with the most data training. The test uses the k value with a value of 2 to 10. The value of k states how many neighbors or data are closest to an object that the system could identify. The optimal use of the k value in the implementation is obtained at a value of k = 4 with precision, recall, and F-Measure results of 0.764856, 0.757583, and 0.751944 respectively and an accuracy of 75.4%</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">k-nearest neighbor, text mining, classification</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h3></li></ul>
<p><span class="font1">At this time information is very easy to obtain, information can spread quickly to all corners of society. With the rapid development of IT, now with just the tap of a finger, someone can get and share information. However, the information that is spread is not all true, there is false information or what is commonly called Hoax which of course is also easily spread by the public. People only think that all information circulating on the internet is true. Ironically, hoax can make the community itself uneasy and there can even be disputes due to circulating hoaxes.</span></p>
<p><span class="font1">There are several studies that discuss the identification of hoaxes previously, based on a study entitled The Naïve Bayes Experiment on Detecting Hoax News in Indonesian by Rahutomo, F., Pratiwi, IYR, &amp;&nbsp;Ramadhani DM, 2019. In this study, static tests were performed on 600 random news with the percentage of training and test data of 60% : 40%, 70% : 30%, and 80% : 20% and each percentage is performed three times the test time so that the average value of accuracy is 82.3%, 82.7% and 83%. The percentage of training and test data is 80%:20%, resulting in the highest average accuracy compared to other data percentages. The nave Bayes method can classify online news in Indonesian with an average accuracy of 82.6% for 600 news data [1]. Another research is the K-Nearest Neighbor</span></p>
<p><span class="font1">Classification Method for Hoax Analysis by Amin, A.I., 2019. The method used is Multinomial Naive Bayes and K-Nearest Neighbor. The documents collected are 300 hoax documents and 300 non-hoax documents in Indonesian. The Documents were taken from websites, broadcast messages and hoax posts on social networks. The best accuracy result with the KNN method is 81.67% and the best accuracy result with the Multinomial Naive Bayes method is 93.33% [2]. The difference from previous researches are this research mainly focus on the classfication result that influenced by the changes of the k value and using different method in preprocessing and weighting.</span></p>
<p><span class="font1">Based on the existing problems and related research that forms the basis for conducting research, therefore the researcher wants to conduct research that identifies hoaxes using the K-Nearest Neighbor method. The accuracy of hoax identification using the K-Nearest Neighbor method will be influenced by the changes of the k value. The value of k states how many neighbors or data are closest to an object that the system could identify.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark6"></a>2.1. &nbsp;&nbsp;&nbsp;Research Stage</span></h3></li></ul>
<p><span class="font1">The research stages are data collection, preprocessing, TF-IDF weighting, KNN Classification, and evaluation.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark7"></a><span class="font1" style="font-weight:bold;"><a name="bookmark8"></a>2.2. &nbsp;&nbsp;&nbsp;Data Collection</span></h3></li></ul>
<p><span class="font1">The research uses 740 random content/issue data that has been verified by TurnBackHoax, of which 370 hoax content data and 370 valid content data. The data is obtained by doing a web crawling on the TurnBackHoax site, each content will be converted into a .txt document and labeled (Hoax or valid) according to the manually verified articles performed by TurnBackHoax institution. All data will be divided into training and testing data using the 10-fold cross validation method. In the research conducted, the data used is only content data that uses Indonesian language.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 1. </span><span class="font1">Example of Dataset</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font2">Tweet</span></p></td><td style="vertical-align:bottom;">
<p><span class="font2">Label</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font2">“ Z sampaikan teman2 untuk sementara waktu jgn ke lippo dulu sdh zona merah, 14 orang terpapar d lippo ” dapat info dr teman semoga Ini cm HOAX </span><span class="font4">@ @ @</span></p></td><td style="vertical-align:top;">
<p><span class="font2">hoax</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Pemberitaan mengenai PT Dirgantara Indonesia (Persero) dijual ke pihak asing, kami nyatakan HOAX. Berita ini adalah berita bohong yang berulang dari tahun 2017 lalu.</span></p></td><td style="vertical-align:top;">
<p><span class="font2">valid</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark9"></a><span class="font1" style="font-weight:bold;"><a name="bookmark10"></a>2.3. &nbsp;&nbsp;&nbsp;Preprocessing</span></h3></li></ul>
<p><span class="font1">The preprocessing stage is a process to prepare raw data before other processes are carried out. In general, preprocessing is done by eliminating inappropriate data or converting data into a form that is easier to process by the system. Preprocessing is very important in conducting sentiment analysis, especially for social media which mostly contains informal and unstructured words or sentences and has a lot of noise [3]. The preprocessing stage consists of the case folding, data cleansing, language normalization, stopwords removal, stemming, and tokenization, those methods are described below:</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark11"></a><span class="font1" style="font-weight:bold;"><a name="bookmark12"></a>2.3.1. &nbsp;&nbsp;&nbsp;Case Folding</span></h3></li></ul>
<p><span class="font1">Case folding is the first stage in preprocessing that aims to replace word by word from uppercase to lowercase letters.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 2. </span><span class="font1">Case Folding Processj</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">iBefore Case Foldingi</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">iAfter Case Foldingi</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">“ Z sampaikan teman2 untuk sementara waktu jgn ke lippo dulu sdh zona merah, 14 orang terpapar d lippo ” dapat info dr teman semoga Ini cm HOAX </span><span class="font3">@ @ @</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">&quot; z sampaikan teman2 untuk sementara waktu jgn ke lippo dulu sdh zona merah, 14 orang terpapar d lippo &quot;&nbsp;dapat info dr teman semoga ini cm hoax </span><span class="font3">@ @ @</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark13"></a><span class="font1" style="font-weight:bold;"><a name="bookmark14"></a>2.3.2. &nbsp;&nbsp;&nbsp;Data Cleansing</span></h3></li></ul>
<p><span class="font1">Data Cleansing aims to cleans the text by removing irrelevant text such as username, emoticons, links, and so on.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 3. </span><span class="font1">Data Cleansing Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">iBefore Data Cleansing</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">iAfter Data Cleansingi</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">&quot; z sampaikan teman2 untuk sementara waktu jgn ke lippo dulu sdh zona merah, 14 orang terpapar d lippo &quot;&nbsp;dapat info dr teman semoga ini cm hoax </span><span class="font3">@ @ @</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">&quot; z sampaikan teman2 untuk sementara waktu jgn ke lippo dulu sdh zona merah, 14 orang terpapar d lippo &quot;&nbsp;dapat info dr teman semoga ini cm hoax</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark15"></a><span class="font1" style="font-weight:bold;"><a name="bookmark16"></a>2.3.3. &nbsp;&nbsp;&nbsp;Language Normalization</span></h3></li></ul>
<p><span class="font1">Language normalization aims to replace common words abbreviations into the original word and replace non-standard words into standard words.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 4. </span><span class="font1">Word List Example for Language Normalization Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">WordiBefore Language Normalizationi</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">WordiAfter Language Normalizationi</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">bhs</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">bahasa</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">ngomong</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">berkata</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">kpd</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">kepada</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">nggak</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">tidak</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">temen</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">teman</span></p></td></tr>
</table>
<p><span class="font1" style="font-weight:bold;">Table 5. </span><span class="font1">Language Normalization Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">iBefore Language Normalizationi</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">iAfter Language Normalizationi</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">&quot; z sampaikan teman2 untuk sementara waktu jgn ke lippo dulu sdh zona merah, 14 orang terpapar d lippo &quot;&nbsp;dapat info dr teman semoga ini cm hoax</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">&quot; z sampaikan teman untuk sementara waktu jangan ke lippo dulu sudah zona merah, 14 orang terpapar di lippo &quot;&nbsp;dapat info dari teman semoga ini cuma hoax</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark17"></a><span class="font1" style="font-weight:bold;"><a name="bookmark18"></a>2.3.4. &nbsp;&nbsp;&nbsp;Stopword Removal</span><span class="font2">j</span></h3></li></ul>
<p><span class="font1">A stopword is a list of not important and not very relevant common words. In this process, these common words are deleted in order to reduce the number of words processed by the system.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 6. </span><span class="font1">Stopword Removal Processj</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">iBefore Stopword Removali</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">iAfter Stopword Removali</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">&quot; z sampaikan teman untuk sementara waktu jangan ke lippo dulu sudah zona merah, 14 orang terpapar di lippo &quot;&nbsp;dapat info dari teman semoga ini cuma hoax</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">&quot; z teman lippo zona merah, 14 orang terpapar lippo &quot;&nbsp;info teman semoga hoax</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark19"></a><span class="font1" style="font-weight:bold;"><a name="bookmark20"></a>2.3.5. &nbsp;&nbsp;&nbsp;Stemming</span></h3></li></ul>
<p><span class="font1">Stemming is changing affixed words into basic words, this proccess is performed using Sastrawi library.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 7. </span><span class="font1">Stemming Process</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">iBefore Stemmingi</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">iAfter Stemmingi</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">&quot; z teman lippo zona merah, 14 orang terpapar lippo &quot;&nbsp;info teman semoga hoax</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">“ z teman lippo zona merah 14 orang papar lippo “ info teman moga hoax</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark21"></a><span class="font1" style="font-weight:bold;"><a name="bookmark22"></a>2.3.6. &nbsp;&nbsp;&nbsp;Tokenization</span><span class="font2">j</span></h3></li></ul>
<p><span class="font1">Tokenization is the process of separating words from a text into multiple tokens. This process will remove any spaces too.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 8. </span><span class="font1">Tokenization Processj</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">iBefore Tokenizationi</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">iAfter Tokenizationi</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font1">“ z teman lippo zona merah 14 orang papar lippo “ info teman moga hoax</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">['z', 'teman', 'lippo', 'zona', 'merah', '14', 'orang', 'papar', 'lippo', 'info', 'teman', 'moga', 'hoax']</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark23"></a><span class="font1" style="font-weight:bold;"><a name="bookmark24"></a>2.4. &nbsp;&nbsp;&nbsp;Term Frequency Inverse Document Frequency (TF-IDF) </span><span class="font1">j</span></h3></li></ul>
<p><span class="font1">The TFIDF is a method to calculate the weight of each word used in the classfication, this method generally used in information retrieval. Accuracy and efficiency is the key value of this simple method. The method of Term Frequency Inverse Document Frequency (TFIDF) is a method of assigning weighting a relationship a word (term) to a document. The TFIDF is a statistical measure used to evaluate how important a word is in a sentence or a document. For a single document each sentence is considered a document. The frequency with which a word appears in a specific document show how important the word is in the document. Document frequency containing the word indicates how common the word is. The weight of the word gets smaller if it appears in many documents and gets bigger if it appears frequently in a document [1].</span></p>
<p><span class="font1">The step to find weight with TF-IDF method are:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">a. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Find term frequency </span><span class="font1" style="font-style:italic;">(tf)</span></p></li>
<li>
<p><span class="font1">b. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Find weighting term frequency (W</span><span class="font0">tf</span><span class="font1">) i</span></p></li></ul>
<p><span class="font15" style="font-style:italic;">_ c</span><span class="font15"> 1 + </span><span class="font15" style="font-style:italic;">log10tft,d,jikatft,d</span><span class="font15"> &gt;0</span></p>
<p><span class="font8" style="font-style:italic;"><sup>Wt</sup>f</span><span class="font15" style="font-style:italic;">~</span><span class="font16" style="font-style:italic;">[</span><span class="font15"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font1">(1)</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">c. &nbsp;&nbsp;&nbsp;Find document frequency (df) i</span></p></li>
<li>
<p><span class="font1">d. &nbsp;&nbsp;&nbsp;Find the weight of inverse document frequency (idf)</span></p></li></ul>
<p><span class="font15" style="font-style:italic;">Idf<sub>t</sub> = LogJ-</span><span class="font1"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)</span></p>
<p><span class="font8" style="font-style:italic;"><sup>a</sup>J</span><span class="font13" style="font-style:italic;">t</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">e. &nbsp;&nbsp;&nbsp;Find the weight of TF-IDFi</span></p></li></ul>
<p><span class="font15" style="font-style:italic;">W<sub>t4</sub></span><span class="font15"> = </span><span class="font15" style="font-style:italic;">Wtf <sub>tdl</sub> X idf<sub>t</sub></span><span class="font1"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)</span></p>
<p><span class="font1">Notes :</span></p>
<p><span class="font1">tf</span><span class="font0">t,d </span><span class="font1">= term frequencyi</span></p>
<p><span class="font15" style="font-style:italic;">Wt</span><span class="font1" style="font-style:italic;">f </span><span class="font12" style="font-style:italic;">t</span><span class="font0" style="font-style:italic;">,</span><span class="font12" style="font-style:italic;">d</span><span class="font1"> = weight of term frequency</span></p>
<p><span class="font1">df = the number of times the document contains a termi</span></p>
<p><span class="font1">N = the total number of documents. i</span></p>
<p><span class="font15" style="font-style:italic;">W</span><span class="font12" style="font-style:italic;">t</span><span class="font0" style="font-style:italic;">,</span><span class="font12" style="font-style:italic;">d</span><span class="font1"> = weight of TF-IDF.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark25"></a><span class="font1" style="font-weight:bold;"><a name="bookmark26"></a>2.5. &nbsp;&nbsp;&nbsp;K-Nearest Neighbor</span></h3></li></ul>
<p><span class="font1">K-Nearest Neighbor (KNN) is a method for classifying objects based on learning data (neighbors) that are closest to the object. Near or far neighbors are usually calculated based on the Euclidean distance. required a classification system as a system capable of finding information. The KNN method is divided into two phases, namely learning (training) and classification or testing (testing). In the learning phase, this algorithm only performs feature vector storage and classification of learning data. In the classification phase, the same features are calculated for the data to be tested (whose classification is unknown). The distance from this new vector to all learning data vectors is calculated, and the k closest neighbors are taken. [5].</span></p>
<p><span class="font1">The steps are performed as followsj:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">a. &nbsp;&nbsp;&nbsp;Define the k value.</span></p></li>
<li>
<p><span class="font1">b. &nbsp;&nbsp;&nbsp;Count the distance of the object of each data group. The distance calculation used the Euclidean distance equation.</span></p></li></ul>
<p><span class="font15" style="font-style:italic;">D(x,y) =</span><span class="font15"> √∑‰</span><span class="font8">ι</span><span class="font15">(%<sub>i</sub> - </span><span class="font15" style="font-style:italic;">y)</span><span class="font1"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)</span></p>
<p><span class="font1">Notes :</span></p>
<p><span class="font1">D = Distancej</span></p>
<p><span class="font1">x = Train dataj</span></p>
<p><span class="font1">y = Test dataj</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">c. &nbsp;&nbsp;&nbsp;Classification results obtained.</span></p></li></ul>
<ul style="list-style:none;"><li>
<h3><a name="bookmark27"></a><span class="font1" style="font-weight:bold;"><a name="bookmark28"></a>2.6. &nbsp;&nbsp;&nbsp;K-Fold Cross Validation</span><span class="font1">j</span></h3></li></ul>
<p><span class="font1">In K-fold cross validation, the initial data are randomly partitioned into k mutually different subsets or &quot;folds,&quot; D1, D2, ..., Dk, each of the partitions is approximately the same size. Training and testing is performed k times. In iteration i, the partition Di is used as the test set, and the remaining partitions are collectively reserved to train the model. In the first iteration, the subsets D2, ..., Dk collectively used as the training data set to get the first model, which is tested on D1; the second iteration is trained on the subsets D1, D3, ..., Dk and tested on D2; etc. Unlike the holdout and random subsampling methods, here each sample is used the same amount for training and</span></p>
<p><span class="font1">once for testing. In general, 10-Fold Cross validation is great for estimating accuracy due to its relatively low bias and variance. [6]</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark29"></a><span class="font1" style="font-weight:bold;"><a name="bookmark30"></a>2.7. &nbsp;&nbsp;&nbsp;Evaluation</span></h3></li></ul>
<p><span class="font1">The evaluation is performed to find how the classification is performed, in this research, evaluation will be conducted using Confusion Matrix in each iteration of K-Fold Cross Validation.</span></p>
<p><span class="font1">Confusion matrix is a method used to perform accuracy calculations on the concept of data mining [7]. The following table shows the Confusion Matrix for the two-class classification model.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 9. </span><span class="font1">Confusion Matrix</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:top;">
<p><span class="font1">Prediction result: Valid</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Prediction</span></p>
<p><span class="font1">Result: Hoax</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">Source: Valid</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">TN</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">FP</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">Source: Hoax</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">FN</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">TP</span></p></td></tr>
</table>
<p><span class="font1">In this study, the meaning of entries in the confusion matrix are as follows: following:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">• TP for correct prediction that the document is a Hoax</span></p></li>
<li>
<p><span class="font1">• TN for correct prediction that the document is Valid</span></p></li>
<li>
<p><span class="font1">• FP for false prediction that the document is a Hoax</span></p></li>
<li>
<p><span class="font1">• FN for false prediction that the document is Valid</span></p></li></ul>
<p><span class="font1">The formula for calculating system performance using entries from the confusion matrix is as follows:</span></p>
<div>
<p><span class="font8" style="font-style:italic;">.. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP</span></p>
<p><span class="font1">True Positive Rate =</span></p>
<p><span class="font8" style="font-style:italic;">TP+FN</span></p>
<p><span class="font1">True Negative Rate = </span><span class="font11" style="font-style:italic;font-variant:small-caps;text-decoration:line-through;"><sup>tn</sup></span></p>
<p><span class="font8" style="font-style:italic;">TN+FP</span></p>
<p><span class="font8" style="font-style:italic;">TP+TN</span></p>
<p><span class="font1" style="font-style:italic;">Accuracy</span><span class="font1"> = </span><span class="font9" style="font-variant:small-caps;text-decoration:line-through;">tp+tn+fp+fn</span><span class="font17" style="font-variant:small-caps;">×</span><span class="font18" style="font-variant:small-caps;"><sup>100</sup></span><span class="font17" style="font-variant:small-caps;">%</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(5)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(6)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">(7)</span></p>
</div><br clear="all">
<p><span class="font1">Precision = ----</span><span class="font15">× 100%</span></p>
<div>
<p><span class="font1">(8)</span></p>
<p><span class="font1">(9)</span></p>
<p><span class="font1">(10)</span></p>
</div><br clear="all">
<p><span class="font8" style="font-style:italic;">TN+FN</span></p>
<p><span class="font1">Recall = </span><span class="font15">— ×100%</span></p>
<p><span class="font8" style="font-style:italic;">TP+FN</span></p>
<p><span class="font8">2</span></p>
<p><span class="font1">F-Measure = —11—</span></p>
<p><span class="font13" style="font-style:italic;">Precision</span><span class="font8" style="font-style:italic;"><sup>+</sup></span><span class="font13" style="font-style:italic;">Accuracy</span></p>
<p><span class="font1">Precision is the proportion of positive predicted cases that are also true positives in the actual data. Recall is the proportion of positive cases that are actually predicted to be true positives. [8]</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark31"></a><span class="font1" style="font-weight:bold;"><a name="bookmark32"></a>3. &nbsp;&nbsp;&nbsp;Research Results and Discussion</span></h3></li></ul>
<p><span class="font1">The research uses 740 random content/issue data that has been verified by TurnBackHoax, of which 370 hoax content data and 370 valid content data. The data processed through the preprocessing stage, those are changing all letters to lowercase, and then purging irrelevant text, language normalization, removing stopwords, returning words to their basic form, and finally separating sentences into words. After going through the preprocessing stage, the TF-IDF weighting was</span></p>
<p><span class="font1">performed with following formula (3). After the weights of each words are obtained, classification is performed using the K-Nearest Neighbor.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark33"></a><span class="font1" style="font-weight:bold;"><a name="bookmark34"></a>3.1. &nbsp;&nbsp;&nbsp;The Test of Parameter k</span></h3></li></ul>
<p><span class="font1">Tests were performed on the KNN Algorithm using K-Fold Cross Validation which resulted in 10 combinations of training data and test data, each partition contains 10% of the data. The data is obtained from the TurnBackHoax Institution which is labeled according to the type of identification manually according to the label on the TurnBackHoax institution. Each combination of training data and test data is included in the KNN process. The tests performed include testing the changes in the value of k starting from k=2, k=3, to k=10, the optimal k value is obtained from the k value which produces the highest accuracy from other k values.</span></p>
<p><span class="font1">Experiments were performed using the KNN algorithm with the aim of comparing the value of the k parameter and finding the value of k which resulted in higher accuracy in the application of the KNN algorithm and the number of k used was 2,3,4, to 10. Evaluating the test of each change in the value of k in the Classification KNN is performed using the 10-Fold Cross validation method.</span></p>
<p><span class="font1" style="font-weight:bold;">Table 10. </span><span class="font1">Tests Result</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font6" style="font-style:italic;">Neighbors</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6" style="font-style:italic;">Precision</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6" style="font-style:italic;">Recall</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6" style="font-style:italic;">F-Measure</span></p></td><td style="vertical-align:bottom;">
<p><span class="font6" style="font-style:italic;">Accuracy</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.697585</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.736544</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.733128</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.735135</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.766831</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.758818</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.752873</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.755405</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.764856</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.757583</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.751944</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.754054</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.716095</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.698657</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.68765</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.695946</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">6</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.748055</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.729735</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.720707</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.727027</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.703489</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.671075</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.653432</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.668919</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.73615</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.707349</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.693543</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.704054</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">9</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.683088</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.635909</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.60719</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.635135</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font6">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.684051</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.64859</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.62693</span></p></td><td style="vertical-align:middle;">
<p><span class="font6">0.647297</span></p></td></tr>
</table>
<h3><a name="bookmark35"></a><span class="font10" style="font-weight:bold;"><a name="bookmark36"></a>TEST OF PARAMETER K RESULT IN K-NEAREST NEIGHBOR</span></h3>
<p><span class="font14">0.8</span></p>
<div><img src="https://jurnal.harianregional.com/media/78507-1.jpg" alt="" style="width:278pt;height:98pt;">
<p><span class="font14">—♦— Precision &nbsp;—■— Recall &nbsp;—⅛- F-Measure &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Accuracy</span></p>
<p><span class="font1" style="font-weight:bold;">Figure 1. </span><span class="font1">Graph of Test Results</span></p>
</div><br clear="all">
<p><span class="font14">0.75</span></p>
<p><span class="font14">0.7</span></p>
<p><span class="font14">0.65</span></p>
<p><span class="font14">0.6</span></p>
<h2><a name="bookmark37"></a><span class="font2"><a name="bookmark38"></a>Graph of the effect of the value of k on the accuracy of KNN</span></h2>
<p><span class="font13" style="font-weight:bold;">0.78</span></p><img src="https://jurnal.harianregional.com/media/78507-2.jpg" alt="" style="width:330pt;height:136pt;">
<p><span class="font1" style="font-weight:bold;">Figure 2. </span><span class="font1">Graph of Test Results</span></p>
<p><span class="font1">Table 8 shows the results of the research on the effect of the k parameter on the results of the KNN classification. The test starts from k value is 2 which continues until k value is 10. The best results are found when k value is 3 with an average accuracy of 0.755 or 75.5% for each fold and the average F-Measure for each fold is 0.752 or 75 ,2%. From Figure 3, it can be seen that the classification results increase at its peak at k = 3 and then tend to decrease in proportion to the increase in the value of k. Since the value of k is the number of closest neighbors of the classification, tests involving a value of k that are too high cause the training data to be more biased which results in lower accuracy and gives poor results.</span></p>
<p><span class="font1">The difference in precision between k=3 and k=4 is 0.02 and the difference in accuracy is 0.001 so that the performance of KNN with k=4 is the most optimal k and with a lower F value indicates that the value of k=4 is more significant than k=3 .</span></p>
<p><span class="font1">From these results, it can be seen the effect of the value of k on accuracy, it can be seen that the accuracy increases from k=2 until the peak point of accuracy is at k=3 and then the accuracy tends to decrease in proportion to the increase in the value of k. In this experiment, the highest accuracy was obtained at 75.5% using the value of k=3 and 75.4% using the value of k=4.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark39"></a><span class="font1" style="font-weight:bold;"><a name="bookmark40"></a>4. &nbsp;&nbsp;Conclusions and Suggestions</span></h3></li></ul>
<p><span class="font1">From results of the research above that has been completed, it should be noted that the K-Nearest Neighbor method can be used to identify hoaxes in news documents, Testing the value of k is performed with a value of 2 to 10. The optimal use of the k value in the implementation is obtained at a value of k=4 with precision, recall, and F-Measure results of 0.764856, 0.757583, and 0.751944 and an accuracy of 75,4%. In this research the author only discusses K-Nearest Neighbor. Therefore, further researchers can develop other classification methods considering the wide range of classification methods and can be developed by applying them to different fields of science and case studies.</span></p>
<h3><a name="bookmark41"></a><span class="font1" style="font-weight:bold;"><a name="bookmark42"></a>References</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;F. Rahutomo, I. Y. R. Pratiwi, and D. M. Ramadhani, “Eksperimen Naïve Bayes Pada Deteksi</span></p></li></ul>
<p><span class="font1">Berita Hoax Berbahasa Indonesia,” J. Penelit. Komun. DAN OPINI PUBLIK, 2019, doi: 10.33299/jpkop.23.1.1805.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;A.I. Amin, Metode Klasifikasi K-Nearest Neighbor Untuk Analisis Hoax. Thesis, Bogor</span></p></li></ul>
<p><span class="font1">Agricultural University (IPB), Bogor. 2019.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[3] &nbsp;&nbsp;&nbsp;S. Mujilahwati, “Pre-Processing Text Mining Pada Data Twitter,” Semin. Nas. Teknol. Inf. dan</span></p></li></ul>
<p><span class="font1">Komun., vol. 2016, no. Sentika, pp. 2089–9815, 2016.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[4] &nbsp;&nbsp;V. Amrizal, “PENERAPAN METODE TERM FREQUENCY INVERSE DOCUMENT</span></p></li></ul>
<p><span class="font1">FREQUENCY (TF-IDF) DAN COSINE SIMILARITY PADA SISTEM TEMU KEMBALI INFORMASI UNTUK MENGETAHUI SYARAH HADITS BERBASIS WEB (STUDI KASUS: HADITS SHAHIH BUKHARI-MUSLIM),” J. Tek. Inform., 2018, doi: 10.15408/jti.v11i2.8623.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;M. M. Baharuddin, H. Azis, and T. Hasanuddin, “ANALISIS PERFORMA METODE K-</span></p></li></ul>
<p><span class="font1">NEAREST NEIGHBOR UNTUK IDENTIFIKASI JENIS KACA,” Ilk. J. Ilm., 2019, doi: 10.33096/ilkom.v11i3.489.269-274.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[6] &nbsp;&nbsp;&nbsp;J. Han, M. Kamber, and J. Pei, Data Mining: Concepts and Techniques. 2012.</span></p></li>
<li>
<p><span class="font1">[7] &nbsp;&nbsp;T. Rosandy, “PERBANDINGAN METODE NAIVE BAYES CLASSIFIER DENGAN METODE</span></p></li></ul>
<p><span class="font1">DECISION TREE (C4.5) UNTUK MENGANALISA KELANCARAN PEMBIAYAAN (Study Kasus : KSPPS / BMT AL-FADHILA,” J. Teknol. Inf. Magister Darmajaya, vol. 2, no. 01, pp. 52–62, 2016.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[8] &nbsp;&nbsp;&nbsp;D. M. W. Powers, “Evaluation: from precision, recall and F-measure to ROC, informedness,</span></p></li></ul>
<p><span class="font1">markedness and correlation,” pp. 37–63, 2020, [Online]. Available: </span><a href="http://arxiv.org/abs/2010.16061"><span class="font1">http://arxiv.org/abs/2010.16061</span></a><span class="font1">.</span></p>
<p><span class="font7" style="font-style:italic;">This page is intentionally left blank. Halaman ini sengaja dikosongkan.</span></p>
<p><span class="font1">226</span></p>