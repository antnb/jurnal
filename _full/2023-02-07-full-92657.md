---
layout: full_article
title: "Implementasi BERT pada Analisis Sentimen Ulasan Destinasi Wisata Bali"
author: "Tristan Bey Kusuma, I Komang Ari Mogi, S.Kom., M.Kom."
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-92657 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-92657"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-92657"  
comments: true
---

<p><span class="font3">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font3">Volume 12, No 2. November 2023</span></p>
<p><span class="font3">p-ISSN: 2301-5373</span></p>
<p><span class="font3">e-ISSN: 2654-5101</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font4" style="font-weight:bold;"><a name="bookmark1"></a>Implementasi BERT pada Analisis Sentimen Ulasan Destinasi Wisata Bali</span></h1>
<p><span class="font3">Tristan Bey Kusuma<sup>a1</sup></span><span class="font3" style="font-weight:bold;">, </span><span class="font3">I Komang Ari Mogi<sup>a2</sup></span></p>
<p><span class="font3"><sup>a</sup>Program Studi Informatika, Fakultas Matematika dan Ilmu Pengetahuan Alam, Universitas Udayana</span></p>
<p><span class="font3">Bali, Indonesia </span><a href="mailto:1tristanbeykusuma@email.com"><span class="font3"><sup>1</sup>tristanbeykusuma@email.com</span></a><span class="font3"> </span><a href="mailto:2arimogi@unud.ac.id"><span class="font3"><sup>2</sup>arimogi@unud.ac.id</span></a></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font3" style="font-style:italic;">In recent years, the contribution of the tourism sector in Bali has increased significantly. The tourism sector has an important role as a source of foreign exchange earnings, and can encourage national economic growth. With the digital age, online opinion are increasingly vital to the growth of Indonesian tourism internationally. Public opinion and reviews on these tourist destinations can be used to identify new tourist destinations which are gaining traction and are in demand. Which is why it will be important to leverage these positive or negative opinions to acquire interesting and vital information on these tourist destinations for further use. One such method to acquire such information are through sentiment analysis to determine whether the a review’s attitude towards a particular tourist destination or experience is positive, negative, or neutral. This study aims to use IndoBERT, a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model for the Indonesian language. This model is trained using a masked language modeling (MLM) objective and next sentence prediction (NSP) objective. This study also compares two different optimizers with a weight decay fix, AdamW and AdaFactor. The results show that sentiment analysis using the IndoBERT model with the AdamW optimizer reaches 97% accuracy and AdaFactor reaches 98,2% accuracy.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font3" style="font-style:italic;">Sentiment Analysis, Review, Natural Language Understanding (NLU), Natural Language Generation (NLG), Natural Language Processing (NLP), BERT, AdamW, Transformer</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark2"></a><span class="font3" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Pendahuluan</span></h3></li></ul>
<p><span class="font3">Kontribusi industri pariwisata telah berkembang pesat dalam tahun-tahun terakhir ini. Proporsi industri pariwisata terhadap keseluruhan ekspor barang dan jasa, yang meningkat secara dramatis dari 10% pada tahun 2005 menjadi 17% pada tahun 2012, menunjukkan hal ini. Kenaikan kontribusi ini terutama diakibatkan oleh meningkatnya jumlah wisatawan lokal dan internasional serta meningkatnya investasi di industri pariwisata [1]. Naiknya industri pariwisata ini dapat memicu peningkatan keuntungan devisa, menciptakan lapangan kerja, dan mempromosikan perluasan bisnis pariwisata yang terkait. Hal ini tentunya menarik negara-negara lain untuk membangun industri pariwisata mereka. Melalui berbagai mekanisme, seperti meningkatnya keuntungan devisa dan iming-iming investasi asing, pariwisata secara signifikan membantu pertumbuhan ekonomi [2]. Hal ini khususnya di provinsi Bali.</span></p>
<p><span class="font3">Provinsi Bali, yang oleh kegiatan pariwisatanya telah dikenal sebagai salah satu kantong devisa Indonesia, hal tersebut menandakan bahwa sektor pariwisata perlu mendapat perhatian sebagai sektor utama dalam mendukung perekonomian makro Bali dan perekonomian Indonesia pada umumnya [3]. Namun, pandemi Covid-19 telah mengakibatkan perlambatan bahkan penurunan pertumbuhan ekonomi pariwisata di yang drastis. Pertumbuhan ekonomi yang mengacu Produk Domestik Bruto (PDB) sektor akomodasi dan pariwisata pada kuartal II-2020 tercatat berkurang sebanyak -33,99% dan - 24,9%. Dengan kontribusi yang besar, turunnya PDB sektor akomodasi dan transportasi juga menimbulkan pertumbuhan negatif pada ekonomi Bali. Pada kuartal II-200 PCB Bali tercatat sebesar -11,06% [4].</span></p>
<p><span class="font3">Namun baru-baru ini, setelah restriksi dan larangan perjalanan telah dibuka, wisatawan dari mancanegara akhirnya dapat kembali berwisata ke Bali. Hal ini tentunya menyebabkan suatu permintaan yang naik. Permintaan yang dihasilkan oleh kegiatan pariwisata dalam bentuk permintaan konsumen dan investasi, akan mendorong ekspansi atau pertumbuhan. Naiknya permintaan wisata ini dapat terlihat dalam ulasan dan opini dalam media sosial maupun situs wisata online. Sentimen atau pendapat ini dapat berupa positif, negatif, maupun netral. Maka dari itu, untuk melakukan analisis atas berbagai opini yang berbeda dan mengkategorikannya kita dapat memanfaatkan machine learning. </span><span class="font3" style="font-style:italic;">Sentiment analysis</span><span class="font3"> atau analisis sentiment adalah suatu proses NLP (</span><span class="font3" style="font-style:italic;">Natural Language Processing</span><span class="font3">) untuk mengidentifikasi dan mengkategorikan pendapat yang diungkapkan dalam sepotong teks, terutama untuk menentukan apakah sikap penulis terhadap topik tertentu adalah positif, negatif, atau netral. Analisis sentiment ini kemudian dapat dijadikan dasar dari sebuah sistem rekomendasi dan visualisasi data. Salah satu metode machine learning untuk melakukan hal ini adalah model transformer.</span></p>
<p><span class="font3">BERT atau </span><span class="font3" style="font-style:italic;">Bidirectional Encoder Representations from Transformers</span><span class="font3"> adalah model </span><span class="font3" style="font-style:italic;">deep learning</span><span class="font3"> di mana setiap elemen </span><span class="font3" style="font-style:italic;">output</span><span class="font3"> terhubung ke setiap elemen </span><span class="font3" style="font-style:italic;">input</span><span class="font3">, dan bobot di antara mereka dihitung secara dinamis berdasarkan koneksi tersebut. Secara historis, model bahasa hanya dapat membaca input teks secara berurutan, baik kiri ke kanan atau kanan ke kiri. Tetapi model seperti ini tidak dapat melakukan keduanya secara bersamaan. BERT berbeda karena dirancang untuk membaca dua arah sekaligus. Menggunakan kemampuan dua arah ini, BERT telah di </span><span class="font3" style="font-style:italic;">pre-train</span><span class="font3"> pada tugas-tugas NLP yaitu </span><span class="font3" style="font-style:italic;">Masked Language Modeling</span><span class="font3"> dan </span><span class="font3" style="font-style:italic;">Next Sentence Prediction</span><span class="font3"> [5]. Dalam pengembangan BERT ini, kita dapat memanfaatkan beberapa model seperti IndoBERT.</span></p>
<p><span class="font3">IndoBERT adalah model bahasa mutakhir untuk bahasa Indonesia berdasarkan model BERT. IndoBERT ini telah dilatih pada kumpulan dataset teks Bahasa Indonesia yang mencakup empat milliar kata. Pada penelitian IndoBERT, telah dibandingkan model IndoBERT tersebut secara ekstensif dengan berbagai </span><span class="font3" style="font-style:italic;">embedding</span><span class="font3"> kata dan model </span><span class="font3" style="font-style:italic;">pre-trained</span><span class="font3"> yang telah dilatih sebelumnya, seperti Multilingual BERT dan XLM-R untuk mengukur efektivitasnya. Hasil menunjukkan bahwa model IndoBERT tersebut mengungguli sebagian besar model lain yang ada. Model IndoBERT mengungguli model Multilingual pada 8 dari 12 tugas. Secara umum, model IndoBERT mencapai skor rata-rata tertinggi pada tugas klasifikasi. Model monolingual seperti IndoBERT mempelajari semantik tingkat sentimen yang lebih baik pada gaya bahasa sehari-hari dan formal daripada model multibahasa, meskipun ukuran model IndoBERT 40%-60% lebih kecil [5].</span></p>
<p><span class="font3">Dalam penelitian ini, penulis mengimplementasikan model IndoBERT untuk melakukan analisis sentiment terhadap data ulasan tempat wisata di Bali. Selain itu, penelitian ini juga akan membandingkan performa </span><span class="font3" style="font-style:italic;">optimizer</span><span class="font3"> dalam melakukan fine-tuning model IndoBERT tersebut. Terdapat dua </span><span class="font3" style="font-style:italic;">optimizer</span><span class="font3"> yang akan dibandingkan, AdamW dan AdaFactor. Dari hasil perbandingan ini diharapkan dapat memilih </span><span class="font3" style="font-style:italic;">optimizer</span><span class="font3"> tepat untuk mengoptimalkan hasil performa IndoBERT tersebut.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark4"></a><span class="font3" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Metode Penelitian</span></h3></li></ul>
<p><span class="font3">Penelitian ini bertujuan membandingkan metode klasifikasi sentimen terhadap ulasan tempat wisata di Bali menggunakan model transformer IndoBERT. Pemerintah dan stakeholder lainnya di bidang pariwisata membutuhkan model analisis sentimen ini sebagai solusi atas permasalahan prioritas pengembangan destinasi wisata melalui identifikasi obyek wisata yang diminati. Teknik </span><span class="font3" style="font-style:italic;">deep learning</span><span class="font3"> yang disebut BERT ini memberikan hasil akurasi yang lebih besar daripada teknik transformer lainnya. BERT ini merupakan teknik </span><span class="font3" style="font-style:italic;">machine learning</span><span class="font3"> untuk </span><span class="font3" style="font-style:italic;">pre-training</span><span class="font3"> pemrosesan bahasa alami (NLP). Teknik IndoBERT dipilih karena berhasil menangkap dengan akurat makna semantik teks yang ditulis dalam bahasa Indonesia. IndoBERT merupakan sebuah model </span><span class="font3" style="font-style:italic;">pre-trained large</span><span class="font3"> yang dilatih dengan sekitar 4 miliar korpus kata dari dataset Bahasa Indonesia khusus, terdiri dari lebih dari 20 Gigabyte data teks. Ini menghasilkan akurasi analisis sentiment bahasa Indonesia yang tinggi. Penelitian ini mencakup beberapa tahapan, yaitu pengumpulan data review teks dan rating melalui </span><span class="font3" style="font-style:italic;">web</span></p>
<p><span class="font3" style="font-style:italic;">crawling</span><span class="font3">, </span><span class="font3" style="font-style:italic;">preprocessing</span><span class="font3">, </span><span class="font3" style="font-style:italic;">splitting</span><span class="font3"> dan </span><span class="font3" style="font-style:italic;">formatting</span><span class="font3"> data, </span><span class="font3" style="font-style:italic;">fine-tuning</span><span class="font3">, </span><span class="font3" style="font-style:italic;">optimization</span><span class="font3">, dan evaluasi hasil. Diagram alur penelitian ditunjukkan pada gambar 1.</span></p><img src="https://jurnal.harianregional.com/media/92657-1.png" alt="" style="width:57pt;height:30pt;">
<p><span class="font5">Pengumpulan Data</span></p><img src="https://jurnal.harianregional.com/media/92657-2.png" alt="" style="width:115pt;height:380pt;">
<p><span class="font3" style="font-weight:bold;">Gambar 1. </span><span class="font3">Diagram Alur Penelitian</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark6"></a><span class="font3" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Pengumpulan Data</span></h3></li></ul>
<p><span class="font3">Data yang digunakan pada penelitian ini bersumber dari </span><span class="font3" style="font-style:italic;">crawling</span><span class="font3"> data pada website menggunakan tools </span><span class="font3" style="font-style:italic;">scraping</span><span class="font3">. Informasi tersebut dikumpulkan dari </span><span class="font3" style="font-style:italic;">review</span><span class="font3"> tempat wisata di pulau Bali yang tertulis dalam bahasa Indonesia dan dimuat dari situs web tripadvisor.com. Data ulasan yang diambil merupakan ulasan </span><span class="font3" style="font-style:italic;">user</span><span class="font3"> pada situs objek wisata. Sehingga data ini dapat dibagi menjadi ulasan bersifat negatif, positif, dan netral. Data tersebut berjumlah 908 dibagi menurut rating. Data tersebut kemudian disimpan dalam file csv. sehingga model analisis sentimen tersebut dapat dilatih. Model yang dibangun menggunakan IndoBERT ini akan</span></p>
<p><span class="font3">melakukan pelatihan serta uji hasil menggunakan data tersebut. Namun, sebelum data diolah, peneliti melakukan pembersihan data yang disebut dengan </span><span class="font3" style="font-style:italic;">pre-processing text</span><span class="font3">.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark8"></a><span class="font3" style="font-weight:bold;"><a name="bookmark9"></a>2.2. &nbsp;&nbsp;&nbsp;Preprocessing Text</span></h3></li></ul>
<p><span class="font3">Tujuan dari </span><span class="font3" style="font-style:italic;">text preprocessing</span><span class="font3"> atau pra-pengolahan adalah untuk merapikan data teks ulasan untuk menghilangkan karakter, kata-kata, dan tanda baca yang tidak diinginkan dalam merepresentasikan makna teks tersebut serta membersihkan data agar dapat digunakan dengan mudah pada tahap pemrosesan selanjutnya. Tahapan </span><span class="font3" style="font-style:italic;">preprocessing</span><span class="font3"> adalah sebagai berikut, diantaranya :</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-style:italic;">a. &nbsp;&nbsp;&nbsp;Case-Folding</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Case-folding</span><span class="font3"> adalah tahapan konversi huruf kecil terhadap setiap huruf atau karakter pada sebuah teks. Dalam penelitian ini, </span><span class="font3" style="font-style:italic;">case-folding</span><span class="font3"> digunakan ketika mengkonversi semua teks dalam </span><span class="font3" style="font-style:italic;">review</span><span class="font3"> dataset menjadi huruf kecil [6].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">b. &nbsp;&nbsp;&nbsp;Tokenisasi</span></p></li></ul>
<p><span class="font3">Dokumen teks dibagi menjadi token melalui proses tokenisasi. Tokenisasi dapat membedah isi teks menjadi kata, frasa, simbol, atau komponen lain yang berguna [6].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-style:italic;">c. &nbsp;&nbsp;&nbsp;Stopword Removal</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Stopword removal</span><span class="font3"> adalah proses menghilangkan kata-kata yang dianggap tidak perlu. Penghapusan </span><span class="font3" style="font-style:italic;">stopword</span><span class="font3"> bertujuan untuk menghilangkan kata-kata yang sering muncul tetapi tidak berkontribusi pada proses analisis data, hanya menyisakan kata-kata yang dianggap dapat menyampaikan makna teks secara akurat [6].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-style:italic;">d. &nbsp;&nbsp;&nbsp;Punctuation Removal</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Punctuation removal</span><span class="font3"> merupakan proses untuk menghilangkan simbol-simbol yang terdapat pada dokumen teks. Ini dapat dicapai dengan menghapus karakter yang tidak penting, termasuk tanda baca, angka, html, karakter, serta simbol lainnya [6].</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-style:italic;">e. &nbsp;&nbsp;&nbsp;Padding</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Padding</span><span class="font3"> adalah penambahan sebuah token untuk mengubah setiap urutan teks agar memiliki panjang yang sama. Teks batch seringkali memiliki panjang yang berbeda, sehingga tidak dapat dikonversi ke tensor atau vektor. </span><span class="font3" style="font-style:italic;">Padding</span><span class="font3"> adalah strategi untuk menambahkan token padding khusus untuk memastikan urutan yang lebih pendek akan memiliki panjang yang sama dengan urutan terpanjang dalam batch atau panjang maksimum yang diterima oleh model [7]. Dalam model BERT ini, panjang maksimum yang penulis gunakan adalah 100.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark10"></a><span class="font3" style="font-weight:bold;"><a name="bookmark11"></a>2.3. &nbsp;&nbsp;&nbsp;Data Splitting</span></h3></li></ul>
<p><span class="font3" style="font-style:italic;">Splitting</span><span class="font3"> atau pembagian dataset dilakukan agar bisa melihat performa model setelah dilatih. Proses pembagian data dibagi menjadi tiga proporsi, yaitu data </span><span class="font3" style="font-style:italic;">training</span><span class="font3"> untuk pelatihan dan </span><span class="font3" style="font-style:italic;">fine-</span><span class="font3">tuning model, data </span><span class="font3" style="font-style:italic;">validation</span><span class="font3"> digunakan untuk mengurangi </span><span class="font3" style="font-style:italic;">overfitting</span><span class="font3"> dan noise pada pelatihan model, dan data </span><span class="font3" style="font-style:italic;">testing</span><span class="font3"> untuk mengevaluasi keakuratan model setelah dilakukan pelatihan. Proporsi data tersebut dibagi dengan persentase 70% sebagai data pelatihan, 15% sebagai data validasi, dan 15% sebagai data pengujian dari total jumlah data. Sehingga, banyaknya data yang digunakan dalam </span><span class="font3" style="font-style:italic;">fine-tuning</span><span class="font3"> awal berjumlah 636.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark12"></a><span class="font3" style="font-weight:bold;"><a name="bookmark13"></a>2.4. &nbsp;&nbsp;&nbsp;Input Formatting</span></h3></li></ul>
<p><span class="font3">Setiap model transformer berbeda tapi memiliki kesamaan dengan yang lain. Oleh karena itu sebagian besar model menggunakan input yang sama, Karena ini, maka kita harus melakukan </span><span class="font3" style="font-style:italic;">encoding</span><span class="font3"> atau pemformatan pada dataset yang kita miliki agar sesuai dengan input model transformer. Ini disebut sebagai </span><span class="font3" style="font-style:italic;">input formatting</span><span class="font3"> [9]. </span><span class="font3" style="font-style:italic;">Input formatting</span><span class="font3"> ini terdiri dari tokenisasi,</span></p>
<p><span class="font3">vektorisasi, dan </span><span class="font3" style="font-style:italic;">mapping</span><span class="font3">. Tokenizer BERT akan mengubah </span><span class="font3" style="font-style:italic;">sequence</span><span class="font3"> kalimat menjadi potongan kata, seperti pada tabel berikut.</span></p>
<p><span class="font3" style="font-weight:bold;">Tabel 1. </span><span class="font3">Tokenisasi &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3" style="text-decoration:underline;">Sequence</span><span class="font3"> </span><span class="font3" style="text-decoration:underline;">T<sup>okens</sup></span><span class="font3"> &nbsp;&nbsp;&nbsp;Kalimat BERT</span></p>
<p><span class="font3">&quot;Saya tidak suka!&quot; ['Saya, 'tidak', ‘suka’, ‘ini’, '!']</span></p>
<p><span class="font3">Kemudian, BertTokenizer.encode_plus() digunakan untuk mengonversi urutan ke dalam format input untuk pengklasifikasi berbasis BERT nanti [8]. BertTokenizer.encode_plus() ini akan mengembalikan kamus tiga objek:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">1.</span><span class="font3" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Input ID</span><span class="font3">, ini sesuai dengan bilangan bulat/urutan token di input</span></p></li>
<li>
<p><span class="font3">2.</span><span class="font3" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Type Token ID</span><span class="font3">, Id ini menunjukkan nomor kalimat yang dimiliki oleh token. (BERT dapat mengambil hingga dua urutan sekaligus).</span></p></li>
<li>
<p><span class="font3">3.</span><span class="font3" style="font-style:italic;"> &nbsp;&nbsp;&nbsp;Attention Mask</span><span class="font3">, mask ini menunjukkan token mana merupakan token yang sebenarnya dan mana yang merupakan token </span><span class="font3" style="font-style:italic;">padding</span><span class="font3"> sehingga perhitungan </span><span class="font3" style="font-style:italic;">attention</span><span class="font3"> akan mengabaikan token </span><span class="font3" style="font-style:italic;">padding</span><span class="font3">.</span></p></li></ul>
<p><span class="font3">Saat </span><span class="font3" style="font-style:italic;">tokenizing</span><span class="font3"> dan vektorisasi, kita dapat menentukan panjang maksimum dari setiap teks. Kemudian, setelah kita menggabungkan langkah untuk tokenisasi, vektorisasi WordPiece, penambahan token khusus, pemotongan ulasan lebih panjang dari panjang maksimal, dan melakukan mapping objek ID dan </span><span class="font3" style="font-style:italic;">attention mask</span><span class="font3"> ke </span><span class="font3" style="font-style:italic;">dictionary</span><span class="font3"> objek, kita dapat melihat hasil seperti berikut.</span></p>
<p><span class="font3" style="font-weight:bold;">Tabel 2. </span><span class="font3">Input Formatting Model BERT</span></p>
<table border="1">
<tr><td colspan="3" style="vertical-align:bottom;">
<p><span class="font3">Sequence : &quot;Saya tidak suka ini!”</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">Input ID</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">Token Type ID</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">Attention Mask</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3">[101, 2123, 1005, 1056, 2066, 2009, 999, 102, 0, 0]</span></p></td><td style="vertical-align:top;">
<p><span class="font9">[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span></p></td><td style="vertical-align:top;">
<p><span class="font10">[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h3><a name="bookmark14"></a><span class="font3" style="font-weight:bold;"><a name="bookmark15"></a>2.5. &nbsp;&nbsp;&nbsp;Fine-Tuning</span></h3></li></ul>
<p><span class="font3">Training dalam model BERT tidak dilakukan dari awal, namun pada model yang sudah dilatih sebelumnya. Model BERT yang telah dilatih sebelumnya telah menyimpan informasi </span><span class="font3" style="font-style:italic;">Language Generation dan Understanding</span><span class="font3"> tentang bahasa Indonesia. Akibatnya, dibutuhkan lebih sedikit waktu untuk melatih model </span><span class="font3" style="font-style:italic;">fine-tuned</span><span class="font3"> tersebut. Dengan </span><span class="font3" style="font-style:italic;">fine-tuning</span><span class="font3"> lapisan bawah jaringan model yang telah dilatih secara luas hanya perlu disetel saat menggunakan </span><span class="font3" style="font-style:italic;">output</span><span class="font3">-nya sebagai fitur untuk pekerjaan analisis sentimen ini [5]. Selain itu, karena bobot yang telah dilatih sebelumnya, metode ini memungkinkan kita untuk menyempurnakan tugas kita pada kumpulan data yang jauh lebih kecil. Biasanya, model yang dibangun dari awal akan membutuhkan kumpulan data yang sangat besar untuk melatih jaringan kita ke akurasi yang baik, yang berarti banyak waktu dan komputasi harus dimasukkan ke dalam pembuatan kumpulan data. Dengan melakukan </span><span class="font3" style="font-style:italic;">tuning</span><span class="font3"> pada IndoBERT, penulis dapat melatih model untuk kinerja yang baik pada jumlah data pelatihan yang jauh lebih kecil [8]. Dalam tahap </span><span class="font3" style="font-style:italic;">fine-tuning</span><span class="font3">, model IndoBERT</span></p>
<p><span class="font3">pertama kali dimulai dengan parameter yang dipelajari sebelumnya, dan semua parameter disetel dengan menggunakan data berlabel.</span></p>
<p><span class="font3">Selama </span><span class="font3" style="font-style:italic;">fine-tuning</span><span class="font3">, semua parameter akan disetel. BERT mengambil urutan kata sebagai input yang terus mengalir ke atas BERT stack. Setiap lapisan menerapkan </span><span class="font3" style="font-style:italic;">self-attention</span><span class="font3">, dan meneruskan hasilnya melalui jaringan </span><span class="font3" style="font-style:italic;">feed-forward</span><span class="font3">, dan kemudian menyerahkannya ke </span><span class="font3" style="font-style:italic;">encoder</span><span class="font3"> berikutnya. </span><span class="font3" style="font-style:italic;">Di fine-tuning</span><span class="font3"> ini pada tiap </span><span class="font3" style="font-style:italic;">sequence</span><span class="font3"> akan ditambahkan token-token khusus. [CLS] adalah simbol khusus yang ditambahkan di depan setiap contoh input. Status tersembunyi terakhir yang sesuai dengan token ini digunakan sebagai representasi urutan agregat untuk tugas klasifikasi. Kemudian, [SEP] adalah token yang digunakan untuk memisahkan pasangan kalimat. Jika di satu </span><span class="font3" style="font-style:italic;">sequence</span><span class="font3"> terdapat beberapa pasang kalimat maka akan digunakan token ini untuk memisahkan tiap kalimat tersebut [9].</span></p>
<p><span class="font3" style="font-weight:bold;">Gambar 2. </span><span class="font3">BERT Model Input</span></p><img src="https://jurnal.harianregional.com/media/92657-3.jpg" alt="" style="width:205pt;height:118pt;">
<p><span class="font3">Pada output BERT, model hanya fokus pada output hanya dari posisi pertama (yang diberikan token [CLS]). Hasil token [CLS] ini dimasukkan ke dalam lapisan output untuk kegunaan klasifikasi. Jika kita memiliki lebih banyak label yang ingin diklasifikasikan, cukup mengubah jaringan pengklasifikasi untuk memiliki lebih banyak neuron keluaran yang kemudian melewati </span><span class="font3" style="font-style:italic;">softmax</span><span class="font3"> untuk pemusatan training [9].</span></p>
<p><span class="font3" style="font-weight:bold;">Gambar 3. </span><span class="font3">BERT Model Output</span></p><img src="https://jurnal.harianregional.com/media/92657-4.jpg" alt="" style="width:204pt;height:108pt;">
<p><span class="font0" style="font-weight:bold;">[CLSl Help Prince Mayuko</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark16"></a><span class="font3" style="font-weight:bold;"><a name="bookmark17"></a>2.6. &nbsp;&nbsp;&nbsp;Optimizer</span></h3></li></ul>
<p><span class="font3" style="font-style:italic;">Adaptive moment estimation</span><span class="font3"> (adam) merupakan salah satu algoritma yang dapat menggantikan prosedur stochastic gradient descent klasik untuk memperbaharui </span><span class="font3" style="font-style:italic;">weight network</span><span class="font3"> berdasarkan data training secara iteratif. Cara kerja adam dapat digambarkan sebagai penggabungan sifat</span></p>
<p><span class="font3">terbaik dari dua ekstensi </span><span class="font3" style="font-style:italic;">stochastic gradient descent</span><span class="font3"> yaitu </span><span class="font3" style="font-style:italic;">adaptive gradient algorithm</span><span class="font3"> dan </span><span class="font3" style="font-style:italic;">root mean square propagation</span><span class="font3"> dengan penggabungan tersebut adam mampu memberikan pengoptimalan suatu algoritma yang mampu menangani </span><span class="font3" style="font-style:italic;">sparse gradients</span><span class="font3"> pada </span><span class="font3" style="font-style:italic;">noisy problem</span><span class="font3">. Dengan penggunaan teknik optimasi yang mampu menurunkan gradien, metode ini sangat efisien ketika bekerja pada data yang besar dan parameter yang besar. AdamW adalah metode optimasi stokastik yang memodifikasi implementasi tipikal </span><span class="font3" style="font-style:italic;">weight decay</span><span class="font3"> pada Adam (Adaptive Moment Estimation). Regularisasi L2 pada Adam biasanya diimplementasikan dengan modifikasi di bawah ini dimana wt adalah laju </span><span class="font3" style="font-style:italic;">weight decay</span><span class="font3"> pada waktu t [10].</span></p>
<p><span class="font3" style="font-style:italic;">g<sub>t</sub> = Vf(θ<sub>t</sub>) + ω<sub>t</sub>θ<sub>t</sub></span><span class="font3"> (1)</span></p>
<p><span class="font3">Sehingga AdamW menyesuaikan istilah </span><span class="font3" style="font-style:italic;">weight decay</span><span class="font3"> untuk muncul di pembaruan atau </span><span class="font3" style="font-style:italic;">update </span><span class="font3">gradient [10].</span></p>
<div><img src="https://jurnal.harianregional.com/media/92657-5.jpg" alt="" style="width:194pt;height:24pt;">
</div><br clear="all">
<p><span class="font3">(2)</span></p>
<p><span class="font3">Sedangkan, AdaFactor adalah sebuah metode optimasi stokastik berdasarkan adam yang mengurangi penggunaan memori sambil mempertahankan manfaat empiris dari adaptasi. Adafactor mengurangi penggunaan memori pertama-tama dengan mengusulkan untuk mengganti matriks gradien kuadrat yang dismoothing dengan aproximasi peringkat rendah [11].</span></p>
<div><img src="https://jurnal.harianregional.com/media/92657-6.jpg" alt="" style="width:74pt;height:17pt;">
</div><br clear="all">
<p><span class="font3">(3)</span></p>
<h2><a name="bookmark18"></a><span class="font10"><a name="bookmark19"></a>Λ<sub>t</sub>=β<sub>2</sub>Λ<sub>f</sub>-l + (l-β<sub>2</sub>)(G<sub>t</sub><sup>2</sup>)l<sub>ra</sub></span></h2>
<div>
<p><span class="font3">(4)</span></p>
</div><br clear="all">
<h2><a name="bookmark20"></a><span class="font10"><a name="bookmark21"></a>C<sub>f</sub> = P<sub>2</sub>C<sub>f</sub>-<sub>1</sub>+(l-β<sub>2</sub>)lξ(G<sup>2</sup>) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(5)</span></h2>
<div><img src="https://jurnal.harianregional.com/media/92657-7.jpg" alt="" style="width:138pt;height:17pt;">
</div><br clear="all">
<ul style="list-style:none;"><li>
<p><span class="font3">(6)</span></p></li></ul>
<p><span class="font3">Kedua, Adafactor menghilangkan momentum sepenuhnya. Dengan meningkatkan tingkat decay seiring waktu dan memotong pembaruan gradient sesuai akar mean-square [11].</span></p>
<p><span class="font7" style="font-style:italic;">RMS(U<sub>t</sub>) = RMS<sub>xex</sub>(u<sub>xt</sub>^ <sub>χ</sub> Mean<sub>xsx</sub>φ)</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">(7)</span></p></li></ul>
<p><span class="font3">Dan akhirnya, Adafactor mengalikan learning rate dengan skala parameter (ini disebut relative step size) [11].</span></p>
<p><span class="font7" style="font-style:italic;">a<sub>t</sub> = max (ε<sub>2</sub>RM $ (X <sub>t</sub>-<sub>i</sub>X) Pt</span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(8)</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3" style="font-weight:bold;">2.7. &nbsp;&nbsp;&nbsp;Penilaian dan Evaluasi</span></p></li></ul>
<p><span class="font3">Penilaian performa model adalah tahapan untuk menentukan akurasi dan kinerja dari model IndoBERT tersebut dalam melakukan analisis sentimen. Evaluasi dari kedua metode optimizer ini dilakukan dengan menghitung nilai </span><span class="font3" style="font-style:italic;">precision</span><span class="font3">, </span><span class="font3" style="font-style:italic;">recall</span><span class="font3">, f-1 </span><span class="font3" style="font-style:italic;">score</span><span class="font3">, dan akurasi.</span></p>
<p><span class="font3">_ &nbsp;&nbsp;&nbsp;&nbsp;. . <sup>rp</sup></span></p>
<div>
<p><span class="font3">(9)</span></p>
<p><span class="font3">(10)</span></p>
<p><span class="font3">(11)</span></p>
</div><br clear="all">
<p><span class="font7" style="font-style:italic;">Precision = -----</span></p>
<p><span class="font6" style="font-weight:bold;font-style:italic;">TP+PF</span></p>
<div>
<p><span class="font6" style="font-weight:bold;font-style:italic;">TP+TN</span><span class="font6" style="font-weight:bold;font-style:italic;text-decoration:underline;"> </span><span class="font6" style="font-weight:bold;font-style:italic;">TP+FN+TN+FP</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/92657-8.jpg" alt="" style="width:154pt;height:23pt;">
</div><br clear="all">
<p><span class="font7" style="font-style:italic;">Accurcay =</span></p><img src="https://jurnal.harianregional.com/media/92657-9.jpg" alt="" style="width:84pt;height:23pt;">
<p><span class="font3">(12)</span></p>
<p><span class="font3">Keterangan :</span></p>
<p><span class="font3">TP/True Positive = jumlah data relevan yang secara benar diklasifikasian sebagai kecocokan oleh model</span></p>
<p><span class="font3">TN/True Negative &nbsp;&nbsp;&nbsp;= jumlah data tidak relevan yang diklasifikasikan sebagai tidak cocok</span></p>
<p><span class="font3">dengan benar oleh model</span></p>
<p><span class="font3">FP/False Positive &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= jumlah data yang tidak relevan, namun diklasifikasikan sebagai</span></p>
<p><span class="font3">kecocokan oleh model</span></p>
<p><span class="font3">FN/False Negative = jumlah data relevan, namun tidak diklasifikasikan sebagai kecocokan data oleh sistem</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark22"></a><span class="font3" style="font-weight:bold;"><a name="bookmark23"></a>3. &nbsp;&nbsp;&nbsp;Hasil dan Pembahasan</span></h3></li></ul>
<p><span class="font3">Data yang akan diolah dalam penelitian ini berasal dari tripadvisor.com. Data berupa ulasan pengguna yang diposting pada beberapa tempat wisata di Bali. Data diambil dalam jangka waktu 6 tahun terakhir. Karakteristik data yang diperoleh berbahasa Indonesia dan termasuk ke dalam data tidak terstruktur. Proses pengambilan data dilakukan dengan metode </span><span class="font3" style="font-style:italic;">crawling</span><span class="font3">. </span><span class="font3" style="font-style:italic;">Crawling</span><span class="font3"> dilakukan dengan menggunakan kata kunci nama tempat wisata. Kemudian data teks dan rating diekstrak menggunakan beautifulsoup. Jumlah data yang berhasil didapatkan sebesar 908 data. Pada hasil data ini ditetapkan nilai 1-2 adalah negative, 3 adalah neutral, dan 4-5 adalah positive. Data tersebut merupakan data mentah yang selanjutnya diproses ke tahap selanjutnya agar data bersih, rapi dan siap untuk diolah serta dianalisis. Pembagian dataset kedalam 3 proporsi, mendapatkan hasil akurasi tertinggi pada proporsi 70:15:15 dengan hasil akurasi training sebesar 97% dan akurasi validation sebesar 82%. Hasil testing mendapatkan akurasi sebesar 95%. Selain dari hasil akurasi, peneliti menggunakan </span><span class="font3" style="font-style:italic;">confusion matrix</span><span class="font3"> untuk mengukur sejauh mana model IndoBERT melakukan prediksi. Para peneliti yang berupaya membuat program yang dapat menganalisis ulasan tentang tempat-tempat wisata berdasarkan sentimen diharapkan dapat menganggap temuan penelitian ini berguna.</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark24"></a><span class="font3" style="font-weight:bold;"><a name="bookmark25"></a>3.1. &nbsp;&nbsp;&nbsp;Analisis</span></h3></li></ul>
<p><span class="font3">Kami melakukan pelatihan pada model IndoBERT dengan </span><span class="font3" style="font-style:italic;">batch size</span><span class="font3"> 32, 3 epoch, dan </span><span class="font3" style="font-style:italic;">learning rate optimizer</span><span class="font3"> sebesar 0.00003. </span><span class="font3" style="font-style:italic;">Fine-tuning</span><span class="font3"> ini selesai dalam waktu 36 menit. Dapat dilihat bahwa model IndoBERT mampu mengklasifikasikan dengan benar label positif sebanyak 725 dan salah dalam memprediksi label sebanyak 25. Label neutral berhasil diprediksi dengan benar sebanyak 76 dan salah dalam memprediksi label sebanyak 5 data. Label negative berhasil diprediksi dengan benar sebanyak 58 dan salah dalam memprediksi label sebanyak 19. Dapat dilihat disini bahwa persentase label positive benar adalah 79,8%, negative benar adalah 6,38%, dan label lainnya adalah 13,96%.</span></p>
<div><img src="https://jurnal.harianregional.com/media/92657-10.jpg" alt="" style="width:194pt;height:133pt;">
</div><br clear="all">
<p><span class="font3" style="font-weight:bold;">Gambar 4. </span><span class="font3">Diagram Pembagian Hasil Klasifikasi</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark26"></a><span class="font3" style="font-weight:bold;"><a name="bookmark27"></a>3.2. &nbsp;&nbsp;&nbsp;Evaluasi Optimizer</span></h3></li></ul>
<p><span class="font3">Hasil evaluasi pengujian dari IndoBERT untuk analisis sentimen menggunakan AdamW </span><span class="font3" style="font-style:italic;">Optimizer</span><span class="font3"> terhadap data </span><span class="font3" style="font-style:italic;">testing</span><span class="font3"> dengan mendapatkan nilai </span><span class="font3" style="font-style:italic;">accuracy</span><span class="font3">, </span><span class="font3" style="font-style:italic;">precision</span><span class="font3">, </span><span class="font3" style="font-style:italic;">recall</span><span class="font3">, dan f-1 </span><span class="font3" style="font-style:italic;">score</span><span class="font3"> dapat dilihat sebagai berikut. Hasil dapat dilihat pada tabel dan </span><span class="font3" style="font-style:italic;">confusion matrix</span><span class="font3"> yang mencakup nilai sebenarnya dan nilai prediksi dari analisis sentiment berikut.</span></p>
<div><img src="https://jurnal.harianregional.com/media/92657-11.jpg" alt="" style="width:105pt;height:126pt;">
<p><span class="font3">neutral -</span></p>
<p><span class="font8">11</span></p>
<p><span class="font8">O</span></p>
</div><br clear="all">
<div>
<p><span class="font8">21</span></p>
</div><br clear="all">
<div>
<p><span class="font8">19</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/92657-12.jpg" alt="" style="width:63pt;height:138pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/92657-13.jpg" alt="" style="width:24pt;height:18pt;">
</div><br clear="all">
<div>
<p><span class="font8">4</span></p><img src="https://jurnal.harianregional.com/media/92657-14.jpg" alt="" style="width:84pt;height:25pt;">
<p><span class="font1" style="font-weight:bold;">Predicted sentiment</span></p>
</div><br clear="all">
<p><span class="font3" style="font-weight:bold;">Tabel 3. </span><span class="font3">Perbandingan Evaluasi Hasil AdamW</span></p>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">F-1</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.98</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.91</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.97</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Negative</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.92</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.98</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.97</span></p></td></tr>
<tr><td colspan="4" style="vertical-align:middle;">
<p><span class="font3">Akurasi : 0.97</span></p></td></tr>
</table>
<p><span class="font3" style="font-weight:bold;">Gambar 5. </span><span class="font3">Confusion Matrix Hasil Klasifikasi AdamW</span></p>
<p><span class="font3">Kemudian, dibawah ini merupakan hasil evaluasi pengujian dari IndoBERT untuk analisis sentimen menggunakan AdaFactor </span><span class="font3" style="font-style:italic;">Optimizer</span><span class="font3"> terhadap data </span><span class="font3" style="font-style:italic;">testing</span><span class="font3"> dengan mendapatkan nilai </span><span class="font3" style="font-style:italic;">accuracy</span><span class="font3">, </span><span class="font3" style="font-style:italic;">precision</span><span class="font3">, </span><span class="font3" style="font-style:italic;">recall</span><span class="font3">, dan f-1 </span><span class="font3" style="font-style:italic;">score</span><span class="font3">. Hasil dapat dilihat pada tabel dan </span><span class="font3" style="font-style:italic;">confusion</span><span class="font3"> matrix berikut.</span></p>
<div>
<h3><a name="bookmark28"></a><span class="font3" style="font-weight:bold;"><a name="bookmark29"></a>Tabel 4.</span></h3>
<p><span class="font3">Evaluasi Hasil</span></p>
</div><br clear="all">
<div>
<table border="1">
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">F-1</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Positive</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.99</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.95</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.98</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Negative</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.95</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.99</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.98</span></p></td></tr>
<tr><td colspan="4" style="vertical-align:middle;">
<p><span class="font3">Akurasi : 0.98</span></p></td></tr>
</table>
</div><br clear="all">
<div>
<p><span class="font3">Perbandingan AdaFactor</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/92657-15.jpg" alt="" style="width:259pt;height:187pt;">
</div><br clear="all">
<p><span class="font3" style="font-weight:bold;">Gambar 6. </span><span class="font3">Confusion Matrix Hasil Klasifikasi AdaFactor</span></p>
<ul style="list-style:none;"><li>
<h3><a name="bookmark30"></a><span class="font3" style="font-weight:bold;"><a name="bookmark31"></a>4. &nbsp;&nbsp;&nbsp;Kesimpulan</span></h3></li></ul>
<p><span class="font3">Berdasarkan hasil evaluasi yang telah didapatkan sebelumnya, hasil penyetelan pada model BERT untuk analisis sentimen ulasan destinasi wisata di Bali menunjukkan bahwa model IndoBERT menghasilkan akurasi 95% dengan rasio dataset 70:15:15. dibandingkan dengan keseluruhan hasil prediksi. Akurasi yang dihasilkan recall menghasilkan 95% yang berarti bahwa IndoBERT dapat mengembalikan seluruh informasi mengenai nilai prediksi pada tingkatan baik. Serta, akurasi presisi yang didapat sebesar 93% yang berarti bahwa model ini baik dalam ketepatan memprediksi sentimen.</span></p>
<p><span class="font3">Berdasarkan keseluruhan hasil dari pemodelan IndoBERT, bisa disimpulkan bahwa IndoBERT mampu mengklasifikasikan dan memprediksi dengan sangat akurat karena semua akurasi diatas 90%. Dari penelitian ini, kedua optimisasi tersebut dapat memberikan hasil yang tepat dalam mengklasifikasikan ulasan objek wisata untuk mengkategorikan pendapat dalam ulasan tersebut termasuk kedalam sentimen postif atau negatif. Kemudian, hasil optimasi dengan</span></p>
<p><span class="font3">AdamW </span><span class="font3" style="font-style:italic;">optimizer</span><span class="font3"> mampu memberikan performa dengan nilai akurasi sebesar 97%. Sedangkan optimasi dengan AdaFactor </span><span class="font3" style="font-style:italic;">optimizer</span><span class="font3"> mampu memberikan performa yang terbaik dengan nilai akurasi sebesar 98,2%. Sehingga dari hasil nilai prediksi sentimennya dapat disimpulkan bahwa model dengan performa terbaik yaitu dengan penerapan AdaFactor </span><span class="font3" style="font-style:italic;">optimizer</span><span class="font3">. Hal ini dapat diakibatkan optimisasi memori dan </span><span class="font3" style="font-style:italic;">smoothing</span><span class="font3"> yang diterapkan AdaFactor.</span></p>
<p><span class="font3">Disarankan agar penelitian selanjutnya membandingkan hasil perolehannya dengan penelitian ini menggunakan teknik pra-pelatihan dan algoritma-algoritma </span><span class="font3" style="font-style:italic;">deep learning</span><span class="font3"> lainnya. Dapat disimpulkan bahwa dataset ulasan wisata Bali untuk tugas analisis sentimen dapat diperkaya lagi. Penelitian kedepannya dapat dengan melakukan eksplorasi terhadap performa dari modelmodel NLG seperti GPT, T5, dan BART dalam tugas serupa.</span></p>
<h3><a name="bookmark32"></a><span class="font3" style="font-weight:bold;"><a name="bookmark33"></a>Referensi</span></h3>
<ul style="list-style:none;"><li>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;B. A. Utami and A. Kafabih, “Sektor Pariwisata Indonesia di Tengah Pandemi Covid 19,”</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">JDEP (Jurnal Dinamika Ekonomi Pembangunan)</span><span class="font3">, vol. 4, no. 1, pp. 8–14, 2021, doi: </span><span class="font2">10.33005/jdep.v4i1.198</span><span class="font3">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;A. P. Yakup, “Pengaruh Sektor Pariwisata Terhadap Pertumbuhan Ekonomi Di</span></p></li></ul>
<p><span class="font3">Indonesia,” </span><span class="font3" style="font-style:italic;">Universitas Airlangga Surabaya</span><span class="font3">, &nbsp;&nbsp;&nbsp;2019, [Online]. Available:</span></p>
<p><a href="https://repository.unair.ac.id/86231/"><span class="font3">https://repository.unair.ac.id/86231/</span></a><span class="font3">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;Y. Soritua, “Analisis Peran Sektor Pariwisata Menjadi Pendapatan Utama Daerah (Studi</span></p></li></ul>
<p><span class="font3">Banding: Peran Sektor Pariwisata di Provinsi Bali),” </span><span class="font3" style="font-style:italic;">Jurnal Ilmu Manajemen dan Akuntansi</span><span class="font3">, vol. 3, no. 2, pp. 1–7, 2017, doi: 10.33366/ref.v3i2.506.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;R. Andrianto, “Dua Tahun Pandemi, Ekonomi Bali Ngenes Sekali,” 8 Mar. 2022. [Online].</span></p></li></ul>
<p><span class="font3">Available: </span><a href="https://www.cnbcindonesia.com/news/20220308120609-4-320904/dua-tahun-pandemi-ekonomi-bali-ngenes-sekali"><span class="font3">https://www.cnbcindonesia.com/news/20220308120609-4-320904/dua-tahun-pandemi-ekonomi-bali-ngenes-sekali</span></a><span class="font3">. [accessed 2 Oct. 2022]</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[5] &nbsp;&nbsp;&nbsp;S. Cahyawijaya </span><span class="font3" style="font-style:italic;">et al.</span><span class="font3">, “IndoNLG: Benchmark and Resources for Evaluating Indonesian</span></p></li></ul>
<p><span class="font3">Natural Language Generation,” in </span><span class="font3" style="font-style:italic;">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</span><span class="font3">, pp. 8875–8898, &nbsp;2021, doi:</span></p>
<p><span class="font3">10.18653/v1/2021.emnlp-main.699.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[6] &nbsp;&nbsp;&nbsp;A. Tabassum and R. R. Patil, “A Survey on Text Pre-Processing &amp;&nbsp;Feature Extraction</span></p></li></ul>
<p><span class="font3">Techniques in Natural Language Processing,” </span><span class="font3" style="font-style:italic;">International Research Journal of Engineering and Technology</span><span class="font3">, vol. 7, no. 6, pp. 4864–4867, 2020, [Online]. Available: </span><a href="http://www.irjet.net"><span class="font3">www.irjet.net</span></a><span class="font3">.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[7] &nbsp;&nbsp;&nbsp;A. Subakti, H. Murfi, and N. Hariadi, “The performance of BERT as data representation</span></p></li></ul>
<p><span class="font3">of text clustering,” </span><span class="font3" style="font-style:italic;">Journal Big Data</span><span class="font3">, vol. 9, no. 1, 2022, doi: 10.1186/s40537-022-00564-9.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[8] &nbsp;&nbsp;&nbsp;C. Sun, X. Qiu, Y. Xu, and X. Huang, “How to Fine-Tune BERT for Text Classification?,”</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Lecture Notes in Computer Science,</span><span class="font3"> vol. 11856, no. 2, pp. 194–206, 2019, doi:</span></p>
<p><span class="font3">10.1007/978-3-030-32381-3_16.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[9] &nbsp;&nbsp;&nbsp;J. Alammar, “The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer</span></p></li></ul>
<p><span class="font3">Learning),” 27 Jun. 2018. [Online]. Available: </span><a href="https://jalammar.github.io/illustrated-transformer/"><span class="font3">https://jalammar.github.io/illustrated-transformer/</span></a><span class="font3">. [accessed 5 Oct. 2022]</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[10] &nbsp;&nbsp;I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” in </span><span class="font3" style="font-style:italic;">7th International</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Conference on Learning Representations, ICLR 2019</span><span class="font3">, 2019.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[11] &nbsp;&nbsp;N. Shazeer and M. Stern, “Adafactor: Adaptive learning rates with sublinear memory</span></p></li></ul>
<p><span class="font3">cost,” in </span><span class="font3" style="font-style:italic;">35th International Conference on Machine Learning, ICML 2018</span><span class="font3">, vol. 10, pp.</span></p>
<p><span class="font3">7322–7330, 2018.</span></p>
<p><span class="font3" style="font-style:italic;">This page is intentionally left blank.</span></p>
<p><span class="font3">420</span></p>