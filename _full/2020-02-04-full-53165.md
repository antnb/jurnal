---
layout: full_article
title: "Optimization Artificial Neural Network Using Artificial Bee Colony in Letter Recognition Classification"
author: "I Gusti Ngurah Alit Indrawan, I Made Widiartha"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-53165 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-53165"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-53165"  
comments: true
---

<p><span class="font0">p-ISSN: 2301-5373</span></p>
<p><span class="font0">e-ISSN: 2654-5101</span></p>
<p><span class="font0">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font0">Volume 8, No 4. May 2020</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font1" style="font-weight:bold;"><a name="bookmark1"></a>Optimization Artificial Neural Network Using Artificial Bee Colony in Letter Recognition Classification</span></h1>
<p><span class="font0">I Gusti Ngurah Alit Indrawan<sup>a1</sup>, I Made Widiartha<sup>b2</sup></span></p>
<p><span class="font0"><sup>a,b</sup>Computer Science Major in Mathematics and Science Faculty of Udayana University Unud Street, Bukit Jimbaran, Badung, Bali. Postal Code: 80364. Indonesia</span></p>
<p><a href="mailto:1alitindrawan71@gmail.com"><span class="font0"><sup>1</sup>alitindrawan71@gmail.com</span></a></p>
<p><a href="mailto:2madewidiartha@unud.ac.id"><span class="font0"><sup>2</sup>madewidiartha@unud.ac.id</span></a></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font0" style="font-style:italic;">Artificial Neural Networks or commonly abbreviated as ANN is one branch of science from the field of artificial intelligence which is often used to solve various problems in fields that involve grouping and pattern recognition. This research aims to classify Letter Recognition datasets using Artificial Neural Networks which are weighted optimally using the Artificial Bee Colony algorithm. The best classification accuracy results from this study were 92.85% using a combination of 4 hidden layers with each hidden layer containing 10 neurons.</span></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Keywords </span><span class="font0" style="font-style:italic;">:Artificial Neural Network, Letter Recognition, Feedforward, Artificial Bee Colony</span></p>
<p><span class="font0" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font0" style="font-style:italic;">Image Processing, Expert System, Information</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font0" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font0">Artificial neural network or commonly abbreviated as ANN is one branch of science from the field of Artificial Intelligence. ANN is a tool for solving various problems, especially in areas that involve grouping and pattern recognition [1].</span></p>
<p><span class="font0">Many things can be classified using this ANN, one of which is the classification of letters. The introduction of letters needs to be done because it will be very useful in the current era of technological progress. Old documents that are still in paper format will certainly be damaged within a certain time so that documents in digital form are also needed. In addition, letter recognition can also be used to create letter recognition applications in children.</span></p>
<p><span class="font0">The data to be used in this study is a secondary dataset, the letter recognition dataset. This dataset consists of 20,000 data with 16 attributes with integer values between 0 to 15 and 1 class of letter classification results.</span></p>
<p><span class="font0">Peter W. Frey and David J. Slate wrote a study entitled &quot;Letter recognition using Holland-style adaptive classifiers&quot; which uses the same dataset as this study. In this study the classification was carried out using the Holland-style adaptive classifier algorithm by dividing 16,000 data as training data and 4,000 data as training data and obtained an accuracy of 80% [2].</span></p>
<p><span class="font0">Hussein Salim Qasim also conducted research using the same dataset as this research. The study, entitled Letter Recognition Data Using Neural Networks, successfully implemented Artificial Neural Networks to classify letters. Of the 20,000 data, only 1,000 data were used as samples and 1,000 of these data were divided into 3, namely 60% of training data, 20% of test data, and 20% of validation data. The accuracy of this study is 60% [3].</span></p>
<p><span class="font0">In this research, the classification of letters using Artificial Neural Networks will be carried out with optimization in weighting using the Artificial Bee Colony algorithm.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0" style="font-weight:bold;">2. &nbsp;&nbsp;Reseach Methods</span></p></li></ul>
<p><span class="font0" style="font-weight:bold;">Data Collection</span></p>
<p><span class="font0">The dataset used in this research is secondary data, namely the letter recognition dataset. This dataset was obtained from UCI Machine Learning Repository [4]. This dataset has 20,000 data and consists of 16 attributes and 1 class. Each attribute in this dataset has an integer value between 0 and 15. While the classes in this dataset are between 'A' to 'Z'. The following is a table of the letter recognition dataset attributes :</span></p>
<p><span class="font0">Tabel 1. Dataset attributes</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font0" style="font-weight:bold;">Attribute</span></p></td><td style="vertical-align:middle;">
<p><span class="font0" style="font-weight:bold;">Explanation</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">x-box</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Horizontal position of the box</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">y-box</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Vertical position of the box</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">width</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Square Width</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">height</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Box height</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">onpix</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0"># Of pixels</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">x-bar</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">x pixel on the box</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">y-bar</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">y pixel on the box</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">x2bar</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">x variance</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">y2bar</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">y variance</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">xybar</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Correlation of x with y</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">x2ybr</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">x * x * y</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">xy2br</span></p></td><td style="vertical-align:top;">
<p><span class="font0">x * y * y</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">x-ege</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Point from left to right</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">xegvy</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Correlation of x-ege with y</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font0">y-ege</span></p></td><td style="vertical-align:bottom;">
<p><span class="font0">Point from top to bottom</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">yegvx</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">Correlation of y-ege with x</span></p></td></tr>
</table>
<p><span class="font0">The class in the dataset is initially of the character data type which is then converted to an integer. The 'A' class will be changed to 1, 'B' to 2, and so on until 'Z' becomes 26.</span></p>
<h2><a name="bookmark4"></a><span class="font0" style="font-weight:bold;"><a name="bookmark5"></a>Dataset</span></h2>
<p><span class="font0">In this research, not all datasets are used because some data intersect with each other because some letters have similar shapes, making it difficult for the classification process later. Only 5 letter classes are used, namely the letters 'A', 'E', 'I', 'O', and 'U'. The five letters are used because they have a level of similarity far enough so that they will be classified properly later. In total there are 3,878 data used by the division of 2,714 training data and 1,164 test data.</span></p>
<h2><a name="bookmark6"></a><span class="font0" style="font-weight:bold;"><a name="bookmark7"></a>Neural Network</span></h2>
<p><span class="font0">Artificial Neural Network is one of the artificial representations of the human brain that always tries to simulate the learning process in the human brain. The term artificial here is used because this neural network is implemented using a computer program that is able to complete a number of calculation processes during the learning process [5].</span></p>
<p><span class="font0">In this research, the network training that will be made will be carried out by using 4 combinations of the number of different hidden layers between 1 hidden layer to 4 hidden layers with each neuron on the same hidden layer of 10 neurons.</span></p>
<h2><a name="bookmark8"></a><span class="font0" style="font-weight:bold;"><a name="bookmark9"></a>Artificial Bee Colony</span></h2>
<p><span class="font0">Bonabeau et al. define swarm intelligence as an experiment to design algorithms or to distribute problem solving tools inspired by the collective behavior of the social colonies of insects and other animal societies. The term swarm generally refers to a collection of controls from interacting agents and individuals [6]. One example of the swarm intelligence algorithm is the Artificial Bee Colony Algorithm.</span></p>
<p><span class="font0">Artificial Bee Colony (ABC) is an artificial intelligence algorithm that can be used for optimization in artificial neural network training [7]. The Artificial Bee Colony algorithm has a strong ability to find optimal global results or global best [8]. The ABC Algorithm model has 3 groups of bees, namely: employed bees, onlookers, and scouts in the artificial bee colony.</span></p>
<p><span class="font0">There are 3 main stages on basic ABC. The first stage, generate initial solutions from random food sources. To update possible solutions, each employed bee chooses a candidate for a new food source position, which is different from the previous position. The new position of the food source is calculated by the following equation: x_(ij= ) θ_ij+ </span><span class="font3">∅</span><span class="font0">(θ_ij- θ_kj) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)</span></p>
<p><span class="font0">Where:</span></p>
<p><span class="font0">xi : solutio from θi .</span></p>
<p><span class="font0">θi : employed bee posisition i</span></p>
<p><span class="font0">θk : Neighbor employed bee from θi .</span></p>
<p><span class="font0">Ø : Random number from [-1,1] where number from i ≠ k n : number employed bee</span></p>
<p><span class="font0">D : Dimension.</span></p>
<p><span class="font0">In the second stage, each onlooker bee chooses one food source obtained from the employed bee. The probability of a food source being chosen can be obtained from the equation below:</span></p>
<p><span class="font0">P_ij= (F(θ_i))/(∑_(k=1)^S▒</span><span class="font2">〖</span><span class="font0">F(θ_k)</span><span class="font2">〗</span><span class="font0">) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)</span></p>
<p><span class="font0">Dimana :</span></p>
<p><span class="font0">Pij : chance to choose employed bee-i</span></p>
<p><span class="font0">F(θ)I : Fitness Value from employed bee-i</span></p>
<p><span class="font0">S : number employed bee</span></p>
<p><span class="font0">Θi : position from employed bee</span></p>
<p><span class="font0">After choosing a food source, the onlooker bee goes to the selected food source and selects a new prospective food source. Then in the final stage, the limit is a limit that has been set in the ABC Algorithm cycle and controls the number of certain solutions that are not updated. Any food sources that do not increase beyond the limit will be abandoned and replaced with new positions and employed bees become scout bees. The new random position chosen by scout bee will be calculated via the equation below: θ_(ij= θ_(j min)+rand (θ_(j max)+ θ_(j min))) &nbsp;&nbsp;&nbsp;&nbsp;(3)</span></p>
<p><span class="font0">Where :</span></p>
<p><span class="font0">rand: a random number between [0,1]</span></p>
<p><span class="font0">θj max: upper limit of the position source in the dimension j</span></p>
<p><span class="font0">θi min: the lower boundary of the position source in the dimension j</span></p>
<p><span class="font0">Weight Optimization</span></p>
<p><span class="font0">The initial weight of each neuron in the hidden layer network will be optimized using the Artificial Bee Colony algorithm. At first each weight on the hidden layer will be generated randomly by a neural network, then later it will be optimized by the Artificial Bee Colony algorithm.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark10"></a><span class="font0" style="font-weight:bold;"><a name="bookmark11"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font0">After each of the 4 experiments performed with the number of hidden layers that differ with each hidden layer containing 10 neurons in the artificial condition network without optimization and a neural network with optimization, the following results are obtained.</span></p>
<p><span class="font0">Tabel 2. Result testing without optimization</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font0">Hidden Layer</span></p></td><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td><td style="vertical-align:top;">
<p><span class="font0">2</span></p></td><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">4</span></p></td></tr>
</table>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font0">Epocs</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">30</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">50</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">18</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">30</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">Time</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">00:03</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">00:10</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">00:12</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">00:19</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">39.63%</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">50.47%</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">82.11%</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">90.91%</span></p></td></tr>
</table>
<p><span class="font0">Tabel 3. Result testing with optimization</span></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font0">Hidden Layer</span></p></td><td style="vertical-align:top;">
<p><span class="font0">1</span></p></td><td style="vertical-align:top;">
<p><span class="font0">2</span></p></td><td style="vertical-align:top;">
<p><span class="font0">3</span></p></td><td style="vertical-align:top;">
<p><span class="font0">4</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">Epocs</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">23</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">28</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">12</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">20</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font0">Time</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">00:12</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">00:07</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">02:15</span></p></td><td style="vertical-align:middle;">
<p><span class="font0">01:19</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font0">Accuracy</span></p></td><td style="vertical-align:top;">
<p><span class="font0">42.99%</span></p></td><td style="vertical-align:top;">
<p><span class="font0">77.81%</span></p></td><td style="vertical-align:top;">
<p><span class="font0">89.59%</span></p></td><td style="vertical-align:top;">
<p><span class="font0">92.85%</span></p></td></tr>
</table>
<p><span class="font0">From table 2 and table 3, it can be seen the results of testing artificial neural networks without optimization and with optimization using artificial bee colony. From the test results it can be seen that the number of training iterations or epocs on the optimized neural network is less than the artificial network requirements that are not optimized at the same number of hidden layers. In addition, the classification accuracy of optimized neural networks is always higher than the accuracy of classification &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of &nbsp;&nbsp;&nbsp;&nbsp;artificial &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;neural &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;networks &nbsp;&nbsp;&nbsp;&nbsp;that &nbsp;&nbsp;&nbsp;&nbsp;are &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not &nbsp;&nbsp;&nbsp;&nbsp;optimized.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font0" style="font-weight:bold;"><a name="bookmark13"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font0">The conclusions obtained from this study are as follows:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;Artificial Terms Network can be used to classify letter recognition datasets well depending on the number of hidden layer combinations and neurons used.</span></p></li>
<li>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;The Artificial Bee Colony algorithm is proven to be capable of optimizing the weighting of artificial neural networks thereby reducing the number of training iterations and also improving classification accuracy.</span></p></li>
<li>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;The best classification accuracy results obtained by 92.85% using a combination of 4 hidden layers with each hidden layer containing 10 neurons</span></p></li></ul>
<p><span class="font0">The suggestions from the authors are:</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;Using all datasets available in the letter recognition dataset, but it is necessary to pre-process the data first, thereby reducing noise in the data so that the data can be used optimally in future studies.</span></p></li>
<li>
<p><span class="font0">• &nbsp;&nbsp;&nbsp;Can use other Swarm Intelligence Optimazion for optimization on the artificial neural network, such as Ant Colony Optimazion, Particle Swarm Optimazation, and others.</span></p></li></ul>
<h2><a name="bookmark14"></a><span class="font0" style="font-weight:bold;"><a name="bookmark15"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font0">[1] &nbsp;&nbsp;&nbsp;&nbsp;D. Puspitaningrum, &quot;Pengantar Jaringan Syaraf Tiruan,&quot; 2006.</span></p></li>
<li>
<p><span class="font0">[2] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P. W. Frey and D. J. Slate, &quot;Letter recognition using Holland-style adaptive classifiers,&quot;</span></p></li></ul>
<p><span class="font0" style="font-style:italic;">Machine learning,</span><span class="font0"> vol. 6, pp. 161-182, 1991.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[3] &nbsp;&nbsp;&nbsp;&nbsp;H. S. Qasim, &quot;Letter Recognition Data Using Neural Network,&quot; </span><span class="font0" style="font-style:italic;">International Journal of</span></p></li></ul>
<p><span class="font0" style="font-style:italic;">Scientific &amp;&nbsp;Engineering Research,</span><span class="font0"> vol. 4, 2013.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[4] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D. a. G. C. Dua. (2017). &nbsp;&nbsp;</span><span class="font0" style="font-style:italic;">[4] Machine Learning Repository</span><span class="font0">. Available:</span></p></li></ul>
<p><a href="http://archive.ics.uci.edu/ml"><span class="font0" style="text-decoration:underline;">http://archive.ics.uci.edu/ml</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[5] &nbsp;&nbsp;&nbsp;&nbsp;M. Andrijasa and M. Mistianingsih, &quot;Penerapan Jaringan Syaraf Tiruan Untuk Memprediksi</span></p></li></ul>
<p><span class="font0">Jumlah Pengangguran di Provinsi Kalimantan Timur Dengan Menggunakan Algoritma Pembelajaran Backpropagation,&quot; </span><span class="font0" style="font-style:italic;">Informatika Mulawarman: Jurnal Ilmiah Ilmu Komputer,</span><span class="font0"> vol. 5, pp. 50-54, 2016.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[6] &nbsp;&nbsp;&nbsp;&nbsp;J. A. Bullinaria and K. AlYahya, &quot;Artificial bee colony training of neural networks: comparison</span></p></li></ul>
<p><span class="font0">with back-propagation,&quot; </span><span class="font0" style="font-style:italic;">Memetic Computing,</span><span class="font0"> vol. 6, pp. 171-182, 2014.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[7] &nbsp;&nbsp;&nbsp;C. OZTURK AND D. KARABOGA, &quot;HYBRID ARTIFICIAL BEE COLONY ALGORITHM FOR</span></p></li></ul>
<p><span class="font0">NEURAL NETWORK TRAINING,&quot; IN </span><span class="font0" style="font-style:italic;">2011 IEEE CONGRESS OF EVOLUTIONARY COMPUTATION (CEC)</span><span class="font0">, 2011, PP. 84-88</span></p>
<ul style="list-style:none;"><li>
<p><span class="font0">[8] &nbsp;&nbsp;&nbsp;&nbsp;C. Ozturk and D. Karaboga, &quot;Hybrid artificial bee colony algorithm for neural network training,&quot;</span></p></li></ul>
<p><span class="font0">in </span><span class="font0" style="font-style:italic;">2011 IEEE congress of evolutionary computation (CEC)</span><span class="font0">, 2011, pp. 84-88.</span></p>
<p><span class="font0">473</span></p>