---
layout: full_article
title: "Text Classification System Using Text Mining with XGBoost Method"
author: "Ni Kadek Dwi Rusjayanthi, Anak Agung Kompiang Oka Sudana, I Nyoman Prayana Trisna"
categories: merpati
canonical_url: https://jurnal.harianregional.com/merpati/full-100799 
citation_abstract_html_url: "https://jurnal.harianregional.com/merpati/id-100799"
citation_pdf_url: "https://jurnal.harianregional.com/merpati/full-100799"  
comments: true
---

<p><span class="font1">JURNAL ILMIAH MERPATI VOL. 11, NO. 2 AUGUST 2023 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p-ISSN: 2252-3006</span></p>
<p><span class="font1">e-ISSN: 2685-2411</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font3" style="font-weight:bold;"><a name="bookmark1"></a>Text Classification System Using Text Mining with XGBoost Method</span></h1>
<p><span class="font1">Ni Kadek Dwi Rusjayanthi<sup>a1</sup>, Anak Agung Kompiang Oka Sudana<sup>a2</sup>, I Nyoman Prayana Trisna<sup>a3 a</sup>Department of Information Technology, Udayana University, Badung, Indonesia e-mail: </span><span class="font1" style="text-decoration:underline;"><sup>1</sup> </span><a href="mailto:dwi.rusjayanthi@unud.ac.id"><span class="font1" style="text-decoration:underline;">dwi.rusjayanthi@unud.ac.id</span></a><span class="font1" style="text-decoration:underline;">, </span><a href="mailto:2agungokas@unud.ac.id"><span class="font1" style="text-decoration:underline;"><sup>2</sup>agungokas@unud.ac.id</span></a><span class="font1">, </span><a href="mailto:3prayana.trisna@unud.ac.id"><span class="font1" style="text-decoration:underline;"><sup>3</sup>prayana.trisna@unud.ac.id</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstrak</span></p>
<p><span class="font1" style="font-style:italic;">Data berukuran besar di masa kini dapat dimanfaatkan untuk analisis, sehingga dapat diperoleh pengetahuan penting/bermanfaat pada berbagai domain. Analisis teks dapat dilakukan memanfaatkan text mining menggunakan metode komputasi sehingga ekstraksi pengetahuan dapat dilakukan pada data teks berukuran besar, termasuk pemrosesan terkait data teks yang tidak terstruktur, yang ditulis dalam bahasa alami. Klasifikasi pada text mining adalah salah satu tipe pekerjaan dengan proses pencarian sekumpulan model atau fungsi yang menggambarkan dan membedakan kelas data text dengan tujuan agar model tersebut dapat digunakan untuk memprediksi kelas dari suatu objek (data text) yang belum diketahui kelasnya. Text mining dilakukan pada penelitian ini untuk analisis data teks melalui Sistem Klasifikasi Teks menggunakan salah satu metode klasifikasi yaitu Metode XGBoost (eXtreme Gradient Boosting). Sistem klasifikasi teks dikembangkan untuk mengklasifikasikan teks berupa artikel. Akurasi tertinggi yang diperoleh dari pengujian yaitu sebesar 77%, dengan presisi sebesar 81% dan recall sebesar 77%.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Kata kunci: </span><span class="font1" style="font-style:italic;">text mining, data teks, klasifikasi, Metode XGBoost</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">Large data nowadays can be used for analysis; thus, it can obtain important/valuable knowledge in various domains. Text analysis can be carried out by utilizing text mining using computational methods so that knowledge extraction can be carried out on large text data, including processing related to unstructured text data, which is written in natural language. Classification in text mining is a type of work with the searching process for a set of models or functions that describe and differentiate text data classes with the aim that the model can be used to predict the class of an object (text data) whose class is unknown. Text mining was carried out in this research to analyze text data through the Text Classification System using a classification method, namely the XGBoost (eXtreme Gradient Boosting) Method. A text classification system was developed to classify text in the form of articles.</span><span class="font1"> The highest accuracy obtained from the test is 77%, with a precision of 81% and a recall of 77%.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">text mining, text data, classification, XGBoost Method</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font1">The availability of massive data nowadays can be used for analysis; thus, it can obtain important/useful knowledge in various domains. Text mining can be used for text analysis using computational methods so that knowledge extraction can be carried out on large text data, including processing related to unstructured text data written in natural language.</span></p>
<p><span class="font1">Text mining is a process of extracting information, where users interact with documents using analytical tools in the form of data mining components, including clustering components. Text mining adopts various techniques from other fields, such as data mining, information retrieval, machine learning, statistics and mathematics, linguistics, natural language processing (NLP), and visualization. Activities related to research for text mining include text extraction and storage, preprocessing, statistical data collection, indexing, and content analysis [1]. Text mining tasks include text categorization, text clustering, concept/entity extraction, sentiment</span></p>
<p><span class="font1">analysis, document summarization, and entity-relation modeling. Text mining is used to process unstructured data, in contrast to data mining which is used to process structured data [2].</span></p>
<p><span class="font1">Classification is the searching process or a set of models or functions that describe and differentiate data classes. The goal of classification is that the model can be used to predict the class of an object whose class is unknown [3]. Classification is often referred to as the supervised method because it uses previously labeled data as examples of correct data. Text classification is a classification process on textual data that has been carried out in several studies using various methods. Terms often used in text data include documents, words, phrases, corpus, and lexicon. Documents are sequences of words and punctuation marks that follow the grammatical rules of the language. Sentences, paragraphs, sections, chapters, books, web pages, emails, and more are some instances of the documents in this context. A term is usually a word but can also be a word or phrase pair. The corpus is a collection of documents, while the lexicon is a set of all the unique words in the corpus. Popular methods used for classification in the text include Nearest Neighbor (NN), Naïve Bayes (NB), Support Vector Machine (SVM), Decision Tree (DT), and Neural Networks method [4].</span></p>
<p><span class="font1">Text classification utilizing embedded representation of words and word sense has been studied and produced a stable classification process, especially in classifications with complex semantics [5]. Local features of phrases and global sentence semantics have been used for text classification using the AC-BiLSTM (attention-based bidirectional long short-term memory with convolution layer) method [6]. In addition, word embedding has also been used for text classification using various classification methods [7]. However, the method commonly used is TFIDF. TFIDF is a method that integrates term frequency and inverse document frequency. Term frequency calculated by the i-th term is the frequency of the t-th term appearing in the d-th document. Inverse Document Frequency helps reduce the influence of common words in the corpus [8].</span></p>
<p><span class="font1">Text mining in this research analyzed text data through a text classification system using classification techniques. The classification method used was the XGBoost (eXtreme Gradient Boosting) method, one of the tree-based machine learning methods that utilize treeboosting techniques [9]. This study employed XGBoost because it enabled resource optimization through cache access patterns, data compression, and sharding. The data used in this research were Indonesian language article data. TFIDF (Term Frequency Inverse Document Frequency) for feature extraction, ANOVA (Analysis of Variance) for feature selection, and PCA (Principal Component Analysis) for feature dimension reduction were other techniques applied in this work. The XGBoost method has been studied in several studies related to classification, including building a milk source classification model (dairy farming) [10], diabetes prediction [11], traffic accidents prediction [12], gourami supply estimation [13], and landslide hazard mapping [14]. The utilization of the XGBoost method for text mining has been carried out in several studies, including the hybrid model development for Ukrainian language sentiment analysis [15], integrated technology analysis of patent data [16], classification of injury rates based on accident narrative data [17], and classification of proactive personality in social media users [18].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Method / Proposed Method</span></h2></li></ul>
<p><span class="font1">System development was carried out in two main stages: the training and the testing stage. System overview can be seen in Figure 1.</span></p><img src="https://jurnal.harianregional.com/media/100799-1.jpg" alt="" style="width:281pt;height:28pt;"><img src="https://jurnal.harianregional.com/media/100799-2.jpg" alt="" style="width:291pt;height:91pt;">
<p><span class="font1">Figure 1 System overview</span></p>
<p><span class="font1">The training stage aimed to build a model, and the testing stage aimed to test system performance through a model formed using the classification method, namely XGBoost, during the training stage.</span></p>
<p><span class="font1">Articles classification based on Figure 2 began with document preprocessing. Preprocessing aimed to prepare text into data that could be processed at the next stage (training). The process carried out at the training stage includes (1) Case folding or changing all the letters in the article to lowercase; (2) Tokenization or the stage where a collection of characters in a text that has gone through a case folding process was broken down into units of words (tokens) with the first step, namely dividing the document into smaller parts, namely paragraphs and then sentences; (3) Filtering or the stage of taking important/related words by removing non-alphabetic characters and stopwords or meaningless words/related to determining the topic, as well as removing characters other than letters such as numbers, punctuation, whitespace or blank characters, while filtering in research was based on Sastrawi Algorithm and; (4) Stemming was the stage of mapping and decomposing the form of a word into its basic word form. This process also used Sastrawi Algorithm.</span></p>
<p><span class="font1">Furthermore, documents that have gone through preprocessing were subjected to TFIDF weighting or the term weighting process. Feature selection was used in feature extraction results to reduce feature size by selecting more relevant features. The feature selection method used was ANOVA (Analysis of Variance). Feature dimension reduction was applied to the feature from feature selection results to reduce feature dimensions. The method used for feature dimension reduction was PCA (Principal Component Analysis). The training was carried out using the XGBoost Method, which was applied to the TFIDF-weighted features of the training data document. The testing was carried out on test data (features) using the model produced during training as explanation of the stages of research that illustrates the logical sequence to get research output in line with expectations.</span></p>
<p><span class="font1">The text data used in this research were Indonesian articles obtained from the news site </span><a href="http://www.cnnindonesia.com"><span class="font1">www.cnnindonesia.com</span></a><span class="font1">. The data used consisted of five article topics: Economy, Sports, Entertainment, Lifestyle, and Technology. Each article topic consisted of 20 articles, so the total amount of data used was 100 article data.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font1" style="font-weight:bold;"><a name="bookmark7"></a>3. &nbsp;&nbsp;&nbsp;Literature Study</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark8"></a>3.1. &nbsp;&nbsp;&nbsp;Classification</span></h2></li></ul>
<p><span class="font1">Classification is a technique in Data Mining that is used to extract patterns/knowledge from text in Text Mining. The purpose/function of classification is knowledge extraction or model building to predict previously unknown class/category data [3]. The model is formed through the training phase, while the accuracy or performance of the model is obtained through the testing phase. Classification is included in the supervised learning category because the data used is labeled data or with the class column.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark9"></a><span class="font1" style="font-weight:bold;"><a name="bookmark10"></a>3.2. &nbsp;&nbsp;&nbsp;XGBoost (eXtreme Gradient Boosting) Method</span></h2></li></ul>
<p><span class="font1">XGBoost is a tree-based machine learning method which utilizes tree boosting techniques [9]. The utilization of cache access patterns, data compression, as well as sharding in XGBoost are the main components that can support more optimal use of resources. The tree boosting technique implemented in XGBoost is scalable, which is effective for preventing overfitting [19].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark11"></a><span class="font1" style="font-weight:bold;"><a name="bookmark12"></a>3.3. &nbsp;&nbsp;&nbsp;TF-IDF</span></h2></li></ul>
<p><span class="font1">TFIDF is a method/technique of word weighting using term frequency ( </span><span class="font9">tf </span><span class="font1">), and inverse document frequency ( </span><span class="font8" style="font-style:italic;">idf</span><span class="font1"> ). The term frequency is obtained by calculating the frequency of a term/word (</span><span class="font1" style="font-style:italic;">t</span><span class="font1">) contained in document (</span><span class="font1" style="font-style:italic;">d</span><span class="font1">), and the inverse document frequency ( </span><span class="font8" style="font-style:italic;">idf</span><span class="font1"> ) is obtained by the logarithmic ratio between the number of documents in the corpus (</span><span class="font1" style="font-style:italic;">N</span><span class="font1">) and the number of documents that have the term </span><span class="font1" style="font-style:italic;">t</span><span class="font1"> ( ). The inverse document frequency is useful for reducing the influence of common words on the corpus [8] , obtained by calculating using Equation (1), while the TFIDF weight is obtained using Equation (2).</span></p>
<p><a href="#bookmark13"><span class="font8">idf</span><span class="font6" style="font-style:italic;">t</span><span class="font1"> = </span><span class="font8">log</span><span class="font7">10 </span><span class="font8">Mdf</span><span class="font6" style="font-style:italic;">t</span><span class="font1">(1)</span></a></p>
<p><a href="#bookmark14"><span class="font9">w &nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font2">= </span><span class="font9">(1 </span><span class="font2">+ </span><span class="font9">logtf ,)</span><span class="font2">×</span><span class="font9">log </span><span class="font9" style="font-style:italic;">N/df</span><span class="font1">(2)</span></a></p>
<p><a href="#bookmark15"><span class="font9" style="font-style:italic;"><sub>t</sub></span><span class="font9"><sub>,</sub></span><span class="font9" style="font-style:italic;"><sub>d</sub> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;t</span><span class="font9">,</span><span class="font9" style="font-style:italic;">dt</span></a></p>
<p><span class="font1">The TFIDF weighting function is to obtain values that can be used to represent documents of the training data.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font1" style="font-weight:bold;"><a name="bookmark17"></a>3.4. &nbsp;&nbsp;&nbsp;Feature Selection and Dimentionality Reduction</span></h2></li></ul>
<p><span class="font1">High-dimensional datasets with large feature sizes can cause various obstacles in the learning process in machine learning. These obstacles include increasing the dimensions of the search space and data preparation for the learning process, as well as increasing computational complexity [20]. Feature selection is a process of selecting attributes that are considered relevant in the machine learning process, including text mining. Reducing feature size is useful for saving training time and reducing model complexity, and can even support model performance. One of the feature selection methods is Anova (Analysis of Variance) method which utilizes variations between the average features and data attributes from various classes/groups. Dimensionality reduction is used to reduce feature size (dimensions). The method used in this study is PCA (Principal Component Analysis) which is obtained based on the principal data components with the highest value variations.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font1" style="font-weight:bold;"><a name="bookmark19"></a>4. &nbsp;&nbsp;&nbsp;&nbsp;Result and Discussion</span><br><br><span class="font1" style="font-weight:bold;"><a name="bookmark20"></a>4.1. &nbsp;&nbsp;&nbsp;Preprocessing</span></h2></li></ul>
<p><span class="font1">Preprocessing was conducted for training and testing data. The preprocessing stages consisted of case folding, tokenization, filtering, and stemming. An example of one of the initial article data before the preprocessing stage is shown in Figure 2.</span></p><img src="https://jurnal.harianregional.com/media/100799-3.jpg" alt="" style="width:173pt;height:110pt;">
<p><span class="font1">Figure 2 Example of raw text data</span></p>
<p><span class="font1">Case folding was applied to change text data to lowercase. The resulting data from the case folding process is shown in Figure 3, where words that previously consisted of uppercase letters have changed to lowercase letters.</span></p>
<p><span class="font0" style="font-weight:bold;">P∙*goruh 9*1« a*u* pae⅛oι∙</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">M⅛M Uerwklwr </span><span class="font0" style="font-weight:bold;font-variant:small-caps;">#pM </span><span class="font0" style="font-weight:bold;">pol* aauh otoriter orβng to# dengan gape per</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">gaeuhan otarito* aha</span><span class="font4" style="font-weight:bold;">∣∣</span><span class="font0" style="font-weight:bold;"> Mengendalikan tolwuh ke∙&gt;4dupw</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">' enek. Mtlai dari «era bee⅜(kap<sub>l </sub>MOiantfl yang haru» dioakan. hingga parIhal MWMilih Jwwatn ι∙a* kuliah nanti orang Iwa dengan gaya owι</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">gew⅛&gt;ιβn ∙ep∙ril ini pun MogharwpMari anak ιalalu patuh, tidak holen MMyak terpa </span><span class="font0" style="font-weight:bold;font-variant:small-caps;">j </span><span class="font4" style="font-weight:bold;font-variant:small-caps;">∣</span><span class="font0" style="font-weight:bold;font-variant:small-caps;"> </span><span class="font4" style="font-weight:bold;font-variant:small-caps;">∣</span><span class="font0" style="font-weight:bold;font-variant:small-caps;"> ■</span></p>
<p><span class="font1">Figure 3 The results of the case folding process</span></p>
<p><span class="font1">The results of the tokenization process are shown in Figure 4, where the formed data has been separated into units of words/tokens. The data, which originally consisted of paragraphs, was formed into smaller parts, namely sentences. From sentences, the data was formed into even smaller parts in the form of tokens/words.</span></p>
<p><span class="font0" style="font-weight:bold;">pengeruk, poU, a»uh, pe∙⅛M</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">tukβ∏, Uarekter<sub>l </sub>atM⅛g .<sub>4</sub> pnle<sub>l</sub> a</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">Mh<sub>i</sub> MwrHer<sub>1</sub> arang, kue, i1angar</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;"><sub>4</sub> gaya, paMga&lt;uhaι</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">, Otnriter, adon, WengeRdalikan, ∙olur</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">ιh<sub>l</sub> kaMdu</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">um, anat, ., auUl. ilerl, cara. Iiareihag. ,, Mkenan<sub>4</sub> yang her*», dicakar, ,, Iiinggg<sub>v</sub> perihal, Meoilih<sub>l </sub>Jurwion<sub>4</sub> taat, kuliah, rantI<sub>4</sub> .<sub>4</sub> arang, tua, dengari, gaya, peoge«iih«r, ««pert l</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;"> i</span><span class="font4" style="font-weight:bold;">∣</span><span class="font0" style="font-weight:bold;">&gt;l, pun, werg^erapken, aROk,,,,</span></p>
<p><span class="font1">Figure 4 The results of the tokenization process</span></p>
<p><span class="font1">The resulting data from the filtering process is shown in Figure 5. There was an omission of non-alphabetical characters, such as numbers and punctuation marks, so the data only consisted of the letters a to z.</span></p>
<p><span class="font0" style="font-weight:bold;">pengaruh pole asuh pembentukan karakter enek pala «tuh Dtorltar arang tua dengan gaya Pengoavhaii otoriter akan Mngendallhan seluruh kehidupan enek wiei dari cara bersikap Mhanen yang Kerua Jiiewkwr Iiingge perihal CMiilih JuruEan saat kuliah nanti Drang tua dengan gaya Prngatuhen Mprrti Ini pw Mngharwpkwri anak Weleiu patuh IlUek Iuilah henyak...</span></p>
<p><span class="font1">Figure 5 The omission of non-alphabetical characters</span></p>
<p><span class="font1">As shown in Figure 6, the omission of meaningless words was also conducted through a filtering process. The omitted words included the words dengan, seluruh, mulai, dari, yang, akan, nanti, dan seperti.</span></p>
<p><span class="font0" style="font-weight:bold;">pengaruh pola acuh pecb∙ot-uken karakter anak pola asuh otoriter ιxarg tua gaya pengasuhan skaritnr Mrigwndal ikan kehidupan era⅛ b*r*ckφ ■akanan d 1 MMan perihal BMillh </span><span class="font0" style="font-weight:bold;font-variant:small-caps;">Jutomm</span><span class="font0" style="font-weight:bold;"> kuliah orang tua &lt;»» pwngckuhe* Mengharapkan onak patuh Menenteng peraturan ditetapkan, orang tua</span></p>
<p><span class="font0" style="font-weight:bold;">Pvt<sup>a</sup>MaaaLahan anak orang tua ManyeLeaaikannva kekuasaan perintah gaya pengasuhan otoriter</span></p><img src="https://jurnal.harianregional.com/media/100799-4.jpg" alt="" style="width:140pt;height:10pt;">
<p><span class="font1">Figure 6 the omission of meaningless words (stopword)</span></p>
<div>
<p><span class="font1">Figure 7 shows the results of the stemming process, where the data changes to form basic words. Words that experience changes in examples included pembentukan to bentuk, pengasuhan to asuh, mengendalikan to kendali, kehidupan to hidup, and bersikap to sikap.</span></p><img src="https://jurnal.harianregional.com/media/100799-5.jpg" alt="" style="width:173pt;height:108pt;">
</div><br clear="all">
<p><span class="font1">Figure 7 The results of the stemming process</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark21"></a><span class="font1" style="font-weight:bold;"><a name="bookmark22"></a>4.2. &nbsp;&nbsp;&nbsp;&nbsp;Feature Extraction</span></h2></li></ul>
<p><span class="font1">Feature extraction in this research was carried out using the TFIDF method. Feature extraction aimed to obtain features from the data. Based on word frequency in the data as well as word frequency across the entire dataset, the TFIDF approach generated data features. The distribution of words across the data (common words) was represented using IDF. The frequency of words with less common word influence was represented by TFIDF.</span></p><img src="https://jurnal.harianregional.com/media/100799-6.jpg" alt="" style="width:422pt;height:150pt;">
<div>
<p><span class="font1">The resulting features for each data were 2048 features/attributes. Ten words with the highest IDF scores can be seen in Figure 8. The highest TFIDF scores for ten features/words on each topic are shown in Figures 9 to 13.</span></p><img src="https://jurnal.harianregional.com/media/100799-7.jpg" alt="" style="width:413pt;height:130pt;">
<p><span class="font1">Figure 9 The ten highest weighted features for Entertainment Topic</span></p>
</div><br clear="all">
<div>
<p><span class="font0" style="font-weight:bold;">&lt;⅛&gt;∙ Hd&lt;ρ FtMtre Iwprrutxei</span></p><img src="https://jurnal.harianregional.com/media/100799-8.jpg" alt="" style="width:411pt;height:120pt;">
</div><br clear="all">
<div>
<p><span class="font1">Figure 10 The ten highest weighted features for Lifestyle Topic</span></p><img src="https://jurnal.harianregional.com/media/100799-9.jpg" alt="" style="width:418pt;height:143pt;">
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/100799-10.jpg" alt="" style="width:419pt;height:131pt;">
<p><span class="font1">Figure 1 The ten highest weighted features for the Technology Topic</span></p>
</div><br clear="all">
<div>
<p><span class="font5" style="font-weight:bold;">C4*∙ΓΛQ∙ FtMaft W⅜M&lt;taMt4</span></p><img src="https://jurnal.harianregional.com/media/100799-11.jpg" alt="" style="width:410pt;height:116pt;">
<p><span class="font1">Figure 2 The ten highest weighted features for the Sport Topic</span></p>
</div><br clear="all">
<div>
</div><br clear="all">
<p><span class="font1">Figure 9 shows the ten highest weighted features for Entertainment Topic. Figure 10 shows the ten highest weighted features for Lifestyle Topic. Figure 11 shows the ten highest weighted features for Economic Topic. Figure 12 shows the ten highest weighted features for Technology Topic. Figure 13 shows the ten highest weighted features for the Sport Topic.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark23"></a><span class="font1" style="font-weight:bold;"><a name="bookmark24"></a>4.3. &nbsp;&nbsp;&nbsp;Feature Selection</span></h2></li></ul>
<p><span class="font1">Feature selection in this research aimed to select features based on their relevance to the topic. It was carried out using the ANOVA method, which utilized variations between the average feature/attribute data from various classes/groups. The ten features resulting from the feature selection process can be seen in Figure 14. The size of the data features after the feature selection process was 215.</span></p><img src="https://jurnal.harianregional.com/media/100799-12.jpg" alt="" style="width:364pt;height:114pt;">
<p><span class="font1">Figure 143 The ten highest weighted features from the feature selection process</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark25"></a><span class="font1" style="font-weight:bold;"><a name="bookmark26"></a>4.4. &nbsp;&nbsp;&nbsp;Feature Reduction</span></h2></li></ul>
<p><span class="font1">Dimensional reduction aimed to reduce the data feature dimension, which used the PCA method in this research. PCA was obtained based on the principal data components with the highest value variation. The change in feature size resulted from a dimension reduction to 3 features. Visualization of the feature distribution resulting from feature selection is shown in Figure 15. Entertainment and Sports Topic features have separate feature sections from other features in one direction. Meanwhile, Economic Topic features have a section separated from other features in two directions. Lifestyle Topic features with a slightly separate section from other features, while Technology Topic features whose data were still combined with other features.</span></p>
<div><img src="https://jurnal.harianregional.com/media/100799-13.jpg" alt="" style="width:221pt;height:215pt;">
<p><span class="font1">Figure 154 The feature distribution resulting from dimensional reduction</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark27"></a><span class="font1" style="font-weight:bold;"><a name="bookmark28"></a>4.5. &nbsp;&nbsp;The Model Testing Result</span></h2></li></ul>
<p><span class="font1">The test was carried out on a 30% ratio data test or 30 article data. Based on multiple experiments with various parameter combinations, the optimum model was found. The best</span></p>
<p><span class="font1">model for using the XGBoost method in this study shows the highest accuracy of 77%, with a precision of 81% and a recall of 77%. The confusion matrix of the test model obtained is shown in Figure 16. The misclassification for Economic and Sports Topics occurred for 1 article each, whereas 2 articles for Entertainment Topics and 3 articles on Technology Topics are misclassified. Economic Topics are incorrectly classified as Entertainment Topics, Sports Topics are incorrectly classified as Lifestyle Topics, and Entertainment Topics are incorrectly classified as Lifestyle Topics. Technology Topics that encountered misclassification of 2 articles were classified into Entertainment Topics and 1 article into Economic Topics.</span></p><img src="https://jurnal.harianregional.com/media/100799-14.jpg" alt="" style="width:222pt;height:180pt;">
<p><span class="font1">Figure 165 The Confusion matrix of test result</span></p>
<p><span class="font1">The ensuing model combination of parameters includes </span><span class="font1" style="font-style:italic;">max_depth</span><span class="font1">: 2; </span><span class="font1" style="font-style:italic;">min_child_weight</span><span class="font1">: 1; </span><span class="font1" style="font-style:italic;">gamma</span><span class="font1">: 0.0; </span><span class="font1" style="font-style:italic;">subsample</span><span class="font1">: 0.6; </span><span class="font1" style="font-style:italic;">colsample_bytree</span><span class="font1">: 0,1; </span><span class="font1" style="font-style:italic;">learning_level</span><span class="font1">: 0.2; and </span><span class="font1" style="font-style:italic;">n_estimator</span><span class="font1">: 100.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark29"></a><span class="font1" style="font-weight:bold;"><a name="bookmark30"></a>5. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font1">The conducted research aims to classify text data by employing the XGBoost classification method. Text mining was applied to classify text data through several stages, namely as follows. There was preprocessing, feature extraction using the TFIDF method, feature selection using the ANOVA method, feature dimension reduction using the PCA method, and classification using the XGBoost method. A text classification system was developed to classify text of articles. The highest accuracy obtained from the test is 77%, with a precision of 81% and a recall of 77%. Misclassification occurred on the Topic of Economics, Sports, and the most errors on the Topic of Technology.</span></p>
<h2><a name="bookmark31"></a><span class="font1" style="font-weight:bold;"><a name="bookmark32"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;Lestari, N. M. A., &amp;&nbsp;Sudarma, M. 2017. Perencanaan Search Engine E-commerce dengan Metode Latent Semantic Indexing Berbasis Multiplatform. </span><span class="font1" style="font-style:italic;">Lontar Komputer: Jurnal Ilmiah Teknologi Informasi</span><span class="font1">. </span><a href="https://doi.org/10.24843/lkjiti.2017.v08.i01.p04"><span class="font1">https://doi.org/10.24843/lkjiti.2017.v08.i01.p04</span></a></p></li>
<li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;Devi, A. S., Putra, I. K. G. D., &amp;&nbsp;Sukarsa, I. M. 2015. Implementasi Metode Clustering DBSCAN pada Proses Pengambilan Keputusan. </span><span class="font1" style="font-style:italic;">Lontar Komputer: Jurnal Ilmiah Teknologi Informasi</span><span class="font1">. </span><a href="https://doi.org/10.24843/lkjiti.2015.v06.i03.p05"><span class="font1">https://doi.org/10.24843/lkjiti.2015.v06.i03.p05</span></a></p></li>
<li>
<p><span class="font1">[3] Zaki, M. J. &amp;&nbsp;Meira W. 2014. DATA MINING Fundamental Concepts and Algorithms (2nd</span></p></li></ul>
<p><span class="font1">ed.) [Online]. Available: </span><a href="https://dataminingbook.info/"><span class="font1">https://dataminingbook.info/</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[4] Deng, X., Li, Y., Weng, J., &amp;&nbsp;Zhang, J. (2019). Feature selection for text classification: A</span></p></li></ul>
<p><span class="font1">review. </span><span class="font1" style="font-style:italic;">Multimedia Tools and Applications</span><span class="font1">, </span><span class="font1" style="font-style:italic;">78</span><span class="font1">(3). </span><a href="https://doi.org/10.1007/s11042-018-6083-5"><span class="font1" style="text-decoration:underline;">https://doi.org/10.1007/s11042-018-6083-5</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;Hartmann, J., Huppertz, J., Schamp, C., &amp;&nbsp;Heitmann, M. 2019. Comparing automated text classification methods. </span><span class="font1" style="font-style:italic;">International Journal of Research in Marketing</span><span class="font1">, </span><span class="font1" style="font-style:italic;">36</span><span class="font1">(1), 20–38.</span></p></li></ul>
<p><a href="https://doi.org/10.1016/j.ijresmar.2018.09.009"><span class="font1" style="text-decoration:underline;">https://doi.org/10.1016/j.ijresmar.2018.09.009</span></a></p>
<ul style="list-style:none;"><li>
<p><a href="#bookmark33"><span class="font1">[6] &nbsp;&nbsp;&nbsp;Liu, G., &amp;&nbsp;Guo, J. 2019. Bidirectional LSTM with attention mechanism and convolutional layer &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classification. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font1" style="font-style:italic;">Neurocomputing</span><span class="font1">,</span></a></p></li></ul>
<p><a href="https://doi.org/10.1016/j.neucom.2019.01.078"><span class="font1">https://doi.org/10.1016/j.neucom.2019.01.078</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[7] &nbsp;&nbsp;&nbsp;Stein, R. A., Jaques, P. A., &amp;&nbsp;Valiati, J. F. 2019. An analysis of hierarchical text</span></p></li></ul>
<p><a href="#bookmark34"><span class="font1">classification &nbsp;&nbsp;&nbsp;&nbsp;using &nbsp;&nbsp;&nbsp;&nbsp;word &nbsp;&nbsp;&nbsp;&nbsp;embeddings. &nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font1" style="font-style:italic;">Information &nbsp;&nbsp;&nbsp;&nbsp;Sciences</span><span class="font1">,</span></a></p>
<p><a href="https://doi.org/10.1016/j.ins.2018.09.001"><span class="font1" style="text-decoration:underline;">https://doi.org/10.1016/j.ins.2018.09.001</span></a></p>
<ul style="list-style:none;"><li>
<p><a href="#bookmark35"><span class="font1">[8] &nbsp;&nbsp;&nbsp;Kowsari, K., Meimandi, K. J., Heidarysafa, M., Mendu, S., Barnes, L., &amp;&nbsp;Brown, D. 2019. Text classification algorithms: A survey. </span><span class="font1" style="font-style:italic;">Information (Switzerland)</span><span class="font1">, &nbsp;</span><span class="font1" style="font-style:italic;">10</span><span class="font1">(4),1–68.</span></a></p></li></ul>
<p><a href="https://doi.org/10.3390/info10040150"><span class="font1" style="text-decoration:underline;">https://doi.org/10.3390/info10040150</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[9] &nbsp;&nbsp;&nbsp;Chen, T., &nbsp;&amp;&nbsp;Guestrin, C. (2016). </span><span class="font1" style="font-style:italic;">XGBoost: A Scalable Tree Boosting System</span><span class="font1">.</span></p></li></ul>
<p><a href="https://doi.org/10.1145/2939672.2939785"><span class="font1">https://doi.org/10.1145/2939672.2939785</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[10] &nbsp;&nbsp;&nbsp;Mu, F., Gu, Y., Zhang, J., &amp;&nbsp;Zhang, L. 2020. Milk source identification and milk quality estimation using an electronic nose and machine learning techniques. In </span><span class="font1" style="font-style:italic;">Sensors</span></p></li></ul>
<p><span class="font1" style="font-style:italic;">(Switzerland)</span><span class="font1"> (Vol. 20, Issue 15, pp. 1–14). MDPI AG. </span><a href="https://doi.org/10.3390/s20154238"><span class="font1" style="text-decoration:underline;">https://doi.org/10.3390/s20154238</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[11] &nbsp;&nbsp;&nbsp;Li, M., Fu, X., &amp;&nbsp;Li, D. 2020. Diabetes Prediction Based on XGBoost Algorithm. </span><span class="font1" style="font-style:italic;">IOP Conference &nbsp;&nbsp;&nbsp;&nbsp;Series: &nbsp;&nbsp;&nbsp;&nbsp;Materials &nbsp;&nbsp;&nbsp;&nbsp;Science &nbsp;&nbsp;&nbsp;&nbsp;and &nbsp;&nbsp;&nbsp;&nbsp;Engineering</span><span class="font1">, &nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font1" style="font-style:italic;">768</span><span class="font1">(7).</span></p></li></ul>
<p><a href="https://doi.org/10.1088/1757-899X/768/7/072093"><span class="font1" style="text-decoration:underline;">https://doi.org/10.1088/1757-899X/768/7/072093</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[12] &nbsp;&nbsp;&nbsp;Nyoman, N., Pinata, P., Sukarsa, M., Kadek, N., &amp;&nbsp;Rusjayanthi, D. 2020. Prediksi</span></p></li></ul>
<p><span class="font1">Kecelakaan Lalu Lintas di Bali dengan XGBoost pada Python. </span><span class="font1" style="font-style:italic;">JURNAL ILMIAH MERPATI</span><span class="font1">, </span><span class="font1" style="font-style:italic;">8</span><span class="font1">(3), 188–196. </span><a href="https://ojs.unud.ac.id/index.php/merpati/article/download/63592/37798/"><span class="font1">https://ojs.unud.ac.id/index.php/merpati/article/download/63592/37798/</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[13] &nbsp;&nbsp;&nbsp;Sukarsa, I. M., Pandika Pinata, N. N., Kadek Dwi Rusjayanthi, N., &amp;&nbsp;Wisswani, N. W. 2021. Estimation of Gourami Supplies Using Gradient Boosting Decision Tree Method of XGBoost. </span><span class="font1" style="font-style:italic;">TEM Journal</span><span class="font1">, </span><span class="font1" style="font-style:italic;">10</span><span class="font1">(1). </span><a href="https://doi.org/10.18421/TEM101-17"><span class="font1" style="text-decoration:underline;">https://doi.org/10.18421/TEM101-17</span></a></p></li>
<li>
<p><span class="font1">[14] &nbsp;&nbsp;&nbsp;Can, R., Kocaman, S., &amp;&nbsp;Gokceoglu, C. 2021. A comprehensive assessment of XGBoost algorithm for landslide susceptibility mapping in the upper basin of Ataturk dam, Turkey. </span><span class="font1" style="font-style:italic;">Applied Sciences (Switzerland)</span><span class="font1">, </span><span class="font1" style="font-style:italic;">11</span><span class="font1">(11). </span><a href="https://doi.org/10.3390/app11114993"><span class="font1" style="text-decoration:underline;">https://doi.org/10.3390/app11114993</span></a></p></li>
<li>
<p><span class="font1">[15] &nbsp;&nbsp;&nbsp;Shakhovska, K., Shakhovska, N., &amp;&nbsp;Veselý, P. 2020. The sentiment analysis model of services &nbsp;&nbsp;&nbsp;providers’ &nbsp;&nbsp;&nbsp;feedback. &nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font1" style="font-style:italic;">Electronics &nbsp;&nbsp;&nbsp;(Switzerland)</span><span class="font1">, &nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font1" style="font-style:italic;">9</span><span class="font1">(11), &nbsp;&nbsp;&nbsp;&nbsp;1–15.</span></p></li></ul>
<p><a href="https://doi.org/10.3390/electronics9111922"><span class="font1">https://doi.org/10.3390/electronics9111922</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[16] &nbsp;&nbsp;&nbsp;Jun, S. 2021. Technology integration and analysis using boosting and ensemble. </span><span class="font1" style="font-style:italic;">Journal of Open Innovation: &nbsp;&nbsp;&nbsp;Technology, &nbsp;&nbsp;&nbsp;Market, &nbsp;&nbsp;&nbsp;and &nbsp;&nbsp;&nbsp;Complexity</span><span class="font1">, &nbsp;&nbsp;&nbsp;</span><span class="font1" style="font-style:italic;">7</span><span class="font1">(1), &nbsp;&nbsp;&nbsp;1–15.</span></p></li></ul>
<p><a href="https://doi.org/10.3390/joitmc7010027"><span class="font1">https://doi.org/10.3390/joitmc7010027</span></a></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[17] &nbsp;&nbsp;&nbsp;Das, S., Datta, S., Zubaidi, H. A., &amp;&nbsp;Obaid, I. A. 2021. Applying interpretable machine learning to classify tree and utility pole related crash injury types. </span><span class="font1" style="font-style:italic;">IATSS Research</span><span class="font1">, </span><span class="font1" style="font-style:italic;">45</span><span class="font1">(3), 310–316. </span><a href="https://doi.org/10.1016/j.iatssr.2021.01.001"><span class="font1">https://doi.org/10.1016/j.iatssr.2021.01.001</span></a></p></li>
<li>
<p><span class="font1">[18] &nbsp;&nbsp;&nbsp;Wang, P., Yan, Y., Si, Y., Zhu, G., Zhan, X., Wang, J., &amp;&nbsp;Pan, R. 2020. Classification of Proactive Personality: Text Mining</span></p></li>
<li>
<p><span class="font1">[19] &nbsp;&nbsp;&nbsp;Thongsuwan, S., Jaiyen, S., Padcharoen, A., &amp;&nbsp;Agarwal, P. 2021. ConvXGB: A new deep learning model for classification problems based on CNN and XGBoost. </span><span class="font1" style="font-style:italic;">Nuclear Engineering and Technology</span><span class="font1">, </span><span class="font1" style="font-style:italic;">53</span><span class="font1">(2). </span><a href="https://doi.org/10.1016/j.net.2020.04.008"><span class="font1" style="text-decoration:underline;">https://doi.org/10.1016/j.net.2020.04.008</span></a></p></li>
<li>
<p><span class="font1">[20] &nbsp;&nbsp;&nbsp;Jia, W., Sun, M., Lian, J. et al. 2022. Feature dimensionality reduction: a review. </span><span class="font1" style="font-style:italic;">Complex &amp;&nbsp;Intelligent Systems</span><span class="font1">. 8, 2663–2693. </span><a href="https://doi.org/10.1007/s40747-021-00637-x"><span class="font1">https://doi.org/10.1007/s40747-021-00637-x</span></a></p></li></ul>
<p><span class="font1" style="font-style:italic;">Text Classification System Using Text Mining with XGBoost Method (Ni Kadek Dwi</span></p>
<p><span class="font1" style="font-style:italic;">Rusjayanthi)</span></p>
<p><span class="font1">70</span></p>