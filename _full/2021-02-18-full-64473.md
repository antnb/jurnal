---
layout: full_article
title: "Implementation of Feature Selection using Information Gain Algorithm and Discretization with NSL-KDD  Intrusion Detection System"
author: "Dharma Putra, I Gusti Agung Gede Arya Kadyanan"
categories: jik
canonical_url: https://jurnal.harianregional.com/jik/full-64473 
citation_abstract_html_url: "https://jurnal.harianregional.com/jik/id-64473"
citation_pdf_url: "https://jurnal.harianregional.com/jik/full-64473"  
comments: true
---

<p><span class="font1">p-ISSN: 2301-5373</span></p>
<p><span class="font1">e-ISSN: 2654-5101</span></p>
<p><span class="font1">Jurnal Elektronik Ilmu Komputer Udayana</span></p>
<p><span class="font1">Volume 9 No. 3, February 2021</span></p>
<p><span class="font4" style="font-weight:bold;">Implementation of Feature Selection using Information Gain Algorithm and Discretization with NSL-KDD Intrusion Detection System </span><span class="font0">x</span></p>
<p><span class="font1">I Gusti Bagus Dharma Putra<sup>a1</sup></span><span class="font1" style="font-weight:bold;">, </span><span class="font1">I Gusti Agung Gede Arya Kadyanan<sup>a2</sup></span></p>
<p><span class="font1"><sup>a</sup>Informatics Department, Faculty of Math and Science, Udayana University South Kuta, Badung, Bali, Indonesia </span><a href="mailto:1dharmatkjone@gmail.com"><span class="font1"><sup>1</sup>dharmatkjone@gmail.com</span></a></p>
<p><a href="mailto:2gungde@unud.ac.id"><span class="font1"><sup>2</sup>gungde@unud.ac.id</span></a></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font1" style="font-style:italic;">Feature selection is one of the research on data mining for datasets that have relatively many attributes. Eliminating some attributes that are irrelevant to the label class will be able to improve the performance of the classification algorithm. The Information Gain algorithm is one of the algorithms for searching for features that are irrelevant to the label class. This algorithm uses wrapper techniques to eliminate irrelevant attributes. This research aims to implement feature selection using the Information Gain algorithm against the NSL KDD intrusion detection dataset which has a large number of relative attributes. The dataset of the selected attribute will be performed by a classification algorithm so that an attribute reduction can improve the compute process and improve the accuracy of the algorithm model used.</span></p>
<p><span class="font1" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font1" style="font-style:italic;">Feature Selection, Information Gain, Intrusion Detection System (IDS), NSL-KDD, Discretization</span></p>
<ul style="list-style:none;"><li><a name="caption1"></a>
<h2><a name="bookmark0"></a><span class="font1" style="font-weight:bold;"><a name="bookmark1"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font1">Feature selection is one of the most important data mining techniques in preprocessing for the selection of relatively many features on the dataset[1]. It aims to reduce data thereby speeding up computing processes and producing accurate models of the algorithms used. Feature selection is usually used to select optimal features, reduce dimensions, improve algorithm accuracy, and remove irrelevant features[2].</span></p>
<p><span class="font1">Intrusion detection systems (IDS) have been introduced as security techniques for detecting various attacks. IDS can be identified by two techniques, namely abuse detection, and anomaly detection. Abuse detection techniques can detect known attacks by checking attack patterns, such as virus-detection by antivirus applications. However, they cannot detect unknown attacks and need to update their attack patterns whenever there is a new attack. On the other hand, anomaly detection identifies unusual patterns of activity that deviate from normal use as interference[3].</span></p>
<p><span class="font1">Intrusion Detection System (IDS) is one of the important research in the field of computer networking or computer security. Several studies have conducted an intrusion detection system with calcification-based data mining to detect attacks on computer networks by analyzing data packets in the network. In the process of doing machine learning must have good data (complete, true, consistent, and integrated). Before data mining is done, the data needs to be processed in advance to ensure its quality. Moreover, many features in the data may reduce classification performance. So it takes a feature selection technique to select the relevant features for the data[4].</span></p>
<p><span class="font1">The NSL-KDD dataset is a dataset used to benchmark various classification methods for intrusion detection. This dataset has quite a lot of features namely 41 features that are continuous and discrete with normal or anomaly labels (Dos, Probe, R2L, U2R). Feature selection is one of the most important processes for eliminating uns needed features in nsl-kdd datasets. Not all</span></p>
<p><span class="font1">attributes can have an effect in a label class, therefore eliminating non-essential attributes with label classes is critical to improving classification performance[5].</span></p>
<p><span class="font1">This study aims to select features that are important or relevant to the label class and reduce computing time. The selection of features of the dataset to be used is the Information Gain (IG) technique. To group data with continuous type can be done by the binning method with the number of bins that is 12.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font1" style="font-weight:bold;"><a name="bookmark3"></a>2. &nbsp;&nbsp;&nbsp;Research Methods</span></h2></li></ul>
<p><span class="font1">This research is an experimental study with the discrete of continuous numerical value variables using binning methods while using attribute selection. The dataset used in this study is NSL-KDD'99 obtained in </span><a href="http://nsl.cs.unb.ca/NSL-KDD/"><span class="font1">http://nsl.cs.unb.ca/NSL-KDD/</span></a><span class="font1"> which is grouped into 4 categories of attacks namely DoS, R2L, U2R, and Probe.</span></p><img src="https://jurnal.harianregional.com/media/64473-1.jpg" alt="" style="width:110pt;height:213pt;">
<p><span class="font1" style="font-weight:bold;">Figure 1. </span><span class="font1">Feature selection process scheme</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font1" style="font-weight:bold;"><a name="bookmark5"></a>2.1. &nbsp;&nbsp;&nbsp;NSL-KDD’99 Dataset</span></h2></li></ul>
<p><span class="font1">The data used in this study is the 1999 NSL-KDD Cup dataset. NSL-KDD is the solution to the problem in the 1999 KDD Cup dataset (KDD-99). Intrusion detection systems because there are not many alternative datasets available and publicly accessible [6]. This dataset consists of a normal class and 39 types of attacks. In this study, the types of attacks contained in the dataset were grouped into 4 categories namely DoS, R2L, U2R, and Probe. As found in Table 1. and the NSL-KDD dataset attribute type is contained in Table 2. below.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1" style="font-weight:bold;">Table 1. </span><span class="font1">IDS Attack Category</span></p></li></ul>
<p><span class="font1">Source: </span><a href="http://nsl.cs.unb.ca/NSL-KDD/"><span class="font1">http://nsl.cs.unb.ca/NSL-KDD/</span></a></p>
<table border="1">
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Normal (1825)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Dos (3631)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Probe (3631)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">R2L (2436)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">U2R (56)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font1">Normal</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Back (297)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">Satan</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">guess_passwd</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">buffer_overflow</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">(1825)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">Land (4) Pod (35) Smurf (528) Teardrop (9) apache2 (615) Udpstorm (2) Processtable (572) Mailbomb (245)</span></p>
<p><span class="font1">Neptune (1324)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">(614) Ipsweep (121) Nmap (58) Portsweep</span></p>
<p><span class="font1">(133) Mscan (869) Saint (257)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">(1031) ftp_write (2) imap (1) phf (2) multihop (13) warezmaster</span></p>
<p><span class="font1">(506) xlock (8) xsnoop (3) snmpguess</span></p>
<p><span class="font1">(282) snmpgetattack</span></p>
<p><span class="font1">(152) httptunnel</span></p>
<p><span class="font1">(110) sendmail (10) named (16)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">(16) loadmodule (2) rootkit (11) perl (2) sqlattack (2) xterm (10) ps (13)</span></p></td></tr>
</table>
<p><span class="font1" style="font-weight:bold;">Table 2. </span><span class="font1">Dataset Attribute Type Source: </span><a href="http://nsl.cs.unb.ca/NSL-KDD/"><span class="font1">http://nsl.cs.unb.ca/NSL-KDD/</span></a></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font1" style="font-weight:bold;">Nominal</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Biner</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">Numerik</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">protocol_type(2) service(3) flag(4)</span></p></td><td style="vertical-align:top;">
<p><span class="font1">land(7), logged_in(12), root_shell(14), su_attempted(15), is_host_login(21), is_guest_login(22)</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">duration(1), src_bytes(5), dst_bytes(6), wrong_fragment(8), urgent(9), hot(10), num_failed_logins(11), num_compromised(13), num_root(16), num_file_creations(17), num_shells(18), num_access_files(19),</span></p>
<p><span class="font1">num_outbound_cmds(20), count(23), srv_count(24), serror_rate(25), srv_serror_rate(26), rerror_rate(27), srv_rerror_rate(28),</span></p>
<p><span class="font1">same_srv_rate(29), diff_srv_rate(30), srv_diff_host_rate(31),</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font1" style="font-weight:bold;"><a name="bookmark7"></a>2.2. &nbsp;&nbsp;&nbsp;Normalization</span></h2></li></ul>
<p><span class="font1">In this preprocessing process, an attribute separation will be performed which is then performed normally. Normalization is a transformation process in which a numeric attribute is scaled in a smaller range such as -1.0 to 1.0, or 0.0 to 1.0. In this study the methods/techniques applied to data normalization are:</span></p>
<p><span class="font11" style="font-style:italic;">, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v-min<sub>a</sub> &nbsp;&nbsp;, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;. &nbsp;&nbsp;,</span></p>
<p><span class="font7" style="font-style:italic;">v</span><span class="font1"> = ------:—</span><span class="font7" style="font-style:italic;">(Jiewjmax<sub>n</sub>- newjmιn<sub>n</sub>) + πewjmιπ<sub>n</sub></span></p>
<p><span class="font11" style="font-style:italic;">max<sub>a</sub>-mi∏<sub>a</sub> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font5" style="font-style:italic;"><sup>a &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</sup></span><span class="font11" style="font-style:italic;"><sup>a</sup>'</span><span class="font6"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>α</sup></span></p>
<p><span class="font1">(1)</span></p>
<p><span class="font1">Description :</span></p>
<p><span class="font1">v : value in numeric data column</span></p>
<ul style="list-style:none;"><li>
<p><span class="font13" style="font-style:italic;">v</span><span class="font10" style="font-style:italic;">'</span><span class="font1" style="font-style:italic;">:</span><span class="font1"> value result on normalization calculation</span></p></li></ul>
<p><span class="font13" style="font-style:italic;">min<sub>a</sub></span><span class="font1"> : minimum value in numeric data column</span></p>
<p><span class="font13" style="font-style:italic;">max<sub>a</sub></span><span class="font1"> : maximum value in numeric data column</span></p>
<p><span class="font13" style="font-style:italic;">newjnax<sub>a</sub></span><span class="font1"> : new maximum value or range limit</span></p>
<p><span class="font13" style="font-style:italic;">newjnin<sub>a</sub></span><span class="font1" style="font-style:italic;">:</span><span class="font1"> new minimum value or range limit</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark8"></a><span class="font1" style="font-weight:bold;"><a name="bookmark9"></a>2.3. &nbsp;&nbsp;&nbsp;Discrete</span></h2></li></ul>
<p><span class="font1">In the process of discrete where attributes that have continuous values are then changed in discrete form. This process is done aimed at minimizing the condition of the appearance of small continuous values, as it can affect in the selection process of features. The Binning method used to discrete variables in this study. For the amount of binning in this study use a minimum of 3 and a maximum of 12.</span></p>
<p><span class="font1">w = (max - min) / (no of bins)</span></p>
<p><span class="font1">(2)</span></p>
<p><span class="font1">Description :</span></p>
<p><span class="font1">w : interval limit</span></p>
<p><span class="font1">max = maximum value in numeric data</span></p>
<p><span class="font1">min = minimum value in numeric data</span></p>
<p><span class="font1">bins = interval size</span></p>
<div>
<p><span class="font1" style="font-weight:bold;">2.4. Feature Selection</span></p>
<p><span class="font1">Feature selection is done to reduce less relevant features in the classification process. From the next processing result to the feature selection stage with Information Gain. Calculation of Information Gain. After calculating the 41 attributes with the gain value next select the attribute that has the gain value with the highest weight.</span></p><img src="https://jurnal.harianregional.com/media/64473-2.jpg" alt="" style="width:416pt;height:91pt;">
<p><span class="font1" style="font-weight:bold;">Figure 2. </span><span class="font1">Information gain feature selection process</span></p>
</div><br clear="all">
<div><a name="caption2"></a>
<h1><a name="bookmark10"></a><span class="font8" style="font-style:italic;"><a name="bookmark11"></a>Info </span><span class="font3" style="font-style:italic;">(</span><span class="font8" style="font-style:italic;">D</span><span class="font3" style="font-style:italic;">)= - </span><span class="font8" style="font-style:italic;">∑<sup>c</sup><sub>i</sub> P<sub>i</sub>Iog<sub>2</sub></span></h1>
<ul style="list-style:none;"><li>
<p><span class="font1">(3)</span></p></li></ul>
<p><span class="font1">Description :</span></p>
<p><span class="font1">c = Number of values in the target attribute (number of classification classes)</span></p>
<p><span class="font1">pi = Number of samples for class i</span></p>
</div><br clear="all">
<div>
<p><span class="font7" style="font-style:italic;">InfoA </span><span class="font2" style="font-style:italic;">(</span><span class="font7" style="font-style:italic;">D</span><span class="font2" style="font-style:italic;">)=</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">(4)</span></p></li></ul>
</div><br clear="all">
<div>
<p><span class="font12">∑</span></p>
</div><br clear="all">
<div>
<p><span class="font11" style="font-style:italic;">V</span></p>
<p><span class="font6">7 = 1</span></p>
</div><br clear="all">
<div>
<p><span class="font6"><sup>l</sup>Dj<sup>l</sup> </span><span class="font7" style="font-style:italic;">XInfo</span><span class="font2" style="font-style:italic;">(</span><span class="font7" style="font-style:italic;">Dj</span><span class="font2" style="font-style:italic;">)</span></p>
</div><br clear="all">
<div>
<p><span class="font1">Description : A = Attribute</span></p>
</div><br clear="all">
<p><span class="font1">|Dj|= Total number of data samples</span></p>
<p><span class="font1">|D| = Number of samples for j value</span></p>
<p><span class="font1">v = A possible value for attribute A</span></p>
<p><span class="font1">Then the information gain value used to measure the effectiveness of an attribute in data claiming can be calculated with the formula below:</span></p>
<p><span class="font7" style="font-style:italic;">Gain </span><span class="font2" style="font-style:italic;">(</span><span class="font7" style="font-style:italic;">A</span><span class="font2" style="font-style:italic;">)=</span><span class="font9" style="font-style:italic;">∣</span><span class="font7" style="font-style:italic;">Info </span><span class="font2" style="font-style:italic;">(</span><span class="font7" style="font-style:italic;">D</span><span class="font2" style="font-style:italic;">)-</span><span class="font7" style="font-style:italic;">InfoA</span><span class="font2" style="font-style:italic;">(</span><span class="font7" style="font-style:italic;">D</span><span class="font2" style="font-style:italic;">)I</span></p>
<p><span class="font1">(5)</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark12"></a><span class="font1" style="font-weight:bold;"><a name="bookmark13"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font1">The dataset in this study is an NSL-KDD'99 dataset that has gone through several processes before. The data used was 125,973 data consisting of 39 types of attacks. Of the 39 types of attacks grouped into categories of attacks. Here are the results of the feature workings obtained.</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">Number &nbsp;Number</span></p>
<p><span class="font1" style="font-weight:bold;">of &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of</span></p>
<p><span class="font1" style="font-weight:bold;">Classes &nbsp;features</span></p></td><td style="vertical-align:top;">
<p><span class="font1" style="font-weight:bold;">Feature number</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font1">2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;41</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">3,4,29,33,34,12,38,39,25,26,23,32,31,41,2,27,28,40, 36,24,30,35,37,8,1,19,10,22,15,17,14,18,16,11,13,5,7,6,9,21,20</span></p></td></tr>
</table>
<p><span class="font1" style="font-weight:bold;">4. &nbsp;&nbsp;&nbsp;Conclusion</span></p>
<p><span class="font1">From the results of the study can be concluded that information gain can be used to determine the effect of dataset attributes on classification. In the process of selection of dataset, features are carried out binary classification process ( normal and attack). The selection method of features proposed in this study can help in the process of improving performance development in the Classification Of Intrusion Detection System (IDS).</span></p>
<h2><a name="bookmark14"></a><span class="font1" style="font-weight:bold;"><a name="bookmark15"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font1">[1] &nbsp;&nbsp;&nbsp;&nbsp;D. A. Effendy, K. Kusrini, and S. Sudarmawan, “Classification of intrusion detection</span></p></li></ul>
<p><span class="font1">system (IDS) based on computer network,” </span><span class="font1" style="font-style:italic;">Proc. - 2017 2nd Int. Conf. Inf. Technol. Inf. Syst. Electr. Eng. ICITISEE 2017</span><span class="font1">, vol. 2018-Janua, pp. 90–94, 2018, doi: 10.1109/ICITISEE.2017.8285566.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[2] &nbsp;&nbsp;&nbsp;H. Malhotra and P. Sharma, “Intrusion Detection using Machine Learning and Feature</span></p></li></ul>
<p><span class="font1">Selection,” </span><span class="font1" style="font-style:italic;">Int. J. Comput. Netw. Inf. Secur.</span><span class="font1">, vol. 11, no. 4, pp. 43–52, 2019, doi: 10.5815/ijcnis.2019.04.06.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[3] &nbsp;&nbsp;&nbsp;J. Malviya, V. Patel, and A. Srivastava, “New Naïve Bayes Classifier for Improve</span></p></li></ul>
<p><span class="font1">Intrusion Detection System Accuracy,” pp. 7–11, 2017, [Online]. Available: </span><a href="http://www.irjeas.org"><span class="font1">www.irjeas.org</span></a><span class="font1">,.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[4] &nbsp;&nbsp;&nbsp;T. Ahmad and M. N. Aziz, “Data preprocessing and feature selection for machine</span></p></li></ul>
<p><span class="font1">learning intrusion detection systems,” </span><span class="font1" style="font-style:italic;">ICIC Express Lett.</span><span class="font1">, vol. 13, no. 2, pp. 93–101, 2019, doi: 10.24507/icicel.13.02.93.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[5] &nbsp;&nbsp;&nbsp;A. Prof and S. H. Hashem, “Denial of Service Intrusion Detection System ( IDS ) Based</span></p></li></ul>
<p><span class="font1">on Naïve Bayes Classifier using NSL KDD and KDD Cup 99 Datasets University of Technology - Computer Science Department Hafsa Adil University of Technology -Computer Science Department,” no. 40, 2017.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font1">[6] &nbsp;&nbsp;&nbsp;&nbsp;Hettich. The UCI KDD Archive. California: Department of Information and Computer</span></p></li></ul>
<p><span class="font1">Science. 1999</span></p>
<p><span class="font1">This page is intentionally left blank</span></p>
<p><span class="font1">364</span></p>