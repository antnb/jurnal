---
layout: full_article
title: "Helmet Monitoring System using Hough Circle and HOG based on KNN"
author: "Rachmad Jibril Al Kautsar, Fitri Utaminingrum, Agung Setia Budi"
categories: lontar
canonical_url: https://jurnal.harianregional.com/lontar/full-68130 
citation_abstract_html_url: "https://jurnal.harianregional.com/lontar/id-68130"
citation_pdf_url: "https://jurnal.harianregional.com/lontar/full-68130"  
comments: true
---

<p><span class="font3" style="font-weight:bold;">LONTAR KOMPUTER VOL. 12, NO. 1 APRIL 2021</span></p>
<p><span class="font3" style="font-weight:bold;">DOI : 10.24843/LKJITI.2021.v12.i01.p02</span></p>
<p><span class="font3" style="font-weight:bold;">Accredited Sinta 2 by RISTEKDIKTI Decree No. 30/E/KPT/2018</span></p>
<p><span class="font3" style="font-weight:bold;">p-ISSN 2088-1541</span></p>
<p><span class="font3" style="font-weight:bold;">e-ISSN 2541-5832</span></p><a name="caption1"></a>
<h1><a name="bookmark0"></a><span class="font4" style="font-weight:bold;"><a name="bookmark1"></a>Helmet Monitoring System using Hough Circle and HOG based on KNN</span></h1>
<p><span class="font3">Rachmad Jibril A<sup>a1</sup>, Fitri Utaminingrum<sup>a2</sup>, Agung Setia Budi<sup>a3</sup></span></p>
<p><span class="font3" style="font-weight:bold;"><sup>a</sup></span><span class="font3">Faculty of Computer Science, University of Brawijaya, Malang, Indonesia</span></p>
<p><span class="font3">Malang 65141, Fax +62 0341-565420</span></p>
<p><a href="mailto:1jibril.rachmad@gmail.com"><span class="font3" style="text-decoration:underline;"><sup>1</sup>jibril.rachmad@gmail.com</span></a></p>
<p><a href="mailto:2f3_ningrum@ub.ac.id"><span class="font3" style="text-decoration:underline;"><sup>2</sup>f3_ningrum@ub.ac.id</span><span class="font3"> </span></a><span class="font3">(</span><span class="font2">Corresponding author) </span><a href="mailto:3agungsetiabudi@ub.ac.id"><span class="font3" style="text-decoration:underline;"><sup>3</sup>agungsetiabudi@ub.ac.id</span></a></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Abstract</span></p>
<p><span class="font3" style="font-style:italic;">Indonesian citizens who use motorized vehicles are increasing every year. Every motorcyclist in Indonesia must wear a helmet when riding a motorcycle. Even though there are rules that require motorbike riders to wear helmets, there are still many motorists who disobey the rules. To overcome this, police officers have carried out various operations (such as traffic operation, warning, etc.). This is not effective because of the number of police officers available, and the probability of police officers make a mistake when detecting violations that might be caused due to fatigue. This study asks the system to detect motorcyclists who do not wear helmets through a surveillance camera. Referring to this reason, the Circular Hough Transform (CHT), Histogram of Oriented Gradient (HOG), and K-Nearest Neighbor (KNN) are used. Testing was done by using images taken from surveillance cameras divided into 200 training data and 40 testing data obtained an accuracy rate of 82.5%.</span></p>
<p><span class="font3" style="font-weight:bold;font-style:italic;">Keywords: </span><span class="font3" style="font-style:italic;">Machine learning, Helmet detection, Histogram of an oriented gradient, K-nearest neighbor, Circular hough transform</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark2"></a><span class="font3" style="font-weight:bold;"><a name="bookmark3"></a>1. &nbsp;&nbsp;&nbsp;Introduction</span></h2></li></ul>
<p><span class="font3">Motorized vehicles are one type of transportation used in many parts of the world, especially motorbikes. In Indonesia, the number of people has been using motorbikes was increasing. Based on Police Headquarters data in 2013, the number of motorbikes in Indonesia ware 84,732,652 units, a large number of motorbikes caused a high number of traffic accidents involving motorcycles. In 2013 there were 119,560 motorbikes involved in the accident. Referring to the number of an accident has been recorded the total fatalities reached 26,416 (National Police Headquarters)[1].</span></p>
<p><span class="font3">There are several factors that cause accidents, namely human factors, vehicle factors, and environmental factors[2]. These factors are related to each other, but human factors are the biggest cause of accidents. This is indicated by the records of the National Police Headquarters in 2010-2016, which showed 70% of the causes of accidents were human factors. Many human factors also resulted in the loss of lives. To overcome this, police officers have carried out various operations (such as traffic operation, warning, etc.). This is not effective because of the number of police officers available, and the probability of police officers make mistakes when detecting violations that might be caused due to fatigue</span><span class="font3" style="font-style:italic;">.</span></p>
<p><span class="font3">Over the past few years, many attempts have been made to analyze traffic, including vehicle detection and classification and helmet detection. Modern traffic systems usually use computer vision algorithms, such as background and foreground image detection for the segmentation of moving objects. The use of computer vision algorithms can be applied to the results of video captured by a surveillance camera that is installed on a crossroad or a large road.</span></p>
<p><span class="font3">Previous research about helmet detection has been done by many researchers. Many methods are used for helmet detection, either feature extraction, shape detection, and image classification. Dongmala and Klubsuwan[3]. Proposes to detect half and full helmets using Haar Like Feature and Circular Hough Transform. They use a haar-like feature to detect the helmet.</span></p><img src="https://jurnal.harianregional.com/media/68130-1.jpg" alt="" style="width:241pt;height:512pt;">
<p><span class="font3" style="font-weight:bold;">Figure 1. </span><span class="font3">Proposed Method</span></p>
<p><span class="font3">region that is face/nose/mouth/left eye/right eye, but it can not distinguish between half and full helmet. So, they use CHT to detect the half and full helmet. Wen et al. [4]. Proposed circle arc detection based on the Circular Hough Transform method. They applied it to detect helmets through surveillance cameras at ATMs. The disadvantage of this method is that they only use the geometry feature to verify whether there is a helmet on the image captured by the camera. Geometry features are not enough to detect helmets. The head can be detected as a helmet because it is similarly circular. Rubaiyat et. al.[5]. proposed helmet detection uses for construction safety. They use Discrete Cosine Transform + Histogram of Oriented Gradient (HOG) for human detection method and Color + CHT for the helmet detection method. In this study, helmet</span></p>
<p><span class="font3">detection was carried out after color filtration so that helmet detection could be more accurate. However, this method will provide a disadvantage when the detected helmet is a different color. Based on the research above, we can see that CHT is good for detecting circles. Therefore it can be used to detect helmets that are also circular. Meanwhile, HOG is used to get the feature value to classify the circle that comes from the helmet itself as the main target of detection with a human head without a helmet. The classification method we chose is K-Nearest Neighbor (KNN). KNN was chosen because it is a simple method, only by setting the value for k by analyzing the number of neighbors by looking for the closest distance value as the basis for the classification parameter. Also, KNN can be applied to a multiclass system, wherein my research is divided into two classes, namely the helmet-wearing rider class and the un-helmeted rider class.</span></p>
<p><span class="font3">Based on the above problems and previous literature studies, we propose automatic helmet detection using Circular Hough Transform (CHT) for shape detection, Histogram of Oriented Gradient (HOG) for feature extraction, and K-Nearest Neighbor (KNN) for image classifier. Data is obtained from taking frames on surveillance videos.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark4"></a><span class="font3" style="font-weight:bold;"><a name="bookmark5"></a>2. &nbsp;&nbsp;&nbsp;Research Method</span></h2></li></ul>
<p><span class="font3">In this research, the proposed method can be seen in Fig. 1. The first step is to get the image from a surveillance video. The second step is used to search for a circular object in the image using CHT. The third step is feature extraction using HOG. The fourth step is to classify the extracted feature using KNN. The last step is to get the accuracy, precision, and recall from the KNN classifier.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark6"></a><span class="font3" style="font-weight:bold;"><a name="bookmark7"></a>2.1. &nbsp;&nbsp;&nbsp;Input</span><br><br><span class="font3" style="font-weight:bold;"><a name="bookmark8"></a>2.1.1. &nbsp;&nbsp;&nbsp;Surveillance Video</span></h2></li></ul>
<p><span class="font3">The video used in this research is a surveillance video that we got from a surveillance camera placed in sideroad and crossroads with a resolution of 1920x1080.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark9"></a><span class="font3" style="font-weight:bold;"><a name="bookmark10"></a>2.1.2. &nbsp;&nbsp;&nbsp;Frames</span></h2></li></ul>
<p><span class="font3">We save each frame from the video. The video has 25 fps, so we get 25 images each second of the video. After that, we save the image and will be used in the next steps.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark11"></a><span class="font3" style="font-weight:bold;"><a name="bookmark12"></a>2.2. &nbsp;&nbsp;&nbsp;Detection Circular Object</span><br><br><span class="font3" style="font-weight:bold;"><a name="bookmark13"></a>2.2.1. &nbsp;&nbsp;&nbsp;Grayscale</span></h2></li></ul>
<p><span class="font3">A grayscale image or gray level image is one of the color spaces of an image. The gray level represents the number of quantization intervals in grayscale image processing. At this time, the most used method for storage is 8-bit storage. In an 8 bit grayscale image, there are 256 gray levels from 0 to 255. With 0 is black and 255 is white [6]. In this research, we used equation (1) to convert the RGB image to a grayscale image.</span></p>
<p><span class="font8" style="font-style:italic;">„ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;. &nbsp;&nbsp;&nbsp;R+G+B</span></p>
<p><span class="font14" style="font-style:italic;">Grayscale = —-— &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(1)</span></p>
<p><span class="font3" style="font-style:italic;">R</span><span class="font3"> is the intensity of the pixel in the red channel, </span><span class="font3" style="font-style:italic;">G</span><span class="font3"> is the intensity of the pixel in the green channel, and </span><span class="font3" style="font-style:italic;">B</span><span class="font3"> is the intensity of the pixel in the blue channel.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark14"></a><span class="font3" style="font-weight:bold;"><a name="bookmark15"></a>2.2.2. &nbsp;&nbsp;&nbsp;Circular Hough Transform</span></h2></li></ul>
<p><span class="font3">Circular Hough Transform (CHT) is a method to detect a circular object. Many research has been done using CHT, such as detecting a person from surveillance video[7] and cell detection for bee comb image[8]. CHT is based on the Hough transform.</span></p>
<p><span class="font3">To detect circle CHT is using a voting process that calculates the possibility of edge point that is lying on a circle. It uses the circle formula to set the parameter of three-dimensional space to collect votes and to search a circle within a fixed radius. The votes will be saved in an accumulator. The objective of the CHT is to find the center point from every edge point of a circle in the image through the iteration of the equation (2) and (3).</span></p>
<p><span class="font13" style="font-style:italic;">x = a + R ×</span><span class="font13"> cos (0) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(2)</span></p>
<p><span class="font13">y = ft + R × sin (0) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(3)</span></p>
<p><span class="font3">Denotes </span><span class="font13" style="font-style:italic;">a</span><span class="font3"> and </span><span class="font13" style="font-style:italic;">b</span><span class="font3"> is the center point, </span><span class="font13" style="font-style:italic;">R</span><span class="font3"> is the radius, and </span><span class="font13">0 </span><span class="font3">is the angle. After the iteration accumulator with the most votes is the true center point of a circle.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark16"></a><span class="font3" style="font-weight:bold;"><a name="bookmark17"></a>2.2.3. &nbsp;&nbsp;&nbsp;Save the Detected Object</span></h2></li></ul>
<p><span class="font3">Save all detected images. The stored image will later be used for training and testing data in the classification process.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark18"></a><span class="font3" style="font-weight:bold;"><a name="bookmark19"></a>2.3. &nbsp;&nbsp;&nbsp;Histogram of Oriented Gradient (HOG) Extraction</span></h2>
<div><img src="https://jurnal.harianregional.com/media/68130-2.jpg" alt="" style="width:43pt;height:50pt;">
<p><span class="font3">(a)</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68130-3.jpg" alt="" style="width:25pt;height:49pt;">
<p><span class="font3">(b)</span></p>
</div><br clear="all">
<div>
<p><span class="font3" style="font-weight:bold;">Figure 2. </span><span class="font3">Aspect Ratio in HOG; (a) Original Image 69x79 pixel; (b) Resize Image 64x128 pixel</span></p><img src="https://jurnal.harianregional.com/media/68130-4.jpg" alt="" style="width:153pt;height:142pt;">
<p><span class="font3">(a) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b)</span></p>
</div><br clear="all">
<div>
<p><span class="font3" style="font-weight:bold;">Figure 3. </span><span class="font3">Two Computation Unit in HOG; (a) Cells; (b) Block</span></p><img src="https://jurnal.harianregional.com/media/68130-5.jpg" alt="" style="width:270pt;height:119pt;">
</div><br clear="all"></li></ul>
<p><span class="font3" style="font-weight:bold;">Figure 4. </span><span class="font3">Calculation Process in Each Cell</span></p>
<p><span class="font3">Histogram of Oriented Gradients algorithm is a feature descriptor that is used to extract features from images. The algorithm is based on the distribution of the Gradient in the image. The final feature is a one-dimensional array of histograms from the extracted image. In the HOG, there are two computation units for feature extraction. It is cell and block. The cell size is 8x8 pixels, and the block size is 16x16 pixels. There are four cells in one block. Figure 3. shows the example of the two computation units for feature extraction. After the computation of the current block, it moves to the next block with an overlap of 1 cell[9].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark20"></a><span class="font3" style="font-weight:bold;"><a name="bookmark21"></a>2.3.1. &nbsp;&nbsp;&nbsp;Preprocessing</span></h2></li></ul>
<p><span class="font3">In the preprocessing step, the HOG input image needs to have a fixed aspect ratio, so we get the same amount of feature. In this case, we use a 1:2 ratio, for example, 32x64, 64x128, or 1000x2000, but we cannot use 103x150 because it is not a 1:2 ratio. In this research, we use an image size of 64x128 pixels. Each image is scaled, keeping its aspect ratio preserve.</span></p>
<p><span class="font3">Therefore before we calculate the gradients, we resize every image. The example of the resize image can be seen in Figure 2.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 1. </span><span class="font3">The Example of HOG Feature</span></p>
<table border="1">
<tr><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Feature</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">f1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">f2</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">f3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;…</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">f3778</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">f3779</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">f3780</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Data 1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.206719</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.013714</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.077928 &nbsp;&nbsp;&nbsp;…</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.054473</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.235448</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.130019</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Data 2</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.046905</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.033736</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.033864 &nbsp;&nbsp;&nbsp;…</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.019585</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.024376</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.112724</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Data 3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.47073</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.0784</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.006978 &nbsp;&nbsp;&nbsp;…</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.000861</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.003578</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.010856</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Data 4</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.093873</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.076953</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.025398 &nbsp;&nbsp;&nbsp;…</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.043547</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.014677</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.073801</span></p></td></tr>
</table>
<ul style="list-style:none;"><li>
<h2><a name="bookmark22"></a><span class="font3" style="font-weight:bold;"><a name="bookmark23"></a>2.3.2. &nbsp;&nbsp;&nbsp;Calculate Gradient</span></h2></li></ul>
<p><span class="font3">After we get the resized image, we calculate the Gradient. In this process, we calculate the gradient magnitude and direction from every pixel using equations (4) and (5).</span></p>
<p><span class="font13" style="font-style:italic;">g</span><span class="font13"> = </span><span class="font13" style="font-style:italic;">√g</span><span class="font7" style="font-style:italic;">X</span><span class="font13"> + </span><span class="font13" style="font-style:italic;">g</span><span class="font7" style="font-style:italic;">y</span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)</span></p>
<p><span class="font13" style="font-style:italic;">θ</span><span class="font13"> = </span><span class="font13" style="font-style:italic;">arctan —</span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5)</span></p>
<p><span class="font7" style="font-style:italic;">B</span><span class="font16" style="font-style:italic;">x</span></p>
<p><span class="font3">Denotes </span><span class="font13" style="font-style:italic;">g</span><span class="font3"> is a gradient magnitude, </span><span class="font13" style="font-style:italic;">θ</span><span class="font3"> is gradient direction and </span><span class="font13" style="font-style:italic;">g<sub>x</sub>,g<sub>y</sub></span><span class="font3"> is a gradient of the </span><span class="font13">x</span><span class="font3">-axes and </span><span class="font13">y</span><span class="font3">-axes. We can calculate the </span><span class="font13" style="font-style:italic;">g<sub>x</sub>,g<sub>y</sub></span><span class="font3"> by using Sobel Filtering.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark24"></a><span class="font3" style="font-weight:bold;"><a name="bookmark25"></a>2.3.3. &nbsp;&nbsp;&nbsp;Calculate HOG in each cell</span></h2></li></ul>
<p><span class="font3">in this step, we calculate the Histogram of Gradient in each cell (8x8 Pixels). The Histogram is a vector or an array of 9 bins corresponding to angles 0, 20, 40, …, 160 degrees. So we must put gradient direction and magnitude into a histogram of Gradient. The Gradient of direction is the bins or array, and the Gradient of magnitude is the value of the bins or array. The calculation process can be seen in Figure 4.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark26"></a><span class="font3" style="font-weight:bold;"><a name="bookmark27"></a>2.3.4. &nbsp;&nbsp;&nbsp;Normalization of each block</span></h2></li></ul>
<p><span class="font3">After we get the Histogram of Gradient in each cell, we normalize the Histogram from each block (16x16 pixels). A histogram normalization computation is done by combining all histograms that belong to one block. One block has four cells and has nine feature vectors, so in one block, we have 36 (4 cells x 9 bins) feature vectors. We normalize the block using equation (6).</span></p>
<p><span class="font13">x<sub>1</sub></span><span class="font11"><sup>n</sup> </span><span class="font13">= &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="font3">(6)</span></p>
<p><span class="font7" style="font-style:italic;">√X</span><span class="font16" style="font-style:italic;">2 </span><span class="font7" style="font-style:italic;"><sup>+</sup> x</span><span class="font15" style="font-style:italic;">∣</span><span class="font7" style="font-style:italic;">+-<sup>+</sup></span><span class="font11"><sup> χ</sup></span><span class="font6"><sub>3</sub><sup>2</sup><sub>6</sub></span></p>
<p><span class="font13" style="font-style:italic;">x?</span><span class="font3">is the Normalization of each block result, </span><span class="font13" style="font-style:italic;">x<sub>l</sub></span><span class="font3"> is the feature vector and </span><span class="font13" style="font-style:italic;">i</span><span class="font3"> is a number feature in a block from 1 to 36.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark28"></a><span class="font3" style="font-weight:bold;"><a name="bookmark29"></a>2.3.5. &nbsp;&nbsp;&nbsp;Calculate the Feature</span></h2></li></ul>
<p><span class="font3">The last step of HOG is to calculate the total feature vector from all blocks. In this research, we use an image of 64x128 pixels, so we have seven blocks in a horizontal position and 15 blocks in a vertical position. The total block we have is 105 (7x15) blocks. Each block has 36 feature vectors, so in total, we have 3780 (36x105) feature vectors. Table 1 shows an example of the HOG feature. Each data have 3780 feature vectors.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark30"></a><span class="font3" style="font-weight:bold;"><a name="bookmark31"></a>2.4. &nbsp;&nbsp;&nbsp;Classification Process Using K-Nearest Neighbor (KNN)</span></h2></li></ul>
<p><span class="font3">K-Nearest Neighbor algorithm [10,11] is a method for classifying objects based on the closest distance between the training data and testing data. This algorithm is a simple classifier and easy to apply an algorithm that works well with recognition issues [12]. The training process for KNN only consists of the store the features and labels from training data. The classification process only searched for distance and assigned the label from the k-nearest neighbor who has the most votes.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark32"></a><span class="font3" style="font-weight:bold;"><a name="bookmark33"></a>2.4.1. &nbsp;&nbsp;&nbsp;Set Train and Test Data</span></h2></li></ul>
<p><span class="font3">KNN uses a distance system to calculate classification results. Therefore it requires training data and testing data. The data is obtained from a video with a 1080p resolution and has a frame rate of 25 fps. After that, we crop the area around the head of the motorcyclist to distinguish which one belongs to the positive and negative class. Image with helmet becomes the positive class, and image without helmet becomes the negative class. The video was taken in daytime conditions, and the camera is placed on the side of a road or intersection with a height of 2-4 meters.</span></p>
<p><span class="font3">In this research, we used 200 data consisting of 100 helmet wearing data (positive class) and 100 non-helmet wearing data (negative class). The test data used 40 data consisting of 20 helmetwearing data (positive class) and 20 non-helmet-wearing data (negative class). The training data used 160 data consisting of 80 helmet wearing data (positive class) and 80 non-helmet wearing data (negative class).</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark34"></a><span class="font3" style="font-weight:bold;"><a name="bookmark35"></a>2.4.2. &nbsp;&nbsp;&nbsp;Set </span><span class="font3" style="font-weight:bold;font-style:italic;">k-value</span></h2></li></ul>
<p><span class="font3">In the KNN Classification, we need a </span><span class="font3" style="font-style:italic;">k-value</span><span class="font3">. The </span><span class="font3" style="font-style:italic;">k-value</span><span class="font3"> used to determine how many calculation results will be used for voting.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark36"></a><span class="font3" style="font-weight:bold;"><a name="bookmark37"></a>2.4.3. &nbsp;&nbsp;&nbsp;Calculate distance</span></h2></li></ul>
<p><span class="font3">In this research, we use Euclidean Distance to calculate the distance between neighbors in KNN. The equation for Euclidean distance is shown in equation (7).</span></p>
<p><span class="font13" style="font-style:italic;">dCΓr,Ty)= √tf<sub>l</sub></span><span class="font12" style="font-style:italic;"><sub>τr</sub></span><span class="font13"> - </span><span class="font13" style="font-style:italic;">f</span><span class="font7" style="font-style:italic;">ι</span><span class="font12" style="font-style:italic;"><sub>τy</sub>γ</span><span class="font13"> + </span><span class="font13" style="font-style:italic;">(f<sub>2</sub></span><span class="font12" style="font-style:italic;"><sub>τr</sub> </span><span class="font13" style="font-style:italic;">- f<sub>2</sub></span><span class="font12" style="font-style:italic;"><sub>τy</sub>r</span><span class="font13"> + </span><span class="font13" style="font-style:italic;">-</span><span class="font13"> + </span><span class="font13" style="font-style:italic;">(f<sub>3780</sub></span><span class="font12" style="font-style:italic;"><sub>τr</sub></span><span class="font13"> - </span><span class="font13" style="font-style:italic;">f<sub>3780</sub></span><span class="font12" style="font-style:italic;"><sub>τy</sub></span><span class="font13" style="font-style:italic;">)</span><span class="font7" style="font-style:italic;"><sup>2</sup></span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(7)</span></p>
<p><span class="font13" style="font-style:italic;">d(Tr,Ty)</span><span class="font3"> is the distance, </span><span class="font13" style="font-style:italic;">Tr</span><span class="font3"> is testing data, and </span><span class="font13" style="font-style:italic;">Ty</span><span class="font3"> is training data.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark38"></a><span class="font3" style="font-weight:bold;"><a name="bookmark39"></a>2.4.4. &nbsp;&nbsp;&nbsp;Sort the Distance</span></h2></li></ul>
<p><span class="font3">After we get the distance, we sort the distance from the smallest distance to the largest distance. Then we take some of the top data by following the </span><span class="font3" style="font-style:italic;">k-value</span><span class="font3"> that has been set. For example, if we set the </span><span class="font3" style="font-style:italic;">k-value</span><span class="font3"> to 5, then we take the five most top data.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark40"></a><span class="font3" style="font-weight:bold;"><a name="bookmark41"></a>2.4.5. &nbsp;&nbsp;&nbsp;Determine the Class</span></h2></li></ul>
<p><span class="font3">The result of classification is the class that has the most votes in the k-nearest neighbor. For example, we have </span><span class="font3" style="font-style:italic;">a k-value</span><span class="font3"> of 5. In the five smallest data, we have 3 data with the label of 0 and 2 data with the label of 1, so the classification result is class 0 because it has more votes than class 1 [13].</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark42"></a><span class="font3" style="font-weight:bold;"><a name="bookmark43"></a>3. &nbsp;&nbsp;&nbsp;Result and Discussion</span></h2></li></ul>
<p><span class="font3">In this section, the proposed method was tested by using a dataset that we collect from several frames from the surveillance video. The output of the system is the result of the KNN classification. The result is either the circular object is the helmet-wearing class or the non-helmet wearing class.</span></p>
<p><span class="font3" style="font-weight:bold;text-decoration:underline;">Table 2. </span><span class="font3" style="text-decoration:underline;">Sample Data of Visual Experimental Result</span></p>
<div>
<p><span class="font1">Actual </span><span class="font0">Detected</span></p>
</div><br clear="all">
<div>
<p><span class="font3">Input</span></p>
<p><span class="font3">Result</span></p><img src="https://jurnal.harianregional.com/media/68130-6.jpg" alt="" style="width:271pt;height:241pt;">
<p><span class="font3">1</span></p>
<p><span class="font3">2</span></p>
<p><span class="font3">3</span></p>
<p><span class="font3">2</span></p>
</div><br clear="all">
<div>
<p><span class="font3">1</span></p>
</div><br clear="all">
<div>
<p><span class="font3">2</span></p>
</div><br clear="all">
<div>
<p><span class="font3">2</span></p>
</div><br clear="all">
<div>
<p><span class="font3">1</span></p>
</div><br clear="all">
<div><img src="https://jurnal.harianregional.com/media/68130-7.jpg" alt="" style="width:173pt;height:116pt;">
<p><span class="font3">(b)</span></p>
<p><span class="font3" style="font-weight:bold;">Figure 5. </span><span class="font3">Example of the Dataset</span></p>
</div><br clear="all">
<ul style="list-style:none;"><li>
<h2><a name="bookmark44"></a><span class="font3" style="font-weight:bold;"><a name="bookmark45"></a>3.1. &nbsp;&nbsp;&nbsp;Visual Result</span></h2></li></ul>
<p><span class="font3">The proposed method has been implemented. First, the original image is transformed to Grayscale, and CHT will be implemented to it. The CHT's purpose is to detect the head region of the motorcyclist. In this research, we select the CHT result that shows the head region because our purpose is to detect the helmet.</span></p>
<p><span class="font3">The result of CHT will be cropped and then saved to build training and testing data. The data divided into 200 training data, which was obtained from selected frames consisting of 100 helmets wearing motorcyclist head image and 100 non-helmet wearing motorcyclist head image. The test data used 40 images 20 helmet wearing motorcyclist head image and 20 non-helmet wearing motorcyclist head image that was obtained from a different surveillance video with training data. The data divided into two classes, helmet-wearing class and non-helmet wearing class. Figure 5. shows the example of the dataset.</span></p>
<p><span class="font3">To classify the data, we need to extract the feature from the image. We use HOG for feature extraction. There are 3780 features from each image. After that, we do labeling for each data. We do the feature extraction into both classes. The result is the same, only different in value table 2. Shows the example of the visual experimental results, the detected helmet is marked with a green circle, and the other is not marked. Several conditions cause detection failure, the image of the</span></p>
<p><span class="font3">head is not intact or covered with something, and the image of the head is too small. </span><span class="font17">W</span><span class="font3">e conducted this experiment 40 times.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark46"></a><span class="font3" style="font-weight:bold;"><a name="bookmark47"></a>3.2. &nbsp;&nbsp;&nbsp;Quantitative Result</span></h2></li></ul>
<p><span class="font3">In order to present the performance of the proposed method, an experiment is conducted using the dataset. The measurement method uses the accuracy equation (8), precision equation (9), and recall equation (10). We use all the data in this experiment, which are 40 testing data and 160 training data. The testing data is divided by four, each has ten images, and we will calculate the average accuracy.</span></p>
<p><span class="font3" style="font-weight:bold;">Table 3. </span><span class="font3">Confucion Matrix</span></p>
<table border="1">
<tr><td colspan="2" style="vertical-align:top;">
<p><span class="font3" style="font-weight:bold;">Predictive Relevant</span></p></td><td colspan="3" style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">Irrelevant</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Actual</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Relevant</span></p>
<p><span class="font3" style="font-weight:bold;">Irrelevant</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">True Positive (TP) False Positive (FP)</span></p></td><td colspan="2" style="vertical-align:middle;">
<p><span class="font3">False Negative (FN)</span></p>
<p><span class="font3">True Negative (TN)</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">Table</span></p></td><td colspan="2" style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">4. </span><span class="font3">Testing Result</span></p></td><td style="vertical-align:top;"></td><td style="vertical-align:top;"></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Data</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;font-style:italic;">k-value</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Accuracy</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font3" style="font-weight:bold;">Recall</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font3" style="font-weight:bold;">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.70</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.66</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.70</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.66</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.70</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.66</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.60</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.66</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.40</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.60</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.66</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.40</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3" style="font-weight:bold;">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.60</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.66</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.40</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.70</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.75</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.60</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.90</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3" style="font-weight:bold;">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.90</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.90</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.90</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3" style="font-weight:bold;">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">1.00</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.825</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.865</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.75</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:bottom;">
<p><span class="font3">3</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.80</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.83</span></p></td><td style="vertical-align:bottom;">
<p><span class="font3">0.75</span></p></td></tr>
<tr><td colspan="2" style="vertical-align:top;">
<p><span class="font3" style="font-weight:bold;">Average</span></p>
<p><span class="font3">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.83</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.75</span></p></td></tr>
<tr><td style="vertical-align:top;"></td><td style="vertical-align:middle;">
<p><span class="font3">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.825</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.852</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">0.80</span></p></td></tr>
</table>
<p><span class="font3" style="font-weight:bold;text-decoration:underline;">Table 5. </span><span class="font3" style="text-decoration:underline;">Comparison Result</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font3">Author</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Method</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Acccuracy</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Wonghabut et. al. [14]</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">Aspec Ratio</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">74 %</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3">Rubaiyat et. al. [5]</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Color + CHT</span></p></td><td style="vertical-align:top;">
<p><span class="font3">81 %</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">Proposed Method</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">KNN + HOG</span></p></td><td style="vertical-align:middle;">
<p><span class="font3">82.5 %</span></p></td></tr>
</table>
<p><span class="font3" style="font-weight:bold;text-decoration:underline;">Table 6. </span><span class="font3" style="text-decoration:underline;">Computation Time Result</span></p>
<table border="1">
<tr><td style="vertical-align:top;">
<p><span class="font3">Test Number</span></p></td><td style="vertical-align:top;">
<p><span class="font3">Time (s)</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">1</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.40369</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">2</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.42919</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">3</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.48211</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">4</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.44819</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">5</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.42123</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">6</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.44429</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">7</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.43858</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">8</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.42197</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">9</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.42451</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font3">10</span></p></td><td style="vertical-align:middle;">
<p><span class="font5">1.43190</span></p></td></tr>
<tr><td style="vertical-align:top;">
<p><span class="font3">Average</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">1.43457</span></p>
<p><span class="font7" style="font-style:italic;">. &nbsp;&nbsp;&nbsp;&nbsp;_ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TP+TN</span></p>
<p><span class="font13" style="font-style:italic;">Accuracy</span><span class="font3"> = ----------- (8)</span></p>
<p><span class="font9" style="font-style:italic;font-variant:small-caps;"><sup>j</sup> &nbsp;&nbsp;&nbsp;tp+tn+fp+fn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>v ,</sup></span></p>
<p><span class="font13" style="font-style:italic;">Precision = &nbsp;</span><span class="font10" style="font-style:italic;font-variant:small-caps;text-decoration:line-through;"><sup>tp</sup></span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(9)</span></p>
<p><span class="font9" style="font-style:italic;font-variant:small-caps;">tp+fp &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>v ,</sup></span></p>
<p><span class="font13" style="font-style:italic;">Recall= </span><span class="font9" style="font-style:italic;font-variant:small-caps;">-<sup>p</sup>-</span><span class="font3"> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(10)</span></p>
<p><span class="font9" style="font-style:italic;font-variant:small-caps;">tp+fn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<sup>v &nbsp;&nbsp;,</sup></span></p></td></tr>
</table>
<p><span class="font3">TP (True Positif) is the number of correct predictions in positive class, in this case, is the helmetwearing class, TN (True Negatif) is the number of correct predictions in negative class, in this case, is the non-helmet wearing class. FP (False Positif) is the number of incorrect predictions in the positive class, and FN (False Negatif) is the number of incorrect predictions in the negative[15]. The confusion matrix is shown in Table 3.</span></p>
<p><span class="font3">Testing was done by using different k-value in the KNN classifier. In this research, the k-value that users ware 1,3,5 and 7. In this research, we classify the data into two classes, the first, the helmet-wearing class, and the second, is non-helmet wearing class. Each k-value produce a different result, but the result ware satisfactory. The result of the test is shown in Table 4. The average accuracy, precision, and recall are relatively high, and it is shown that the k-value of 1 and 7 produces the highest score, but the k-value of 1 is better because it useless calculation than k-value 7. After getting the best result from our experiment, we compare it with previous research. We compare our work with two other research about helmet detection, and the result is our work produce slightly better accuracy when detecting helmet. Comparison results can be seen in Table 5.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark48"></a><span class="font3" style="font-weight:bold;"><a name="bookmark49"></a>3.3. &nbsp;&nbsp;&nbsp;Computation Time Resul</span></h2></li></ul>
<p><span class="font3">In order to know how fast the detection time of the proposed method, we do a computation time test. The test was done on a computer that runs Microsoft Windows 10 with a processor Intel(R) Core(TM) i5-4460 and 8 GB memory. The data we use in the test is the training and testing data mention in section 2.4.1. We run the program ten times with K-value 1, and the result is shown in Table 6. The average time we got from the experiments is 1.43457 s.</span></p>
<p><span class="font3">In this research, we use the KNN classification because previous studies have good accuracy. Meanwhile, the consumption of computation time could be reduced by sklearn's tools in Phyton. KNN Classification uses sklearn's tools, only takes 0.4 seconds. Besides that, to increase the speed performance, we resize the original image that has a different size to become 64x128 pixel So, resizing the size of data also will be decreased the speed of KNN performance.</span></p>
<p><span class="font3">The test results show a computation time for each detection. This shows that the proposed method produces a fast time to detect so that it can be implemented in real-time, although the quality is poor. We can improve the quality of real-time implementation by reducing computation</span></p>
<p><span class="font3">time. This problem can be resolved by improving the quality of the computer or using a faster classification method.</span></p>
<ul style="list-style:none;"><li>
<h2><a name="bookmark50"></a><span class="font3" style="font-weight:bold;"><a name="bookmark51"></a>4. &nbsp;&nbsp;&nbsp;Conclusion</span></h2></li></ul>
<p><span class="font3">This study comes about the detection of motorcyclists without a helmet. The system builds based on computer vision technology, which is divided as follows: shape detection, feature extraction, and image classification. The results were satisfactory.</span></p>
<p><span class="font3">The KNN classifier using the feature from HOG can classify between the helmet-wearing motorcyclist and the motorcyclist that is not wearing a helmet. We use different k-value in the testing process. The K-value that got the best result is 1 and 7, with an average accuracy of 82.5 % and average computation time is 1.43457 s. The present result is promising but can be improved. One of the future works is license plate recognition with the purpose of detecting the license plate from motorcyclists that are not wearing a helmet. For this, it is necessary an image with better quality to recognize the characters.</span></p>
<h2><a name="bookmark52"></a><span class="font3" style="font-weight:bold;"><a name="bookmark53"></a>References</span></h2>
<ul style="list-style:none;"><li>
<p><span class="font3">[1] &nbsp;&nbsp;&nbsp;Badan Pusat Statistik Kota Salatiga, “Salatiga Dalam Angka Tahun 2013,” pp. 1, 115, 155, 2013.</span></p></li>
<li>
<p><span class="font3">[2] &nbsp;&nbsp;&nbsp;L. Gicquel, P. Ordonneau, E. Blot, C. Toillon, P. Ingrand, and L. Romo, &quot;Description of various factors contributing to traffic accidents in youth and measures proposed to alleviate recurrence,&quot; </span><span class="font3" style="font-style:italic;">Frontiers of &nbsp;Psychiatry</span><span class="font3">, &nbsp;vol. 8, no. JUN, pp. 1–10, &nbsp;2017, doi:</span></p></li></ul>
<p><span class="font3">10.11591/ijeei.v6i4.463</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[3] &nbsp;&nbsp;&nbsp;P. Doungmala and K. Klubsuwan, &quot;Half and Full Helmet Wearing Detection in Thai- land using Haar Like Feature and Circle Hough Transform on Image Processing Pathasu, &quot;&nbsp;</span><span class="font3" style="font-style:italic;">Proc. - 2016 16th IEEE Int. Conference on Computer and Information Technology CIT 2016, 2016 6th International Symposium Cloud and Service Computing IEEE SC2 2016 2016</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">International Symposium Security and Privacy in Social Networks and Big Data</span><span class="font3">, pp. 611– 614, 2017, doi: 10.1109/CIT.2016.87</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[4] &nbsp;&nbsp;&nbsp;L. J. L. C. Wen C. Chiu S., &quot;The safety helmet detection for ATM's surveillance system via the modified Hough transform,&quot; </span><span class="font3" style="font-style:italic;">Proceedings of Annual IEEE International Carnahan Conference on Security Technology</span><span class="font3">, pp. 259–263, 2003, doi: 10.1109/CCST.2003.1297588</span></p></li>
<li>
<p><span class="font3">[5] &nbsp;&nbsp;&nbsp;A. H. M. Rubaiyat </span><span class="font3" style="font-style:italic;">et al.</span><span class="font3">, &quot;Automatic detection of helmet uses for construction safety,&quot; </span><span class="font3" style="font-style:italic;">Proceedings - 2016 IEEE/WIC/ACM International Conference on Web Intelligence</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Workshops, WIW 2016</span><span class="font3">, no. November, pp. 135–142, 2017, doi: 10.1109/WIW.2016.10</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[6] &nbsp;&nbsp;&nbsp;T. Kumar and K. Verma, &quot;A Theory Based on Conversion of RGB image to Gray image,&quot; </span><span class="font3" style="font-style:italic;">International Journal of Computer Applications.</span><span class="font3">, vol. 7, no. 2, pp. 5–12, 2010, doi: 10.5120/1140-1493</span></p></li>
<li>
<p><span class="font3">[7] &nbsp;&nbsp;&nbsp;H. Liu, Y. Qian, and S. Lin, &quot;Detecting persons using hough circle transform in surveillance video,&quot; </span><span class="font3" style="font-style:italic;">VISAPP 2010 - Proceedings of the International Conference on Computer Vision Theory and Applications</span><span class="font3">, vol. 2, no. January, 2010, doi: 10.5220/0002856002670270</span></p></li>
<li>
<p><span class="font3">[8] &nbsp;&nbsp;&nbsp;L. H. Liew, B. Y. Lee, and M. Chan, &quot;Cell detection for bee comb images using Circular hough transformation,&quot; </span><span class="font3" style="font-style:italic;">CSSR 2010 - 2010 International Conference on Science and Social Research</span><span class="font3">, no. Cssr, pp. 191–195, 2010, doi: 10.1109/CSSR.2010.5773764</span></p></li>
<li>
<p><span class="font3">[9] &nbsp;&nbsp;&nbsp;Pei-Yin Chen, Chien-Chuan Huang, Chih-Yuan Lien, and Yu-Hsien Tsai, &quot;An Efficient Hardware Implementation of HOG Feature Extraction for Human Detection,&quot; </span><span class="font3" style="font-style:italic;">IEEE Transactions on Intelligent Transportation Systems</span><span class="font3">, vol. 15, no. 2, pp. 656–662, 2014, doi: 10.1109/TITS.2013.2284666</span></p></li>
<li>
<p><span class="font3">[10] &nbsp;&nbsp;&nbsp;K. N. Stevens, T. M. Cover, and P. E. Hart, &quot;Nearest Neighbor Pattern Classification,&quot; vol. IT-13, no. 1, pp. 21–27, 1967.</span></p></li>
<li>
<p><span class="font3">[11] &nbsp;&nbsp;&nbsp;J. Maillo, S. Ramírez, I. Triguero, and F. Herrera, &quot;kNN-IS: An Iterative Spark-based design of the k-Nearest Neighbors classifier for big data,&quot; </span><span class="font3" style="font-style:italic;">Knowledge-Based System</span><span class="font3">, vol. 117, pp. 3–15, 2017, doi: 10.1016/j.knosys.2016.06.012</span></p></li>
<li>
<p><span class="font3">[12] &nbsp;&nbsp;&nbsp;F. A. Mufarroha and F. Utaminingrum, &quot;Hand Gesture Recognition using Adaptive Network Based Fuzzy Inference System and K-Nearest Neighbor,&quot; </span><span class="font3" style="font-style:italic;">International Journal of Technology</span><span class="font3">, vol. 8, no. 3, p. 559, 2017, doi: 10.14716/ijtech.v8i3.3146</span></p></li>
<li>
<p><span class="font3">[13] &nbsp;&nbsp;&nbsp;J. Kim, B.S. Kim, and S. Savarese, &quot;Comparing Image Classification Methods: K-Nearest-Neighbor and Support-Vector-Machines,&quot; </span><span class="font3" style="font-style:italic;">Applied Mathematics in Electrical and Computer</span></p></li></ul>
<p><span class="font3" style="font-style:italic;">Engineering</span><span class="font3">, pp. 133–138, 2012.</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[14] &nbsp;&nbsp;&nbsp;P. Wonghabut, J. Kumphong, T. Satiennam, R. Ung-Arunyawee, and W. Leelapatra, &quot;Automatic helmet-wearing detection for law enforcement using CCTV cameras, &quot;&nbsp;</span><span class="font3" style="font-style:italic;">IOP Conference Series: Earth and Environmental Science</span><span class="font3">, vol. 143, no. 1, 2018. doi:</span></p></li></ul>
<p><span class="font3">10.1088/1755-1315/143/1/012063</span></p>
<ul style="list-style:none;"><li>
<p><span class="font3">[15] &nbsp;&nbsp;&nbsp;S. Tiwari, &quot;Blur classification using segmentation based fractal texture analysis,&quot; </span><span class="font3" style="font-style:italic;">Indonesian Journal of Electrical Engineering and Informatics</span><span class="font3">, vol. 6, no. 4, pp. 373–384, 2018. doi: 10.11591/ijeei.v6i4.463</span></p></li></ul>
<p><span class="font3">23</span></p>